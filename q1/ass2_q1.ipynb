{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ass2_q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6_Z6Mrst7xY",
        "outputId": "ebaa1d94-6eb3-4909-fecb-f5c6e861d725"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srWCp3saWnHs",
        "outputId": "b6a04718-ad40-4010-c36f-4211b4787cfa"
      },
      "source": [
        "!ls \"gdrive/MyDrive/wikitext-2\" # check that it has successfully connected\n",
        "# files should be at ur GDrive inside folder wikitext-2\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.test.tokens.txt\" \"test.txt\" # copy the files to colab runtime\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.train.tokens.txt\" \"train.txt\"\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.valid.tokens.txt\" \"valid.txt\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'fgdrive/MyDrive/{dir_ext}wikitext-2': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDH_IyLyasQC"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JSW06aZavxI"
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                # remove the headers e.g.  = = Description = = \n",
        "                if line.startswith('='): \n",
        "                    continue\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word.lower()) # make to lower\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word.lower()]) # make to lower\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.dictionary.idx2word)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BY-5KRizwDS"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BujxiynKzz7V"
      },
      "source": [
        "#=== params\n",
        "corpus = Corpus('/content')\n",
        "n_class = corpus.vocab_size\n",
        "n_step = 7 # n-1 in paper\n",
        "n_hidden = 200 # h in paper\n",
        "embed_size = 200       # m in paper\n",
        "batch_size = 1000\n",
        "order = n_step # order (int): the order of the language model, i.e. length of the history\n",
        "epochs = 40\n",
        "learning_rate = 0.01\n",
        "cuda = torch.cuda.is_available()\n",
        "seed = 42\n",
        "#==="
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCn0n7sLb7uU"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf6-SXVscAqs"
      },
      "source": [
        "#== MODEL ==#\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "    the neural model learns the distributed representation of each word \n",
        "    (embedding matrix C) and \n",
        "    the probability function of a word sequence as a function of their distributed representations. \n",
        "    It has a hidden layer with \n",
        "    tanh activation and the output layer is a \n",
        "    Softmax layer. \n",
        "    The output of the model for each \n",
        "    input of (n - 1) previous words are the \n",
        "    probabilities over the |V | words in the vocabulary for the next word.\n",
        "'''\n",
        "class FNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, context_size, no_hidden):\n",
        "        super(FNNModel,self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_class (int): no. of vocabulary\n",
        "            m (int): size of each embedding vector\n",
        "#n-gram models construct tables of conditional probabilities for the next word, \n",
        "#for each one of a large number of contexts, i.e. combinations of the last n − 1 words\n",
        "            n_step (int): n-1 in paper. #n_step + 1 = n-gram. if n_step = 1, bigram\n",
        "            n_hidden (int): no. of hidden units associated with each word\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Vars:\n",
        "            C: encoder (|V| x m)\n",
        "            H: hiden layer weight (n x (n-1)m)\n",
        "            W: word feature to output weights (|V| x (n-1)m)\n",
        "            d: hidden layer bias (has h no. of elements)\n",
        "            U: hidden-to-output weights (|V| × h matrix)\n",
        "            b: output bias (has |V| no. of elements)\n",
        "        \"\"\"\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.linear1 = nn.Linear(context_size * embed_size, no_hidden)\n",
        "        self.linear2 = nn.Linear(no_hidden, vocab_size)\n",
        "        self.context_size = context_size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        embeds = self.embeddings(inputs).view((-1, self.context_size * self.embed_size))\n",
        "        hidden_output = self.linear1(embeds)\n",
        "        out = F.tanh(hidden_output)\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out)\n",
        "        return log_probs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZez81nOa5Iv"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXNNicaihZe5"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into args.batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data\n",
        "def get_batch(data, i, order):\n",
        "    x = torch.autograd.Variable(torch.t(data[i:i+order]))\n",
        "    y = torch.autograd.Variable(data[i+order].view(-1))\n",
        "    return x, y\n",
        "def evaluate(data, model, criterion):\n",
        "\tmodel.eval()\n",
        "\ttotal_loss = 0\n",
        "\tn_steps = data.size(0) - order - 1\n",
        "\tfor i in tqdm(range(n_steps)):\n",
        "\t\tx, y = get_batch(data, i, order)\n",
        "\t\tout = model(x)\n",
        "\t\tloss = criterion(out, y)\n",
        "\t\ttotal_loss += loss.data.data\n",
        "\treturn total_loss / n_steps"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zavgNg0tklRG"
      },
      "source": [
        "def clock_time(s):\n",
        "    h, s = divmod(s, 3600)\n",
        "    m, s = divmod(s, 60)\n",
        "    return int(h), int(m), int(s)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztusWxUma4mg",
        "outputId": "2e0bc924-b56c-4b91-993f-6d1eef45f98b"
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, batch_size)\n",
        "test_data = batchify(corpus.test, batch_size)\n",
        "if cuda:\n",
        "\ttrain_data, val_data, test_data = train_data.cuda(), val_data.cuda(), test_data.cuda()\n",
        "print('Using cuda: {}'.format(cuda))\n",
        "print('Size of training set: {:,} tokens'.format(np.prod(train_data.size())))\n",
        "print('Size of validation set: {:,} tokens'.format(np.prod(val_data.size())))\n",
        "print('Size of test set: {:,} tokens'.format(np.prod(test_data.size())))\n",
        "print('Vocabulary size: {:,}'.format(corpus.vocab_size))\n",
        "print('Example data:')\n",
        "for k in range(100, 107):\n",
        "    x = [corpus.dictionary.idx2word[i] for i in train_data[k:order+k, 0]]\n",
        "    y = [corpus.dictionary.idx2word[train_data[k+order, 0]]]\n",
        "    print(x, y)\n",
        "#=== initialise model\n",
        "model = FNNModel(\n",
        "    n_class, \n",
        "    embed_size, \n",
        "    n_step, \n",
        "    n_hidden)\n",
        "if cuda:\n",
        "  model.cuda()\n",
        "# Display the model's architecture\n",
        "print('Model: \\n', model)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda: True\n",
            "Size of training set: 2,088,000 tokens\n",
            "Size of validation set: 217,000 tokens\n",
            "Size of test set: 245,000 tokens\n",
            "Vocabulary size: 28,912\n",
            "Example data:\n",
            "['\"', 'nameless', '\"', ',', 'a', 'penal', 'military'] ['unit']\n",
            "['nameless', '\"', ',', 'a', 'penal', 'military', 'unit'] ['serving']\n",
            "['\"', ',', 'a', 'penal', 'military', 'unit', 'serving'] ['the']\n",
            "[',', 'a', 'penal', 'military', 'unit', 'serving', 'the'] ['nation']\n",
            "['a', 'penal', 'military', 'unit', 'serving', 'the', 'nation'] ['of']\n",
            "['penal', 'military', 'unit', 'serving', 'the', 'nation', 'of'] ['gallia']\n",
            "['military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia'] ['during']\n",
            "Model: \n",
            " FNNModel(\n",
            "  (embeddings): Embedding(28912, 200)\n",
            "  (linear1): Linear(in_features=1400, out_features=200, bias=True)\n",
            "  (linear2): Linear(in_features=200, out_features=28912, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4bxaGvDZwK5",
        "outputId": "03ba6161-332e-4e23-8f11-e4162044aa5e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 25 10:21:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    33W /  70W |   2371MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArPR9kzyblhd"
      },
      "source": [
        "# Set seed for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "parameters = [param for param in model.parameters() if param.requires_grad]\n",
        "# Training\n",
        "print('Training...')\n",
        "losses = dict(train=[], val=[])\n",
        "\n",
        "lr = learning_rate # so that can alter later if SGD not descending \n",
        "best_val_loss = None\n",
        "\n",
        "num_steps = train_data.size(0) - order - 1\n",
        "batch_order = np.arange(num_steps)\n",
        "\n",
        "t0 = time.time()\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        np.random.shuffle(batch_order)\n",
        "\n",
        "        for step in range(1, num_steps+1):\n",
        "            idx = batch_order[step-1]\n",
        "            x, y = get_batch(train_data, idx, order)\n",
        "\n",
        "            model.zero_grad()\n",
        "            # Forward pass\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            debug = False\n",
        "            #   if debug:\n",
        "            #     # Debugging softmax approximation.\n",
        "            #     xe = nn.CrossEntropyLoss()\n",
        "            #     true_loss = xe(logits, y)\n",
        "            #     print('approx {:>3.2f}, true {:>3.2f}, diff {:>3.4f}'.format(\n",
        "            #       loss.data, true_loss.data, true_loss.data - loss.data))\n",
        "\n",
        "            # Update parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save loss.\n",
        "            losses['train'].append(loss.cpu().data)\n",
        "            print_every = 500\n",
        "            if step % print_every == 0:\n",
        "                avg_loss = sum(losses['train'][-print_every:]) / print_every\n",
        "                t1 = time.time()\n",
        "                steps_per_second = print_every / (t1 - t0)\n",
        "                print('| epoch {} | step {}/{} | loss {:.4f} | lr {:.3f} | '\n",
        "                    'ngrams/sec {:.1f} | eta {}h{}m{}s'.format(\n",
        "                    epoch, step, num_steps, avg_loss, lr,\n",
        "                    steps_per_second * batch_size,\n",
        "                    *clock_time((num_steps - step) / steps_per_second)))\n",
        "                t0 = time.time()\n",
        "            \n",
        "        print('Evaluating on validation set...')\n",
        "        val_loss = evaluate(val_data, model, criterion)\n",
        "        losses['val'].append(val_loss)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "            epoch, (time.time() - epoch_start_time), val_loss, torch.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "            #=== download checkpoint file\n",
        "            # files.download('checkpoint.pth')\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    \n",
        "# write_losses(losses['train'], args.log_dir, name='train-losses')\n",
        "# write_losses(losses['val'], args.log_dir, name='val-losses')\n",
        "\n",
        "print('Evaluating on test set...')\n",
        "test_loss = evaluate(test_data, model, criterion)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, torch.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jUqz8keSi6Mi",
        "outputId": "8a1db53f-9d08-4458-861b-21505fafc6f0"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2f9d0e5e-a3f8-4422-b029-6d07167a38a5\", \"checkpoint.pth\", 1043084)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbeYK7MGwQbE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}