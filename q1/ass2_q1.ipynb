{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6_Z6Mrst7xY",
        "outputId": "69bb7fb7-1340-4772-8a06-361db38582c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srWCp3saWnHs",
        "outputId": "f1a327c4-08bf-4dde-f35f-41f401a020f7"
      },
      "source": [
        "!ls \"gdrive/MyDrive/wikitext-2\" # check that it has successfully connected\n",
        "# files should be at ur GDrive inside folder wikitext-2\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.test.tokens.txt\" \"test.txt\" # copy the files to colab runtime\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.train.tokens.txt\" \"train.txt\"\n",
        "!cp \"gdrive/MyDrive/wikitext-2/wiki.valid.tokens.txt\" \"valid.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wiki.test.tokens      wiki.train.tokens      wiki.valid.tokens\n",
            "wiki.test.tokens.txt  wiki.train.tokens.txt  wiki.valid.tokens.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDH_IyLyasQC"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JSW06aZavxI"
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                # remove the headers e.g.  = = Description = = \n",
        "                if line.startswith('='): \n",
        "                    continue\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word.lower()) # make to lower\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word.lower()]) # make to lower\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.dictionary.idx2word)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BY-5KRizwDS"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BujxiynKzz7V"
      },
      "source": [
        "#=== params\n",
        "corpus = Corpus('/content')\n",
        "n_class = corpus.vocab_size\n",
        "n_step = 2 # n-1 in paper\n",
        "n_hidden = 2 # h in paper\n",
        "m = 2       # m in paper\n",
        "epochs = 5000\n",
        "batch_size = 20\n",
        "order = 2 # order (int): the order of the language model, i.e. length of the history\n",
        "epochs = 500\n",
        "learning_rate = 0.5 #0.001\n",
        "cuda = torch.cuda.is_available()\n",
        "seed = 42\n",
        "#==="
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCn0n7sLb7uU"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf6-SXVscAqs"
      },
      "source": [
        "#== MODEL ==#\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "    the neural model learns the distributed representation of each word \n",
        "    (embedding matrix C) and \n",
        "    the probability function of a word sequence as a function of their distributed representations. \n",
        "    It has a hidden layer with \n",
        "    tanh activation and the output layer is a \n",
        "    Softmax layer. \n",
        "    The output of the model for each \n",
        "    input of (n - 1) previous words are the \n",
        "    probabilities over the |V | words in the vocabulary for the next word.\n",
        "'''\n",
        "class FNNModel(nn.Module):\n",
        "    def __init__(self, n_class, m, n_step, n_hidden):\n",
        "        super(FNNModel,self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_class (int): no. of vocabulary\n",
        "            m (int): size of each embedding vector\n",
        "#n-gram models construct tables of conditional probabilities for the next word, \n",
        "#for each one of a large number of contexts, i.e. combinations of the last n − 1 words\n",
        "            n_step (int): n-1 in paper. #n_step + 1 = n-gram. if n_step = 1, bigram\n",
        "            n_hidden (int): no. of hidden units associated with each word\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Vars:\n",
        "            C: encoder (|V| x m)\n",
        "            H: hiden layer weight (n x (n-1)m)\n",
        "            W: word feature to output weights (|V| x (n-1)m)\n",
        "            d: hidden layer bias (has h no. of elements)\n",
        "            U: hidden-to-output weights (|V| × h matrix)\n",
        "            b: output bias (has |V| no. of elements)\n",
        "        \"\"\"\n",
        "        self.C = nn.Embedding(n_class,m)\n",
        "        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(torch.Tensor))\n",
        "        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(torch.Tensor))\n",
        "        self.d = nn.Parameter(torch.randn(n_hidden).type(torch.Tensor))\n",
        "        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(torch.Tensor))\n",
        "        self.b = nn.Parameter(torch.randn(n_class).type(torch.Tensor))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.C(x)\n",
        "        x = x.view(-1, n_step * m) # batch_size,n_step * n_class\n",
        "        tanh = torch.tanh(self.d + torch.mm(x, self.H))\n",
        "        output = self.b + torch.mm(x,self.W)+torch.mm(tanh,self.U)\n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZez81nOa5Iv"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXNNicaihZe5"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into args.batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data\n",
        "def get_batch(data, i, order):\n",
        "    x = Variable(torch.t(data[i:i+order]))\n",
        "    y = Variable(data[i+order].view(-1))\n",
        "    return x, y\n",
        "def evaluate(data, model, criterion):\n",
        "\tmodel.eval()\n",
        "\ttotal_loss = 0\n",
        "\tn_steps = data.size(0) - order - 1\n",
        "\tfor i in tqdm(range(n_steps)):\n",
        "\t\tx, y = get_batch(data, i, order)\n",
        "\t\tout = model(x)\n",
        "\t\tloss = criterion(out, y)\n",
        "\t\ttotal_loss += loss.data.data\n",
        "\treturn total_loss / n_steps"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zavgNg0tklRG"
      },
      "source": [
        "def clock_time(s):\n",
        "    h, s = divmod(s, 3600)\n",
        "    m, s = divmod(s, 60)\n",
        "    return int(h), int(m), int(s)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztusWxUma4mg",
        "outputId": "61b026f3-99aa-45f1-9620-3fb9ed10e848"
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, batch_size)\n",
        "test_data = batchify(corpus.test, batch_size)\n",
        "if cuda:\n",
        "\ttrain_data, val_data, test_data = train_data.cuda(), val_data.cuda(), test_data.cuda()\n",
        "print('Using cuda: {}'.format(cuda))\n",
        "print('Size of training set: {:,} tokens'.format(np.prod(train_data.size())))\n",
        "print('Size of validation set: {:,} tokens'.format(np.prod(val_data.size())))\n",
        "print('Size of test set: {:,} tokens'.format(np.prod(test_data.size())))\n",
        "print('Vocabulary size: {:,}'.format(corpus.vocab_size))\n",
        "print('Example data:')\n",
        "for k in range(100, 107):\n",
        "    x = [corpus.dictionary.idx2word[i] for i in train_data[k:order+k, 0]]\n",
        "    y = [corpus.dictionary.idx2word[train_data[k+order, 0]]]\n",
        "    print(x, y)\n",
        "#=== initialise model\n",
        "model = FNNModel(\n",
        "    n_class, \n",
        "    m, \n",
        "    n_step, \n",
        "    n_hidden)\n",
        "if cuda:\n",
        "  model.cuda()\n",
        "# Display the model's architecture\n",
        "print('Model: \\n', model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda: True\n",
            "Size of training set: 2,088,620 tokens\n",
            "Size of validation set: 217,640 tokens\n",
            "Size of test set: 245,560 tokens\n",
            "Vocabulary size: 28,912\n",
            "Example data:\n",
            "['\"', 'nameless'] ['\"']\n",
            "['nameless', '\"'] [',']\n",
            "['\"', ','] ['a']\n",
            "[',', 'a'] ['penal']\n",
            "['a', 'penal'] ['military']\n",
            "['penal', 'military'] ['unit']\n",
            "['military', 'unit'] ['serving']\n",
            "Model: \n",
            " FNNModel(\n",
            "  (C): Embedding(28912, 2)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArPR9kzyblhd",
        "outputId": "68e6bdfd-8532-429b-b67a-be038ec12850"
      },
      "source": [
        "# Set seed for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "parameters = [param for param in model.parameters() if param.requires_grad]\n",
        "# Training\n",
        "print('Training...')\n",
        "losses = dict(train=[], val=[])\n",
        "\n",
        "lr = learning_rate # so that can alter later if SGD not descending \n",
        "best_val_loss = None\n",
        "\n",
        "num_steps = train_data.size(0) - order - 1\n",
        "batch_order = np.arange(num_steps)\n",
        "\n",
        "t0 = time.time()\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        epoch_start_time = time.time()\n",
        "        np.random.shuffle(batch_order)\n",
        "\n",
        "        for step in range(1, num_steps+1):\n",
        "            idx = batch_order[step-1]\n",
        "            x, y = get_batch(train_data, idx, order)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            debug = False\n",
        "            #   if debug:\n",
        "            #     # Debugging softmax approximation.\n",
        "            #     xe = nn.CrossEntropyLoss()\n",
        "            #     true_loss = xe(logits, y)\n",
        "            #     print('approx {:>3.2f}, true {:>3.2f}, diff {:>3.4f}'.format(\n",
        "            #       loss.data, true_loss.data, true_loss.data - loss.data))\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save loss.\n",
        "            losses['train'].append(loss.cpu().data)\n",
        "            print_every = 1000\n",
        "            if step % print_every == 0:\n",
        "                avg_loss = sum(losses['train'][-print_every:]) / print_every\n",
        "                t1 = time.time()\n",
        "                steps_per_second = print_every / (t1 - t0)\n",
        "                print('| epoch {} | step {}/{} | loss {:.4f} | lr {:.3f} | '\n",
        "                    'ngrams/sec {:.1f} | eta {}h{}m{}s'.format(\n",
        "                    epoch, step, num_steps, avg_loss, lr,\n",
        "                    steps_per_second * batch_size,\n",
        "                    *clock_time((num_steps - step) / steps_per_second)))\n",
        "                t0 = time.time()\n",
        "            \n",
        "        print('Evaluating on validation set...')\n",
        "        val_loss = evaluate(val_data, model, criterion)\n",
        "        losses['val'].append(val_loss)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "            epoch, (time.time() - epoch_start_time), val_loss, torch.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "            #=== download checkpoint file\n",
        "            # files.download('checkpoint.pth')\n",
        "        else:\n",
        "            # reduce the learning rate if no improvement has been seen in the validation dataset.\n",
        "            print(\"reducing learning rate\")\n",
        "            lr /= 4.0\n",
        "            optimizer = torch.optim.SGD(parameters, lr=lr)\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    \n",
        "# write_losses(losses['train'], args.log_dir, name='train-losses')\n",
        "# write_losses(losses['val'], args.log_dir, name='val-losses')\n",
        "\n",
        "print('Evaluating on test set...')\n",
        "test_loss = evaluate(test_data, model, criterion)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, torch.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "| epoch 1 | step 1000/104428 | loss 42.6267 | lr 0.500 | ngrams/sec 7157.6 | eta 0h4m49s\n",
            "| epoch 1 | step 2000/104428 | loss 79.2601 | lr 0.500 | ngrams/sec 7583.9 | eta 0h4m30s\n",
            "| epoch 1 | step 3000/104428 | loss 89.8685 | lr 0.500 | ngrams/sec 6920.6 | eta 0h4m53s\n",
            "| epoch 1 | step 4000/104428 | loss 89.7026 | lr 0.500 | ngrams/sec 6561.5 | eta 0h5m6s\n",
            "| epoch 1 | step 5000/104428 | loss 95.4461 | lr 0.500 | ngrams/sec 7506.5 | eta 0h4m24s\n",
            "| epoch 1 | step 6000/104428 | loss 101.9821 | lr 0.500 | ngrams/sec 7653.8 | eta 0h4m17s\n",
            "| epoch 1 | step 7000/104428 | loss 99.1816 | lr 0.500 | ngrams/sec 7609.6 | eta 0h4m16s\n",
            "| epoch 1 | step 8000/104428 | loss 100.9478 | lr 0.500 | ngrams/sec 7137.3 | eta 0h4m30s\n",
            "| epoch 1 | step 9000/104428 | loss 95.2506 | lr 0.500 | ngrams/sec 7362.4 | eta 0h4m19s\n",
            "| epoch 1 | step 10000/104428 | loss 94.7713 | lr 0.500 | ngrams/sec 7556.9 | eta 0h4m9s\n",
            "| epoch 1 | step 11000/104428 | loss 101.5965 | lr 0.500 | ngrams/sec 7510.7 | eta 0h4m8s\n",
            "| epoch 1 | step 12000/104428 | loss 110.0512 | lr 0.500 | ngrams/sec 7488.6 | eta 0h4m6s\n",
            "| epoch 1 | step 13000/104428 | loss 119.4399 | lr 0.500 | ngrams/sec 7693.8 | eta 0h3m57s\n",
            "| epoch 1 | step 14000/104428 | loss 115.3158 | lr 0.500 | ngrams/sec 7571.1 | eta 0h3m58s\n",
            "| epoch 1 | step 15000/104428 | loss 118.7341 | lr 0.500 | ngrams/sec 7593.0 | eta 0h3m55s\n",
            "| epoch 1 | step 16000/104428 | loss 111.4303 | lr 0.500 | ngrams/sec 7625.1 | eta 0h3m51s\n",
            "| epoch 1 | step 17000/104428 | loss 110.7767 | lr 0.500 | ngrams/sec 7663.2 | eta 0h3m48s\n",
            "| epoch 1 | step 18000/104428 | loss 111.0714 | lr 0.500 | ngrams/sec 7570.2 | eta 0h3m48s\n",
            "| epoch 1 | step 19000/104428 | loss 109.7593 | lr 0.500 | ngrams/sec 7575.4 | eta 0h3m45s\n",
            "| epoch 1 | step 20000/104428 | loss 108.3213 | lr 0.500 | ngrams/sec 7331.7 | eta 0h3m50s\n",
            "| epoch 1 | step 21000/104428 | loss 107.7390 | lr 0.500 | ngrams/sec 7001.1 | eta 0h3m58s\n",
            "| epoch 1 | step 22000/104428 | loss 106.7540 | lr 0.500 | ngrams/sec 7491.8 | eta 0h3m40s\n",
            "| epoch 1 | step 23000/104428 | loss 116.7773 | lr 0.500 | ngrams/sec 7722.3 | eta 0h3m30s\n",
            "| epoch 1 | step 24000/104428 | loss 116.9876 | lr 0.500 | ngrams/sec 7079.6 | eta 0h3m47s\n",
            "| epoch 1 | step 25000/104428 | loss 108.6296 | lr 0.500 | ngrams/sec 7838.5 | eta 0h3m22s\n",
            "| epoch 1 | step 26000/104428 | loss 107.0460 | lr 0.500 | ngrams/sec 7604.3 | eta 0h3m26s\n",
            "| epoch 1 | step 27000/104428 | loss 97.7178 | lr 0.500 | ngrams/sec 7712.8 | eta 0h3m20s\n",
            "| epoch 1 | step 28000/104428 | loss 104.8160 | lr 0.500 | ngrams/sec 6958.0 | eta 0h3m39s\n",
            "| epoch 1 | step 29000/104428 | loss 96.9968 | lr 0.500 | ngrams/sec 7403.7 | eta 0h3m23s\n",
            "| epoch 1 | step 30000/104428 | loss 105.8964 | lr 0.500 | ngrams/sec 6891.0 | eta 0h3m36s\n",
            "| epoch 1 | step 31000/104428 | loss 110.6661 | lr 0.500 | ngrams/sec 6965.4 | eta 0h3m30s\n",
            "| epoch 1 | step 32000/104428 | loss 104.6943 | lr 0.500 | ngrams/sec 7295.8 | eta 0h3m18s\n",
            "| epoch 1 | step 33000/104428 | loss 109.3458 | lr 0.500 | ngrams/sec 7704.0 | eta 0h3m5s\n",
            "| epoch 1 | step 34000/104428 | loss 110.7943 | lr 0.500 | ngrams/sec 7684.4 | eta 0h3m3s\n",
            "| epoch 1 | step 35000/104428 | loss 119.8961 | lr 0.500 | ngrams/sec 7865.5 | eta 0h2m56s\n",
            "| epoch 1 | step 36000/104428 | loss 118.4019 | lr 0.500 | ngrams/sec 7064.4 | eta 0h3m13s\n",
            "| epoch 1 | step 37000/104428 | loss 122.2974 | lr 0.500 | ngrams/sec 7853.2 | eta 0h2m51s\n",
            "| epoch 1 | step 38000/104428 | loss 116.8659 | lr 0.500 | ngrams/sec 7099.3 | eta 0h3m7s\n",
            "| epoch 1 | step 39000/104428 | loss 113.2209 | lr 0.500 | ngrams/sec 7361.9 | eta 0h2m57s\n",
            "| epoch 1 | step 40000/104428 | loss 122.3518 | lr 0.500 | ngrams/sec 7400.2 | eta 0h2m54s\n",
            "| epoch 1 | step 41000/104428 | loss 113.5269 | lr 0.500 | ngrams/sec 7393.6 | eta 0h2m51s\n",
            "| epoch 1 | step 42000/104428 | loss 108.0721 | lr 0.500 | ngrams/sec 7236.7 | eta 0h2m52s\n",
            "| epoch 1 | step 43000/104428 | loss 107.2925 | lr 0.500 | ngrams/sec 7588.3 | eta 0h2m41s\n",
            "| epoch 1 | step 44000/104428 | loss 116.1553 | lr 0.500 | ngrams/sec 7620.3 | eta 0h2m38s\n",
            "| epoch 1 | step 45000/104428 | loss 108.5087 | lr 0.500 | ngrams/sec 7818.1 | eta 0h2m32s\n",
            "| epoch 1 | step 46000/104428 | loss 108.7797 | lr 0.500 | ngrams/sec 7269.8 | eta 0h2m40s\n",
            "| epoch 1 | step 47000/104428 | loss 110.0325 | lr 0.500 | ngrams/sec 7588.0 | eta 0h2m31s\n",
            "| epoch 1 | step 48000/104428 | loss 109.1757 | lr 0.500 | ngrams/sec 7795.0 | eta 0h2m24s\n",
            "| epoch 1 | step 49000/104428 | loss 102.7919 | lr 0.500 | ngrams/sec 7579.1 | eta 0h2m26s\n",
            "| epoch 1 | step 50000/104428 | loss 108.2175 | lr 0.500 | ngrams/sec 7739.1 | eta 0h2m20s\n",
            "| epoch 1 | step 51000/104428 | loss 110.4749 | lr 0.500 | ngrams/sec 7118.8 | eta 0h2m30s\n",
            "| epoch 1 | step 52000/104428 | loss 114.2179 | lr 0.500 | ngrams/sec 7382.6 | eta 0h2m22s\n",
            "| epoch 1 | step 53000/104428 | loss 104.9953 | lr 0.500 | ngrams/sec 7436.3 | eta 0h2m18s\n",
            "| epoch 1 | step 54000/104428 | loss 114.6862 | lr 0.500 | ngrams/sec 7065.5 | eta 0h2m22s\n",
            "| epoch 1 | step 55000/104428 | loss 127.7003 | lr 0.500 | ngrams/sec 7240.7 | eta 0h2m16s\n",
            "| epoch 1 | step 56000/104428 | loss 123.2187 | lr 0.500 | ngrams/sec 7501.0 | eta 0h2m9s\n",
            "| epoch 1 | step 57000/104428 | loss 117.8465 | lr 0.500 | ngrams/sec 7590.9 | eta 0h2m4s\n",
            "| epoch 1 | step 58000/104428 | loss 122.3217 | lr 0.500 | ngrams/sec 7797.6 | eta 0h1m59s\n",
            "| epoch 1 | step 59000/104428 | loss 121.8184 | lr 0.500 | ngrams/sec 7713.2 | eta 0h1m57s\n",
            "| epoch 1 | step 60000/104428 | loss 118.5806 | lr 0.500 | ngrams/sec 7767.3 | eta 0h1m54s\n",
            "| epoch 1 | step 61000/104428 | loss 126.3658 | lr 0.500 | ngrams/sec 7710.7 | eta 0h1m52s\n",
            "| epoch 1 | step 62000/104428 | loss 119.9904 | lr 0.500 | ngrams/sec 7018.1 | eta 0h2m0s\n",
            "| epoch 1 | step 63000/104428 | loss 125.6733 | lr 0.500 | ngrams/sec 7669.5 | eta 0h1m48s\n",
            "| epoch 1 | step 64000/104428 | loss 117.7904 | lr 0.500 | ngrams/sec 7499.8 | eta 0h1m47s\n",
            "| epoch 1 | step 65000/104428 | loss 118.9181 | lr 0.500 | ngrams/sec 6899.5 | eta 0h1m54s\n",
            "| epoch 1 | step 66000/104428 | loss 125.8600 | lr 0.500 | ngrams/sec 6806.7 | eta 0h1m52s\n",
            "| epoch 1 | step 67000/104428 | loss 120.8707 | lr 0.500 | ngrams/sec 6704.0 | eta 0h1m51s\n",
            "| epoch 1 | step 68000/104428 | loss 119.6865 | lr 0.500 | ngrams/sec 7330.4 | eta 0h1m39s\n",
            "| epoch 1 | step 69000/104428 | loss 112.9340 | lr 0.500 | ngrams/sec 7697.4 | eta 0h1m32s\n",
            "| epoch 1 | step 70000/104428 | loss 108.1362 | lr 0.500 | ngrams/sec 7097.0 | eta 0h1m37s\n",
            "| epoch 1 | step 71000/104428 | loss 107.3835 | lr 0.500 | ngrams/sec 7064.4 | eta 0h1m34s\n",
            "| epoch 1 | step 72000/104428 | loss 107.6792 | lr 0.500 | ngrams/sec 7358.8 | eta 0h1m28s\n",
            "| epoch 1 | step 73000/104428 | loss 118.4838 | lr 0.500 | ngrams/sec 7278.6 | eta 0h1m26s\n",
            "| epoch 1 | step 74000/104428 | loss 107.3977 | lr 0.500 | ngrams/sec 7536.8 | eta 0h1m20s\n",
            "| epoch 1 | step 75000/104428 | loss 115.8140 | lr 0.500 | ngrams/sec 7075.8 | eta 0h1m23s\n",
            "| epoch 1 | step 76000/104428 | loss 120.9645 | lr 0.500 | ngrams/sec 7818.6 | eta 0h1m12s\n",
            "| epoch 1 | step 77000/104428 | loss 123.5510 | lr 0.500 | ngrams/sec 7089.9 | eta 0h1m17s\n",
            "| epoch 1 | step 78000/104428 | loss 112.5402 | lr 0.500 | ngrams/sec 7160.7 | eta 0h1m13s\n",
            "| epoch 1 | step 79000/104428 | loss 125.1879 | lr 0.500 | ngrams/sec 7638.7 | eta 0h1m6s\n",
            "| epoch 1 | step 80000/104428 | loss 123.1129 | lr 0.500 | ngrams/sec 6819.5 | eta 0h1m11s\n",
            "| epoch 1 | step 81000/104428 | loss 120.5694 | lr 0.500 | ngrams/sec 7243.6 | eta 0h1m4s\n",
            "| epoch 1 | step 82000/104428 | loss 122.1560 | lr 0.500 | ngrams/sec 7570.2 | eta 0h0m59s\n",
            "| epoch 1 | step 83000/104428 | loss 120.9084 | lr 0.500 | ngrams/sec 7739.2 | eta 0h0m55s\n",
            "| epoch 1 | step 84000/104428 | loss 117.2108 | lr 0.500 | ngrams/sec 7604.3 | eta 0h0m53s\n",
            "| epoch 1 | step 85000/104428 | loss 117.7974 | lr 0.500 | ngrams/sec 7652.9 | eta 0h0m50s\n",
            "| epoch 1 | step 86000/104428 | loss 128.1819 | lr 0.500 | ngrams/sec 7539.2 | eta 0h0m48s\n",
            "| epoch 1 | step 87000/104428 | loss 126.7088 | lr 0.500 | ngrams/sec 7694.2 | eta 0h0m45s\n",
            "| epoch 1 | step 88000/104428 | loss 118.2064 | lr 0.500 | ngrams/sec 7517.7 | eta 0h0m43s\n",
            "| epoch 1 | step 89000/104428 | loss 114.7455 | lr 0.500 | ngrams/sec 7408.3 | eta 0h0m41s\n",
            "| epoch 1 | step 90000/104428 | loss 113.0241 | lr 0.500 | ngrams/sec 7128.2 | eta 0h0m40s\n",
            "| epoch 1 | step 91000/104428 | loss 115.3608 | lr 0.500 | ngrams/sec 7356.8 | eta 0h0m36s\n",
            "| epoch 1 | step 92000/104428 | loss 121.5950 | lr 0.500 | ngrams/sec 7693.8 | eta 0h0m32s\n",
            "| epoch 1 | step 93000/104428 | loss 114.6764 | lr 0.500 | ngrams/sec 7646.6 | eta 0h0m29s\n",
            "| epoch 1 | step 94000/104428 | loss 111.8035 | lr 0.500 | ngrams/sec 7399.0 | eta 0h0m28s\n",
            "| epoch 1 | step 95000/104428 | loss 109.1721 | lr 0.500 | ngrams/sec 7430.0 | eta 0h0m25s\n",
            "| epoch 1 | step 96000/104428 | loss 116.2492 | lr 0.500 | ngrams/sec 7562.4 | eta 0h0m22s\n",
            "| epoch 1 | step 97000/104428 | loss 118.7880 | lr 0.500 | ngrams/sec 6991.1 | eta 0h0m21s\n",
            "| epoch 1 | step 98000/104428 | loss 122.2738 | lr 0.500 | ngrams/sec 7061.0 | eta 0h0m18s\n",
            "| epoch 1 | step 99000/104428 | loss 125.8374 | lr 0.500 | ngrams/sec 7669.0 | eta 0h0m14s\n",
            "| epoch 1 | step 100000/104428 | loss 124.4375 | lr 0.500 | ngrams/sec 7622.0 | eta 0h0m11s\n",
            "| epoch 1 | step 101000/104428 | loss 114.8078 | lr 0.500 | ngrams/sec 6937.6 | eta 0h0m9s\n",
            "| epoch 1 | step 102000/104428 | loss 112.4657 | lr 0.500 | ngrams/sec 7494.9 | eta 0h0m6s\n",
            "| epoch 1 | step 103000/104428 | loss 123.2981 | lr 0.500 | ngrams/sec 7627.7 | eta 0h0m3s\n",
            "| epoch 1 | step 104000/104428 | loss 126.7087 | lr 0.500 | ngrams/sec 7577.1 | eta 0h0m1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 209/10879 [00:00<00:05, 2084.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating on validation set...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10879/10879 [00:05<00:00, 2077.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time 287.30s | valid loss 116.62 | valid ppl      inf\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 2 | step 1000/104428 | loss 116.6240 | lr 0.500 | ngrams/sec 2213.3 | eta 0h15m34s\n",
            "| epoch 2 | step 2000/104428 | loss 119.9842 | lr 0.500 | ngrams/sec 6895.8 | eta 0h4m57s\n",
            "| epoch 2 | step 3000/104428 | loss 116.8125 | lr 0.500 | ngrams/sec 7081.4 | eta 0h4m46s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 204/12275 [00:00<00:05, 2039.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n",
            "Evaluating on test set...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12275/12275 [00:06<00:00, 1970.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss 126.30 | test ppl      inf\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jUqz8keSi6Mi",
        "outputId": "8a1db53f-9d08-4458-861b-21505fafc6f0"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint.pth')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2f9d0e5e-a3f8-4422-b029-6d07167a38a5\", \"checkpoint.pth\", 1043084)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbeYK7MGwQbE"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}