

AN ARTIFICIAL NEURAL NETWORK FOR SPATIOTEMPORAL BIPOLAR PATTERNS: APPLICATION TO
PHONEME CLASSIFICATION
Toshiteru Homma
Les E. Atlas
Robert J. Marks II

Interactive Systems Design Laboratory
Department of Electrical Engineering, Ff-l0
University of Washington
Seattle, Washington 98195

ABSTRACT
An artificial neural network is developed to recognize spatio-temporal
bipolar patterns associatively. The function of a formal neuron is generalized by
replacing multiplication with convolution, weights with transfer functions, and
thresholding with nonlinear transform following adaptation. The Hebbian learning rule and the delta learning rule are generalized accordingly, resulting in the
learning of weights and delays. The neural network which was first developed
for spatial patterns was thus generalized for spatio-temporal patterns. It was
tested using a set of bipolar input patterns derived from speech signals, showing
robust classification of 30 model phonemes.

1. INTRODUCTION
Learning spatio-temporal (or dynamic) patterns is of prominent importance in biological
systems and in artificial neural network systems as well. In biological systems, it relates to such
issues as classical and operant conditioning, temporal coordination of sensorimotor systems and
temporal reasoning. In artificial systems, it addresses such real-world tasks as robot control,
speech recognition, dynamic image processing, moving target detection by sonars or radars, EEG
diagnosis, and seismic signal processing.
Most of the processing elements used in neural network models for practical applications
have been the formal neuron l or" its variations. These elements lack a memory flexible to temporal patterns, thus limiting most of the neural network models previously proposed to problems
of spatial (or static) patterns. Some past solutions have been to convert the dynamic problems to
static ones using buffer (or storage) neurons, or using a layered network with/without feedback.
We propose in this paper to use a "dynamic formal neuron" as a processing element for
learning dynamic patterns. The operation of the dynamic neuron is a temporal generalization of
the formal neuron. As shown in the paper, the generalization is straightforward when the activation part of neuron operation is expressed in the frequency domain. Many of the existing learning rules for static patterns can be easily generalized for dynamic patterns accordingly. We show
some examples of applying these neural networks to classifying 30 model phonemes.

? American Institute of Physics 1988

32

2. FORMAL NEURON AND DYNAMIC FORMAL NEURON
The formal neuron is schematically drawn in Fig. l(a), where

r = [Xl Xz ... xd 1
Yi' i = 1,2?... ?N
Zi, i = 1,2. . . . ?N

Input
Activation
Output
Transmittance
Node operator
Neuron operation

W

= [Wil

WiZ ... wiLf

11 where 11(') is a nonlinear memory less transform
Zi

= 11(wTr>

(2.1)

Note that a threshold can be implicitly included as a transmittance from a constant input.
In its original form of formal neuron, Xi E {O,I} and 110 is a unit step function u ('). A
variation of it is a bipolar formal neuron where Xi E {-I, I} and 110 is the sign function sgn O.
When the inputs and output are converted to frequency of spikes, it may be expressed as
Xi E Rand 110 is a rectifying function rO. Other node operators such as a sigmoidal function
may be used.
We generalize the notion of formal neuron so that the input and output are functions of
time. In doing so, weights are replaced with transfer functions, multiplication with convolution,
and the node operator with a nonlinear transform following adaptation as often observed in biological systems.
Fig. 1(b) shows a schematic diagram of a dynamic formal neuron where
r(l) = [Xl(t) xz(t) ... xdt)f
Yi(t), i == 1,2?... . N
Zi(t), i = 1,2?... ?N
w(t) = [Wjl(t) wiZ(t) ... WiL(t)]T
ai (t)

Input
Activation
Output
Transfer function
Adaptation
Node operator
Neuron operation

1l where 110 is a nonlinear memoryless transform
Zj(t)

=ll(ai (-t). W;(t)T .x(t?

(2.2)

For convenience, we denote ? as correlation instead of convolution. Note that convolving a(t)
with b(t) is equivalent to correlating a( -t) with b(t).
If the Fourier transforms r(f)=F{r(t)}, w;(f)=F{W;(t)}, Yj(f)=F{Yi(t)}, and
aj(f) = F {ai(t)} exist, then
Yi (f)

= ai (f)

[Wi (f

fT

r(f)]

(2.3)

where Wi (f fT is the conjugate transpose of Wi (t).
x,(1)

I----zt

1----zt(I)

?

(b)

Fig. 1. Formal Neuron and Dynamic Formal Neuron.

33

3. LEARNING FOR FORMAL NEURON AND DYNAMIC FORMAL NEURON
A number of learning rules for formal neurons has been proposed in the past. In the following paragraphs, we formulate a learning problem and describe two of the existing learning
rules, namely, Hebbian learning and delta learning, as examples.
Present to the neural network M pairs of input and desired output samples
k ::;: 1,2, ... ,M , in order. Let W(k)::;: [w/k) w!k) '" wJk~T where wr) is the
transmittance vector at the k-th step of learning. Likewise, let
{X<k), (lk)},

K(k) = [X<I) x'-2)

... X<k)], r(k)

~(k)

= [z<I) z<2)

... ~k)],

'Ik)

= W(k)x'-k),

z<k)

and

= rfl) t 2) ...
D(k) = [(ll) (l2)

t k)],
'"

(lk)] ,

where

= n<tk?,

and

n<Y> = [T1(Y I) T1(Y2) .. . T1(yN)]T.

The Hebbian learning rule 2 is described as follows *:
W(k) ::;: W(k-I) + a;JC.k)X<k)T

(3.1)

The delta learning (or LMS learning) rule 3, 4 is described as follows:
W(k)

= W(k-I) _

o.{W(k-l)t:k ) _ (lk)}X<k)T

(3.2)

The learning rules described in the previous section are generalized for the dynamic formal
neuron by replacing multiplication with correlation. First, the problem is reformulated and then
the generalized rules are described as follows.
Present to the neural network M pairs of time-varing input
= 1,2, .. . ,M , in order. Let W(k)(t) = [WI(t)(k)(t)
where w/k)(t) is the vector whose elements W;)t)(t) are transfer functions
to the neuron i at the k-th step of learning. The Hebbian learning rule for
then
{X<k)(t), (lk)(t)), k

W(kl(t)

= W(k-I)(t) + 0.(-1}. (lk)(t). X<k)(t)T

and output samples
w~k)(t)?? . wJk)(t)f
connecting the input j
the dynamic neuron is

.

(3.3)

The delta learning rule for dynamic neuron is then
W(kl(t) ::;: W(k-I)(t) - o.(-t). {W(k-Il(t). X<k)(t) - (It)(t)} .X<k)(t)T .

(3.4)

This generalization procedure can be applied to other learning rules in some linear discriminant systems 5 , the self-organizing mapping system by Kohonen6 , the perceptron 7 , the backpropagation model 3 , etc. When a system includes a nonlinear operation, more careful analysis
is necesssay as pointed out in the Discussion section.
4. DELTA LEARNING,PSEUDO INVERSE AND REGULARIZATION
This section reviews the relation of the delta learning rule to the pseudo-inverse and the
technique known as regularization. 4, 6, 8, 9,10
Consider a minimization problem as described below: Find W which minimizes
R

= LII'Ik) -

(lk)U

i = <f-k) -

(lky <tk) - (lk?

(4.1)

subject to t k ) = WX<k) ?
A solution by the delta rule is, using a gradient descent method,
W(k)

-

= W(k-I) _ o.-1... R (k)
aw

? This interpretation assumes a strong supervising signal at the output while learning.

(4.2)

34

where R (k) = II y<k) ... ~A:)1I1. The minimum norm solution to the problem, W*, is unique and
can tie expressed as

W* == D xt

(4.3)

where !. t is the Moore-Penrose pseudo-inverse of!. , i.e.,

X t = lim(XTX + dl/)-lX T = limXT (X XT
-

a-.o -

-

-

On the condition that 0 <

-

a-+O-

- -

+ dl/)-l.
-

(4.4)

a < ~ where An- is the max.imum eigenvalue of !.T!., J'.k) and

(jC.k) are independent, and WCl) is uncorrelated with ~l),

E {W*}

=E (~c..)}

(4.5)

where E {x} denotes the expected value of x. One way to make use of this relation is to calculate W* for known standard data and refine it by (4.2), thereby saving time in the early stage of
learning.
However, this solution results in an ill-conditioned W often in practice. When the problem is ill-posed as such, the technique known as regularization can alleviate the ill-conditioning
of W . The problem is reformulated by finding W which minimizes

R(a)

= Dly<t) -

(jC.k)IIl

+ dlLII wkll 1

1

(4.6)

k

t =

subject to k ) ~k) where W = [Wlw2 ... WN]T .
This reformulation regularizes (4.3) to
W (a) =

D!.T (!.!.T + a2n-1

(4.7)

which is statistically equivalent to Wc..) when the input has an additive noise of variance dl
utlcorrelated with ~l). Interestingly, the leaky LMS algorithm ll leads to a statistically
equivalent solution
W(l)

= ~WCk-l) _ tx~(k-l)~l) -

whete 0 < ~ < 1 and 0 <

E {W(a)}
if dl =

(4.8)

{jC.l)};f<l)T

2

a < Amax ? These solutions are related as

=E {Wc..)}

(4.9)

..!::J!
when WCl) is uncorrelated with ;f<l) .11
a
-

Equation (4.8) can be generalized for a network using dynamic formal neurons, resulting in
a equation similar to (3.4). Making use of (4.9), (4.7) can be generalized for a dynamic neuron
network as
W (t ; a) = F- 1{Q if )!. if fT (!. if )!. if)CT

n-

+ a2

1}

(4.10)

where F-1 denotes the inverse Fourier transform.

s. SYNTHESIS OF BIPOLAR PHONEME PATTERNS
This section illustrates the scheme used to synthesize bipolar phoneme patterns and to
form prototype and test patterns.
The fundamental and first three formant frequencies, along with their bandwidths, of
phonemes provided by Klatt l2 were taken as parameters to synthesize 30 prototype phoneme patterns. The phonemes were labeled as shown in Table 1. An array of L (=100) input neurons
OOVered the range of 100 to 4000 Hz. Each neuron had a bipolar state which was +1 only when
one of the frequency bands in the phoneme presented to the network was within the critical band

35
of the neuron and -1 otherwise. The center frequencies if e) of critical bands were obtained by
dividing the 100 to 4000 Hz range into a log scale by L. The critical bandwidth was a constant
100 Hz up to the center frequency Ie = 500 Hz and 0.2/e Hz when Ie >500 Hz.13
The parameters shown in Table 1 were used to construct Table 1. Labels of Phonemes
30 prototype phoneme patterns. For 9. it was constructed as a
Label
Phoneme
combination of t and 9. Fl. F 2 .F 3 were the first. second. and
1
[i Y]
third formants. and B I' B 2. and B 3. were corresponding
[Ia]
2
bandwidths. The fundamental frequency F 0 = 130 Hz with B 0 =
3
leY]
10 Hz was added when the phoneme was voiced. For plosives.
4
[Ea ]
there was a stop before formant traces start. The resulting bipo[3e']
5
lar patterns are shown in Fig.2. Each pattern had length of 5
6
[el]
time units, composed by linearly interpolating the frequencies
7
[~]
when the formant frequency was gliding.
8
[It ]
A sequence of phonemes converted from a continuous
[ow]
9
pronunciation of digits, {o, zero, one, two, three. four, five, six.
10
[\I~]
seven, eight, nine }, was translated into a bipolar pattern, adding
11
[u w]
12
two time units of transition between two consequtive phonemes
[a;J
13
[a ]
by interpolating the frequency and bandwidth parameters
[aWl
14
linearly. A flip noise was added to the test pattern and created a
15
loY]
noisy test pattern. The sign at every point in the original clean
16
[w]
test pattern was flipped with the probability 0.2. These test pat17
[y]
terns are shown in Fig. 3.
18
[r]
19
[I]
20
[f]
21
[v]
I'IlDM_ Label I 1 5 7 , II Il 15 .7 ., JI 21 Z5 17 It
2 4 , I II 11 14 16 II II U 14 I' II JO
22
[9]
II.
23
[\]
24
[s]
25
[z]
26
[p]
27
[t]
28
[d]
29
[k]
30
[n]

Fig. 2. Prototype Phoneme Patterns. (Thirty phoneme patterns are shown
in sequence with intervals of two time units.)

6. SIMULATION OF SPATIO-TEMPORAL FILTERS FOR PHONEME CLASSIFICATION
The network system described below was simulated and used to classify the prototype
phoneme patterns in the test patterns shown in the previoius section. It is an example of generalizing a scheme developed for static patterns 13 to that for dynamic patterns. Its operation is
in two stages. The first stage operation is a spatio-temporal filter bank:

36

?

!!

z .4

:!

e=

~

!

?

?

I
?

'

,

I

if

"I '
~..

I '

,

lU

'I

U'

(b)

(a)

Fig. 3. Test Patterns. (a) Clean Test Pattern. (b) Noisy Test Pattern.
(6.1)

1(t) = W(t).r(t) , and r(t) = !:(a(-t)y(t? .

The second stage operation is the "winner-take-all" lateral inhibition:
(/(t) = zt(t) , and (/(t+A) = !:(~(-t).(/(t) -

Ii),

(6.2)

and
A(t) = (1

-

114
+ -)/O(t) - -S"fiI' 2,O(t-nA).
SN -

N

(6.3)

11=0

where Ii is a constant threshold vector with elements hi = h and 0(.) is the Kronecker delta
function. This operation is repeated a sufficient number of times, No .13,14 The output is
(/(t + No ?A).
Two models based on different leaming rules were simulated with parameters shown
below.

Model 1 (Spatio-temporal Matched Filter Bank)
Let a(t) = O(t) , (/tk) = et in (3.3) where ek is a unit vector with its elements eki = O(k-i) .
(6.4)

W(t)=!(t)T.
4 1
h=200, and a(t) = 2,-O(t-nA).
11=0 S

Model 2 (Spatio-temporal Pseudo-inverse Filter)
Let D

=L in (4.10). Using the alternative expression in (4.4),
W (t) = F- 1{(! (j fT! (j) + cr2n-lXCT}.
h = O.OS ,cr2 = 1000.0, and a(t)

(6.5)

= O(t).

This minimizes
R (cr,!)

= DI1k )(j) "

(/t")(j )lIi + cr 22,11
k

w" if )lIi

for all ! .

(6.6)

37

Because the time and frequency were finite and discrete in simulation, the result of the
inverse discrete Fourier transform in (6.5) may be aliased. To alleviate the aliasing, the transfer
functions in the prototype matrix:! (t) were padded with zeros, thereby doubling the lengths.
Further zero-padding the transfer functions did not seem to change teh result significantly.
The results are shown in Fig. 4(a)-(d). The arrows indicate the ideal response positions at
the end of a phoneme. When the program was run with different thresholds and adaptation function a (t), the result was not very sensitive to the threshold value, but was, nevertheless affected
by the choice of the adaptation function. The maximum number of iterations for the lateral inhibition network to converge was observed: for the experiments shown in Fig. 4(a) - (d), the
numbers were 44, 69, 29, and 47, respectively. Model 1 missed one phoneme and falsely
responded once in the clean test pattern. It missed three and had one false response in the noisy
test pattern. Model 2 correctly recognized all phonemes in the clean test pattern, and falsealarmed once in the noisy test pattern.

7. DISCUSSION
The notion of convolution or correlation used in the models presented is popular in
engineering disciplines and has been applied extensively to designing filters, control systems, etc.
Such operations also occur in biological systems and have been applied to modeling neural networks. IS ,16 Thus the concept of dynamic formal neuron may be helpful for the improvement of
artificial neural network models as well as the understanding of biological systems. A portion of
the system described by Tank and Hopfield 11 is similar to the matched filter bank model simulated in this paper.
The matched filter bank model (Modell) performs well when all phonemes (as above) are
of the same duration. Otherwise, it would perform poorly unless the lengths were forced to a
maximum length by padding the input and transfer functions with -1' s during calculation. The
pseudo-inverse filter model, on the other hand, should not suffer from this problem. However,
this aspect of the 11KXlel (Model 2) has not yet been explicitly simulated.
Given a spatio-temporal pattern of size L x K, i.e., L spatial elements and K temporal elements, the number of calculations required to process the first stage of filtering by both models is
the same as that by a static formal neuron network in which each neuron is connected to the L x
K input elements. In both cases, L x K multiplications and additions are necessary to calculate
one output value. In the case of bipolar patterns, the rnutiplication used for calculation of activation can be replaced by sign-bit check and addition. A future investigation is to use recursive
filters or analog filters as transfer functions for faster and more efficient calculation. There are
various schemes to obtain optimal recursive or analog filters.t 8,19 Besides the lateral inhibition
scheme used in the models, there are a number of alternative procedures to realize a "winnertake-all" network in analog or digital fashion. IS, 20, 21
As pointed out in the previous section, the Fourier transform in (6.5) requires a precaution
concerning the resulting length of transfer functions. Calculating the recursive correlation equation (3.4) also needs such preprocessing as windowing or truncation. 22
The generalization of static neural networks to dynamic ones along with their learning
rules is strainghtforward as shown if the neuron operation and the learning rule are linear. Generalizing a system whose neuron operation and/or learning rule are nonlinear requires more careful analysis and remains for future work. The system described by Watrous and Shastri l6 is an
example of generalizing a backpropagation model. Their result showed a good potential of the
model and a need for more rigorous analysis of the model. Generalizing a system with recurrent
connections is another task to be pursued. In a system with a certain analytical nonlinearity, the
signals are expressed by Volterra functionals, for example. A practical learning system can then
be constructed if higher kernels are neglected. For example, a cubic function can be used instead
of a sigmoidal function.

38

1'1

3.

0-{'-r.

1\
~

j"--

~

;~.
1\

U

--{.

!

e

(a)

~

z

~
0

'\
.t

?f-t

7\

-

-.

?

?

I

I

, I

I

I

I

I
IS.

t ..

51

I

I

en

TIme

"t

~

l~

~
~
~7

!.

!

1

1

~

e
Ii

(b)

z

";

.:-

~

~

?

1.
l

?

?

j

r--

I

u

I

t ..

I
lSI

tu

TIme

Fig. 4. Performance of Models. (a) Modell with Clean Test Pattern. (b)
Model 2 with Clean Test Pattern. (c) Modell with Noisy Test Pattern.
(d) Model 2 with Noisy Test Pattern. Arrows indicate the ideal response
positions at the end of phoneme.
8. CONCLUSION
The formal neuron was generalized to the dynamic formal neuron to recognize spatiotemporal patterns. It is shown that existing learning rules can be generalized for dynamic formal
neurons.
An artificial neural network using dynamic formal neurons was applied to classifying 30
model phonemes with bipolar patterns created by using parameters of formant frequencies and
their bandwidths. The model operates in two stages: in the first stage, it calculates the correlation between the input and prototype patterns stored in the transfer function matrix, and, in the
second stage, a lateral inhibition network selects the output of the phoneme pattern close to the
input pattern.

39

---{'.-\

3.

1"'? j

,--; '

at

i!!

e

(C)

zii

~

C

It
!"

P,
X

?

?

I

I

I

I

I

t ..

51

u.

t51

nme
.~

3.

"
I

~0
.'--~

u

'1

"?

i!!

e
ii

(d)

z

,..

~

C

?

It

?

I

I

,

?
Fig. 4 (continued.)

Two models with different transfer functions were tested. Model 1 was a matched filter
bank model and Model 2 was a pseudo-inverse filter model. A sequence of phoneme patterns
corresponding to continuous pronunciation of digits was used as a test pattern. For the test pattern, Modell missed to recognize one phoneme and responded falsely once while Model 2
correctly recognized all the 32 phonemes in the test pattern. When the flip noise which flips the
sign of the pattern with the probability 0.2, Model 1 missed three phonemes and falsely
responded once while Model 2 recognized all the phonemes and false-alarmed once. Both
models detected the phonerns at the correct position within the continuous stream.
References
1.

W. S. McCulloch and W. Pitts, "A logical calculus of the ideas imminent in nervous
activity," Bulletin of Mathematical Biophysics, vol. 5, pp. 115-133, 1943.

2.

D. O. Hebb, The Organization of Behavior, Wiley, New York, 1949.

40

3.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning internal representations by
error propagation," in Parallel Distributed Processing. Vol. 1, MIT, Cambridge, 1986.

4.

B. Widrow and M. E. Hoff, "Adaptive switching circuits," Institute of Radio Engineers.
Western Electronics Show and Convention, vol. Convention Record Part 4, pp. 96-104,
1960.

5.

R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis. Chapter 5, Wiley,
New York, 1973.

6.

T. Kohonen, Self-organization and Associative Memory, Springer-Verlag, Berlin, 1984.

7.

F. Rosenblatt, Principles of Neurodynamics, Spartan Books, Washington, 1962.

8.

1. M. Varah, "A practical examination of some numerical methods for linear discrete illposed problems," SIAM Review, vol. 21, no. 1, pp. 100-111, 1979.

9.

C. Koch, J. Marroquin, and A. Y uiIle, "Analog neural networks in early vision," Proceedings of the National Academy of Sciences. USA, vol. 83, pp. 4263-4267, 1986.

10.

G. O. Stone, "An analysis of the delta rule and the learning of statistical associations," in
Parallel Distributed Processing .? Vol. 1, MIT, Cambridge, 1986.

11.

B. Widrow and S. D. Stearns, Adaptive Signal Processing, Prentice-Hall, Englewood
Cliffs, 1985.

12.

D. H. Klatt, "Software for a cascade/parallel formant synthesizer," Journal of Acoustical
Society of America, vol. 67, no. 3, pp. 971-995, 1980.

13.

L. E. Atlas, T. Homma, and R. J. Marks II, "A neural network for vowel classification,"

Proceedings International Conference on Acoustics. Speech. and Signal Processing, 1987.
14.

R. P. Lippman, "An introduction to computing with neural nets," IEEE ASSP Magazine,
April, 1987.

15.

S. Amari and M. A. Arbib, "Competition and cooperation in neural nets," in Systems Neuroscience, ed. J. Metzler, pp. 119-165, Academic Press, New York, 1977.

16.

R. L. Watrous and L. Shastri, "Learning acoustic features from speech data using connectionist networks," Proceedings of The Ninth Annual Conference of The Cognitive Science
Society, pp. 518-530, 1987.

17.

D. Tank and J. J. Hopfield, "Concentrating information in time: analog neural networks
with applications to speech recognition problems," Proceedings of International Conference on Neural Netoworks, San Diego, 1987.

18.

J. R. Treichler, C. R. Johnson,Jr., and M. G. Larimore, Theory and Design of Adaptive
Filters. Chapter 5, Wiley, New York, 1987.

19.

M Schetzen, The Volterra and Wiener Theories of Nonlinear Systems. Chapter 16, Wiley,
New York, 1980.

20.

S. Grossberg, "Associative and competitive principles of learning," in Competition and
Cooperation in Neural Nets, ed. M. A. Arbib, pp. 295-341, Springer-Verlag, New York,
1982.

21.

R. J. Marks II, L. E. Atlas, J. J. Choi, S. Oh, K. F. Cheung, and D. C. Park, "A performance analysis of associative memories with nonlinearities in the correlation domain,"
(submitted to Applied Optics), 1987.

22.

D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing, pp.
230-234, Prentice-Hall, Englewood Cliffs, 1984.

