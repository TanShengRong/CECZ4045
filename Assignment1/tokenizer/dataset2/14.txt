
ENCODING GEOMETRIC INVARIANCES IN
HIGHER-ORDER NEURAL NETWORKS
C.L. Giles
Air Force Office of Scientific Research, Bolling AFB, DC 20332
R.D. Griffin
Naval Research Laboratory, Washington, DC

20375-5000

T. Maxwell
Sachs-Freeman Associates, Landover, MD 20785
ABSTRACT
We describe a method of constructing higher-order neural
networks that respond invariantly under geometric transformations on
the input space. By requiring each unit to satisfy a set of
constraints on the interconnection weights, a particular structure is
imposed on the network. A network built using such an architecture
maintains its invariant performance independent of the values the
weights assume, of the learning rules used, and of the form of the
nonlinearities in the network. The invariance exhibited by a firstorder network is usually of a trivial sort, e.g., responding only to
the average input in the case of translation invariance, whereas
higher-order networks can perform useful functions and still exhibit
the invariance. We derive the weight constraints for translation,
rotation, scale, and several combinations of these transformations,
and report results of simulation studies.
INTRODUCTION
A persistent difficulty for pattern recognition systems is the
requirement that patterns or objects be recognized independent of
irrelevant parameters or distortions such as orientation (position,
rotation, aspect), scale or size, background or context, doppler
shift, time of occurrence, or signal duration. The remarkable
performance of humans and other animals on this problem in the visual
and auditory realms is often taken for granted, until one tries to
build a machine with similar performance. Thoufh many methods have
been developed for dealing with these problems, we have classified
them into two categories: 1) preprocessing or transformation
(inherent) approaches, and 2) case-specific or "brute force"
(learned) approaches. Common transformation techniques include:
Fourier, Hough, and related transforms; moments; and Fourier
descriptors of the input signal. In these approaches the signal is
usually transformed so that the subsequent processing ignores
arbitrary parameters such as scale, translation, etc. In addition,
these techniques are usually computationally expensive and are
sensitive to noise in the input signal. The "brute force" approach
is exemplified by training a device, such as a perceptron, to
classify a pattern independent of it's position by presenting the

@

American Institute of Physics 1988

302

training pattern at all possible positions. MADALINE machines 2 have
been shown to perform well using such techniques. Often, this type
of invariance is pattern specific, does not easily generalize to
other patterns, and depends on the type of learning algorithm
employed. Furthermore, a great deal of time and energy is spent on
learning the invariance, rather than on learning the signal. We
describe a method that has the advantage of inherent invariance but
uses a higher-order neural network approach that must learn only the
desired signal. Higher-order units have been shown to have unique
computational strengths and are quite amenable to the encoding of a
priori know1edge. 3 - 7
MATHEMATICAL DEVELOPMENT
Our approach is similar to the group invariance approach,8,10
although we make no appeal to group theory to obtain our results. We
begin by selecting a transformation on the input space, then require
the output of the unit to be invariant to the transformation. The
resulting equations yield constraints on the interconnection weights,
and thus imply a particular form or structure for the network
architecture.
For the i-th unit Yi of order M defined on a discrete input
space, let the output be given by
Yi[YiM(X),P(x)] - f( WiO + ~ Wi 1 (X1) P(x1)

+ ~~ Wi 2 (X1,X2) P(x1) P(x2) + ...
+~ ... ~ Wi M(X1,? ?XM) P(x1)? ?P(XM) ),

(1)

where p(x) is the input pattern or signal function (sometimes called
a pixel) evaluated at position vector x, wim(xl, ... Xm) is the weight
of order m connecting the outputs of units at Xl, x2, .. Xm to the ith unit, i.e., it correlates m values, f(u) is some threshold or
sigmoid output function, and the summations extend over the input
space. YiM(X) represents the entire set of weights associated with
the i-th unit. These units are equivalent to the sigma-pi units a
defined by Rumelhart, Hinton, and Williams. 7 Systems built from
these units suffer from a combinatorial explosion of terms, hence are
more complicated to build and train. To reduce the severity of this
problem, one can limit the range of the interconnection weights or
the number of orders, or impose various other constraints. We find
that, in addition to the advantages of inherent invariance, imposing
an invariance constraint on Eq. (1) reduces the number of allowed
aThe sigma-pi neural networks are multi-layer networks with
higher-order terms in any layer. As such, most of the neural
networks described here can be considered as a special case of the
sigma-pi units. However, the sigma-pi units as originally formulated
did not have invariant weight terms, though it is quite simple to
incorporate such invariances in these units.

303

weights, thus simplifying the architecture and shortening the
training time.
We now define what we mean by invariance. The output of a unit
is invariant with respect to the transformation T on the input
pattern if 9
(2)

An example of the class of invariant response defined by Eq. (2)
would be invariant detection of an object in the receptive field of a
panning or zooming camera. An example of a different class would be
invariant detection of an object that is moving within the field of a
fixed camera. One can think of this latter case as consisting of a
fixed field of "noise" plus a moving field that contains only the
object of interest. If the detection system does not respond to the
fixed field, then this latter case is included in Eq. (2).
To illustrate our method we derive the weight constraints for
one-dimensional translation invariance. We will first switch to a
continuous formulation, however, for reasons of simplicity and
generality, and because it is easier to grasp the physical
significance of the results, although any numerical simulation
requires a discrete formulation and has significant implications for
the implementation of our results. Instead of an index i, we now
keep track of our units with the continuous variable u. With these
changes Eq. (2) now becomes
y[u;wM(x),p(X)] = f( wO + JrdXl Wl(U;Xl) P(xl) + ...

+

f?? Jr dXl? .dXM

wM(U;Xl,? ?XM) P(Xl)? .P(XM) ),

(3)

The limits on the integrals are defined by the problem and are
crucial in what follows. Let T be a translation of the input pattern
by -xO, so that

(4)

T[p(x)] - p(x+XO)
where xo is the translation of the input pattern.
Ty[u;wM(x) ,p(x)] - y[u;YM(x),p(x+XO?)

=

Then, from eq (2),

y[u;wM(x),p(x)]

(5)

Since p(x) is arbitrary we must impose term-by-term equality in the
argument of the threshold function; i.e.,

f dXl Wl(U;Xl)

P(xl) =

f dxl Wl(U;Xl)

P(xl+XO),

(Sa)

Jr fdxl dX2 W2 (U;Xl,X2) P(xl) P(x2) =
Jr

f dXl

dX2 W2 (U;Xl,X2) P(xl+XO) P(x2+XO),
etc.

(Sb)

304

Making the substitutions xl. xl-XO, x2 ",x2-XO, etc, we find that

f dXl Wl(U;Xl)

f f dxl

P(xl) -

f dxl WI(U;Xl-XO)

P(XI) ,

(6a)

dX2 W2 (U;XI,X2) P(xI) P(x2) -

f f dXI

dX2 W2 (U;XI-XO,X2-XO) P(xI) P(x2),

(6b)

etc.
Note that the limits of the integrals on the right hand side must be
adjusted to satisfy the change-of-variables. If the limits on the
integrals are infinite or if one imposes some sort of periodic
boundary condition, the limits of the integrals on both sides of the
equation can be set equal. We will assume in the remainder of this
paper that these conditions can be met; normally this means the
limits of the integrals extend to infinity. (In an implementation,
it is usually impractical or even impossible to satisfy these
requirements, but our simulation results indicate that these networks
perform satisfactorily even though the regions of integration are not
identical. This question must be addressed for each class of
transformation; it is an integral part of the implementation design.)
Since the functions p(x) are arbitrary and the regions of integration
are the same, the weight functions must be equal. This imposes a
constraint on the functional form of the weight functions or, in the
discrete implementation, limits the allowed connections and thus the
number of weights. In the case of translation invariance, the
constraint on the functional form of the weight functions requires
that
w1(U;XI) - wl(u;X].-XO),
w2(U;XI,X2) - w2(U;XI-XO,X2-XO),

(7a)

(7b)

etc.
These equations imply that the first order weight is independent of
input position, and depends only on the output position u. The
second order weight is a function only of vector differences,IO i.e.,
w1(u;Xj) - J..(u),

(8a)

w2(U;X].,X2) - w2(u:X]. -Xl)?

(8b)

For a discrete implementation with N input units (pixels) fully
connected to an output unit, this requirement reduces the number of
second-order weights from order N2 to order N, i.e., only weights for
differences of indexes are needed rather than all unique pair
combinations. Of course, this advantage is multiplied as the number
of fully-connected output units increases.

FURTHER EXAMPLES
We have applied these techniques to several other
transformations of interest. For the case of transformation of scale

305

define the scale operator S such that
Sp(x) - aIlp(ax)

(9)

where a is the scale factor, and x is a vector of dimension n. The
factor an is used for normalization purposes, so that a given figure
always contains the same "energy" regardless of its scale.
Application of the same procedure to this transformation leads to the
following constraints on the weights:
wl(u;Xjfa) -= wl(u;~,

w2(u;X1Ia,xv'a) .. w2(u;'X.l.'~)'
w3(u;xlla,x2/a ,x3/a) ... w3(U;X].,X2,X3), etc.

(lOa)
(lOb)
(lOc)

Consider a two-dimensional problem viewed in polar coordinates (r,t).
A set of solutions to these constraints is
J.(u;q,tI) - w1(u;Q),

w2(u;rl,r2;tl,t2) - w2(u;rllr2;tl,t2).
w3 (u;rl,r2,r3;tl,t2,t3) - w3 (u;(rl-r2)/r3;tl,t2,t3).

(lla)
(llb)
(llc)

Note that with increasing order comes increasing freedom in the
selection of the functional form of the weights. Any solution that
satisfies the constraint may be used. This gives the designer
additional freedom to limit the connection complexity, or to encode
special behavior into the net architecture. An example of this is
given later when we discuss combining translation and scale
invariance in the same network.
Now consider a change of scale for a two-dimensional system in
rectangular coordinates, and consider only the second-order weights.
A set of solutions to the weight constraint is:
W2 (U;Xl,Yl;X2,Y2)

-

W2 (U;Xl/Yl;X2/Y2),

- W2 (U;Xl/X2;Yl/Y2),
W2 (U;Xl,Yl;X2,Y2) - w2 (U;(Xl-X2)/(Yl-Y2)), etc.
W2 (U;Xl,Yl;X2,Y2)

(12a)
(l2b)
(12c)

We have done a simulation using the form of Eq. (12b). The
simulation was done using a small input space (8x8) and one output
unit. A simple least-mean-square (back-propagation) algorithm was
used for training the network. When taught to distinguish the
letters T and C at one scale, it distinguished them at changes of
scale of up to 4X with about 15 percent maximum degradation in the
output strength. These results are quite encouraging because no
special effort was required to make the system work, and no
corrections or modifications were made to account for the boundary
condition requirements as discussed near Eq. (6). This and other
simulations are discussed further later.
As a third example of a geometric transformation, consider the
case of rotation about the origin for a two-dimensional space in
polar coordinates. One can readily show that the weight constraints

306

are satisfied if
wl(u;rl,tl) ~ wl(u;rl),
w2(u;rl,r2;tl,t2) - w2(u;rl,r2;tl-t2), etc.

(13a)
(l3b)

These results are reminiscent of the results for translation
invariance. This is not uncommon: seemingly different problems
often have similar constraint requirements if the proper change of
variable is made. This can be used to advantage when implementing
such networks but we will not discuss it further here.
An interesting case arises when one considers combinations of
invariances, e.g., scale and translation. This raises the question
of the effect of the order of the transformations, i.e., is scale
followed by translation equivalent to translation followed by scale?
The obvious answer is no, yet for certain cases the order is
unimportant. Consider first the case of change-of-scale by a,
followed by a translation XC; the constraints on the weights up to
second order are:
Wl(U;Xl) - wl(u; (xl-xo)/a),
w2 (u; Xl ,x2)

0=

w2(u; (xl-xo)/a, (x2-xo)/a) ,

(14a)
(l4b)

and for translation followed by scale the constraints are:
wl(u;Xl) - wl(u; (xl/a)-xo). and

(lSa)

w2(U;Xl,X2) = w2(u;(xl/a)-xo,(x2Ia )-XO) .

(lSb)

Consider only the second-order weights for the two-dimensional case.
Choose rectangular coordinate variables (x,y) so that the translation
is given by (xO,YO). Then
W2 (U;Xl,Yl;X2,Y2) =
w2 (u;(xl/a)-xO,(Yl/a)-YO;(x2/a)-xO'(Y2/a)-yO)'

(l6a)

W2 (U;Xl,Yl;X2,Y2) w2 (U;(Xl- x o)/a, (Yl-yo)/a; (x2- xo)/a, (Y2-Yo)/a).

(16b)

or

If we take as our solution
w2(U;Xl,Yl;X2,Y2) = w2(U;(X1-X2)/(Yl-Y2?,

(17)

then w2 is invariant to scale and translation, and the order is
unimportant. With higher-order weights one can be even more
adventurous.
As a final example consider the case of a change of scale by a
factor a and rotation about the origin by an amount to for a twodimensional system in polar coordinates. (Note that the order of
transformation makes no difference.) The weight constraints up to
second order are:
(18a)

307

(18b)
The first-order constraint requires that wI be independent of the
input variables, but for the second-order term one can obtain a more
useful solution:

(19)
This implies that with second-order weights, one can construct a unit
that is insensitive to changes in scale and rotation of the input
space. How useful it is depends upon the application.
SIMULATION RESULTS
We have constructed several higher-order neural networks that
demonstrated invariant response to transformations of scale and of
translation of the input patterns. The systems were small,
consisting of less than 100 input units, were constructed from
second-and first-order units, and contained only one, two, or three
layers. We used a back-propagation algorithm modified for the
higher-order (sigma-pi) units. The simulation studies are still in
the early stages, so the performance of the networks has not been
thoroughly investigated. It seems safe to say, however, that there
is much to be gained by a thorough study of these systems. For
example, we have demonstrated that a small system of second-order
units trained to distinguish the letters T and C at one scale can
continue to distinguish them over changes in scale of factors of at
least four without retraining and with satisfactory performance.
Similar performance has been obtained for the case of translation
invariance.
Even at this stage, some interesting facets of this approach are
becoming clear: 1) Even with the constraints imposed by the
invariance, it is usually necessary to limit the range of connections
in order to restrict the complexity of the network. This is often
cited as a problem with higher-order networks, but we take the view
that one can learn a great deal more about the nature of a problem by
examining it at this level rather than by simply training a network
that has a general-purpose architecture. 2) The higher-order
networks seem to solve problems in an elegant and simple manner.
However, unless one is careful in the design of the network, it
performs worse than a simpler conventional network when there is
noise in the input field. 3) Learning is often "quicker" than in a
conventional approach, although this is highly dependent on the
specific problem and implementation design. It seems that a tradeoff
can be made: either faster learning but less noise robustness, or
slower learning with more robust performance.
DISCUSSION
We have shown a simple way to encode geometric invariances into
neural networks (instead of training them), though to be useful the
networks must be constructed of higher-order units. The invariant
encoding is achieved by restricting the allowable network

308

architectures and is independent of learning rules and the form of
the sigmoid or threshold functions. The invariance encoding is
normally for an entire layer, although it can be on an individual
unit basis. It is easy to build one or more invariant layers into a
multi-layer net, and different layers can satisfy different
invariance requirements. This is useful for operating on internal
features or representations in an invariant manner. For learning in
such a net, a multi-layered learning rule such as generalized backpropagation 7 must be used. In our simulations we have used a
generalized back-propagation learning rule to train a two-layer
system consisting of a second-order, translation-invariant input
layer and a first-order output layer. Note that we have not shown
that one can not encode invariances into layered first-order
networks, but the analysis in this paper implies that such invariance
would be dependent on the form of the sigmoid function.
When invariances are encoded into higher-order neural networks,
the number of interconnections required is usually reduced by orders
of powers of N where N is the size of the input. For example, a
fully connected, first-order, single-layer net with a single output
unit would have order N interconnections; a similar second-order net,
order N2 . If this second-order net (or layer) is made shift
invariant, the order is reduced to N. The number of multiplies and
adds is still of order N2 .
We have limited our discussion in this paper to geometric
invariances, but there seems to be no reason why temporal or other
invariances could not be encoded in a similar manner.
REFERENCES
1.

D.H. Ballard and C.M. Brown, Computer Vision (Prentice-Hall,
Englewood Cliffs, NJ, 1982).

2.

B. Widrow, IEEE First Int1. Conf. on Neural Networks, 87TH019l7, Vol. 1, p. 143, San Diego, CA, June 1987.

3.

J.A. Feldman, Biological Cybernetics 46, 27 (1982).

4.

C.L. Giles and T. Maxwell, App1. Optics 26, 4972 (1987).

5.

G.E. Hinton, Proc. 7th IntI. Joint Conf. on Artificial
Intelligence, ed. A. Drina, 683 (1981).

6.

Y.C. Lee, G. Doolen, H.H. Chen, G.Z. Sun, T. Maxwell, H.Y. Lee,
C.L. Giles, Physica 22D, 276 (1986).

7.

D.E. Rume1hart, G.E. Hinton, and R.J. Williams, Parallel
Distributed Processing, Vol. 1, Ch. 8, D.E. Rume1hart and J.L.
McClelland, eds., (MIT Press, Cambridge, 1986).

309

8.

T . Maxwell, C.L. Giles, Y.C. Lee, and H.H. Chen, Proc. IEEE
IntI. Conf. on Systems, Man, and Cybernetics, 86CH2364-8, p.
627, Atlanta, GA, October 1986.

9.

W. Pitts and W.S. McCulloch, Bull. Math. Biophys. 9, 127
(1947).

10.

M. Minsky and S, Papert, Perceptrons (MIT Press, Cambridge,
Mass., 1969).

