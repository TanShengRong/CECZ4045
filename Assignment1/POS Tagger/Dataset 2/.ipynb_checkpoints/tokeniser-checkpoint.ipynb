{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/krishnavyas/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#files = glob.glob(\"C:/Users/owner/Desktop/NLP Dataset/*.txt\")\n",
    "files = glob.glob(\"*.txt\")  # reads all text files in the current directory\n",
    "tokens = []\n",
    "stemmed_tokens = []\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_distributor(token_list):\n",
    "    # stores a list of integers corresponding to number of characters of tokens\n",
    "    num_of_characters_list = []\n",
    "    token_count_objects_list = []  # stores all instances of token_count objects\n",
    "    for token in token_list:\n",
    "        temp_length = len(token)\n",
    "        if temp_length not in num_of_characters_list:\n",
    "            num_of_characters_list.append(temp_length)\n",
    "            new_token_count = token_count(temp_length)\n",
    "            token_count_objects_list.append(new_token_count)\n",
    "            new_token_count.token_list.append(token)\n",
    "            new_token_count.count = new_token_count.count + 1\n",
    "        else:\n",
    "            for token_count_object in token_count_objects_list:\n",
    "                if token_count_object.character_num == temp_length:\n",
    "                    token_count_object.token_list.append(token)\n",
    "                    token_count_object.count = token_count_object.count + 1\n",
    "\n",
    "    print(\"sorting results...\")\n",
    "    token_count_objects_list.sort(key=lambda x: x.character_num)\n",
    "\n",
    "    print(\"summarizing results...\")\n",
    "    for obj in token_count_objects_list:\n",
    "        print(\"Tokens with \" + str(obj.character_num) + \" characters:\")\n",
    "        print(obj.count)\n",
    "    return token_count_objects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class token_count:\n",
    "    \"\"\"\n",
    "    a class representing a certain unique character count\n",
    "    keeps a list of tokens that have that character count \n",
    "    as well as count variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, character_num):\n",
    "        self.character_num = character_num\n",
    "        self.token_list = []\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing...\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing...\")\n",
    "for f in files:\n",
    "    #print(\"File name: \" + f)\n",
    "    f = open(f, \"r\", encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    templist = nltk.word_tokenize(text)\n",
    "    for item in templist:\n",
    "        if item not in tokens:\n",
    "            tokens.append(item)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique tokens:\n",
      "8838\n",
      "sorting results...\n",
      "summarizing results...\n",
      "Tokens with 1 characters:\n",
      "90\n",
      "Tokens with 2 characters:\n",
      "819\n",
      "Tokens with 3 characters:\n",
      "1018\n",
      "Tokens with 4 characters:\n",
      "898\n",
      "Tokens with 5 characters:\n",
      "840\n",
      "Tokens with 6 characters:\n",
      "891\n",
      "Tokens with 7 characters:\n",
      "932\n",
      "Tokens with 8 characters:\n",
      "850\n",
      "Tokens with 9 characters:\n",
      "714\n",
      "Tokens with 10 characters:\n",
      "619\n",
      "Tokens with 11 characters:\n",
      "436\n",
      "Tokens with 12 characters:\n",
      "282\n",
      "Tokens with 13 characters:\n",
      "188\n",
      "Tokens with 14 characters:\n",
      "102\n",
      "Tokens with 15 characters:\n",
      "72\n",
      "Tokens with 16 characters:\n",
      "34\n",
      "Tokens with 17 characters:\n",
      "25\n",
      "Tokens with 18 characters:\n",
      "7\n",
      "Tokens with 19 characters:\n",
      "5\n",
      "Tokens with 21 characters:\n",
      "6\n",
      "Tokens with 22 characters:\n",
      "3\n",
      "Tokens with 23 characters:\n",
      "2\n",
      "Tokens with 24 characters:\n",
      "1\n",
      "Tokens with 25 characters:\n",
      "2\n",
      "Tokens with 26 characters:\n",
      "1\n",
      "Tokens with 27 characters:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of unique tokens:\")\n",
    "print(len(tokens))\n",
    "token_count = length_distributor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'number of tokens of each length')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_length = []\n",
    "num_tokens = []\n",
    "for obj in token_count:\n",
    "    token_length.append(obj.character_num)\n",
    "    num_tokens.append(obj.count)\n",
    "# plotting the graph before stemming\n",
    "plt.bar(token_length, num_tokens)\n",
    "plt.xlabel(\"length of the tokens\")\n",
    "plt.ylabel(\"number of tokens of each length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming...\n",
      "No. of unique tokens after stemming is:\n",
      "6167\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5b3a9c6d1af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No. of unique tokens after stemming is:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlength_distributor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-242720945bb5>\u001b[0m in \u001b[0;36mlength_distributor\u001b[0;34m(token_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp_length\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_of_characters_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mnum_of_characters_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mnew_token_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtoken_count_objects_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_token_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnew_token_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "print(\"stemming...\")\n",
    "for token in tokens:\n",
    "    s_token = ps.stem(token)\n",
    "    if s_token not in stemmed_tokens:\n",
    "        stemmed_tokens.append(s_token)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"No. of unique tokens after stemming is:\")\n",
    "print(len(stemmed_tokens))\n",
    "length_distributor(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence segmentation\n",
      "['\\n\\nCorrelational Strength and Computational Algebra\\nof Synaptic Connections Between Neurons\\nEberhard E. Fetz\\nDepartment of Physiology & Biophysics,\\nUniversity of Washington, Seattle, WA 98195\\nABSTRACT\\nIntracellular recordings in spinal cord motoneurons and cerebral\\ncortex neurons have provided new evidence on the correlational strength of\\nmonosynaptic connections, and the relation between the shapes of\\npostsynaptic potentials and the associated increased firing probability.', 'In\\nthese cells, excitatory postsynaptic potentials (EPSPs) produce crosscorrelogram peaks which resemble in large part the derivative of the EPSP.', 'Additional synaptic noise broadens the peak, but the peak area -- i.e., the\\nnumber of above-chance firings triggered per EPSP -- remains proportional to\\nthe EPSP amplitude.', 'A typical EPSP of 100 ~v triggers about .01 firings per\\nEPSP.', 'The consequences of these data for information processing by\\npolysynaptic connections is discussed.', 'The effects of sequential polysynaptic\\nlinks can be calculated by convolving the effects of the underlying\\nmonosynaptic connections.', 'The net effect of parallel pathways is the sum of\\nthe individual contributions.', 'INTRODUCTION\\nInteractions between neurons are determined by the strength and\\ndistribution of their synaptic connections.', 'The strength of synaptic\\ninteractions has been measured directly in the central nervous system by two\\ntechniques.', \"Intracellular recording reveals the magnitude and time course of\\npostsynaptic potentials (PSPs) produced by synaptic connections, and crosscorrelation of extracellular spike trains measures the effect of the PSP's on the\\nfiring probability of the connected cells.\", 'The relation between the shape of\\nexcitatory postsynaptic potentials (EPSPs) and the shape of the crosscorrelogram peak they produce has been empirically investigated in cat\\nmotoneurons 2,4,5 and in neocortical cells 10.', \"RELATION BETWEEN EPSP'S AND CORRELOGRAM PEAKS\\nSynaptic interactions have been studied most thoroughly in spinal\\ncord motoneurons.\", 'Figure 1 illustrates the membrane potential of a\\nrhythmically firing motoneuron, and the effect of EPSPs on its firing.', 'An\\nEPSP occurring sufficiently close to threshold (8) will cause the motoneuron\\nto fire and will advance an action potential to its rising edge (top).', 'Mathematical analysis of this threshold-crossing process predicts that an\\nEPSP with shape e(t) will produce a firing probability f(t), which resembles\\n?', \"American Institute of Phy~ics 1988\\n\\n\\x0c271\\n\\nrI\\n\\nf;\\n\\n::\\n\\n'I\\n\\n8\\n\\n'I\\n\\nI'\\n\\n/..-::-- ----.... ....\", '.\",.\"\"..,.', '\",\\n\\n.,...,.\"\"', '~-\"\"\"\\n\\n/\\'\\n\\nI\\n\\n..,, .... ..\\n\\n)\\n\\n\\\\\\n\\n...\",/\\n\\n,;\\n\\nI\\n\\ni:\\n\\n.. :\\n\\n\\\\\\n\\n--.----r\\n\\n,\\n\\n\\'I\\n\\n.,.,,,,\\n\\n....\\n\\n....\\n\\nEPSP\\ne(t)\\nt\\n\\nCROSS-\\n\\nCORRELOGRAM\\nf(t)\\n\\nTIME\\n\\nt\\n\\nFig.', '1.', \"The relation between EPSP's and motoneuron firing.\", 'Top: membrane trajectory of\\nrhythmically firing motoneuron, showing EPSP crossing threshold (8) and shortening the\\nnormal interspike interval by advancing a spike.', 'V(t) is difference between membrane\\npotential and threshold.', 'Middle: same threshold-crossing process aligned with EPSP, with\\nv(t) plotted as falling trajectory.', 'Intercept (at upward arrow) indicates time of the advanced\\naction potential.', 'Bottom: Cross-correlation histogram predicted by threshold crossings.', 'The\\npeak in the firing rate f(t) above baseline (fo) is produced by spikes advanced from baseline,\\nas indicated by the changed counts for the illustrated trajectory.', 'Consequently, the area in\\nthe peak equals the area of the subsequent trough.', '272\\n\\nthe derivative of the EPSP 4,8.', 'Specifically, for smooth membrane potential\\ntrajectories approaching threshold (the case of no additional synaptic noise):\\nf(t)\\n\\n=fo + (fo/v) del dt\\n\\n(1)\\n\\nv\\n\\nwhere fo is the baseline firing rate of the motoneuron and is the rate of\\nclosure between motoneuron membrane potential and threshold.', 'This\\nrelation can be derived analytically by tranforming the process to a\\ncoordinate system aligned with the EPSP (Fig.', '1, middle) and calculating the\\nrelative timing of spikes advanced by intercepts of the threshold trajectories\\nwith the EPSP 4.', 'The above relation (1) is also valid for the correlogram\\ntrough during the falling phase of the EPSP, as long as del dt >\\nif the EPSP\\nfalls more rapidly than\\nthe trough is limited at zero firing rate (as\\nillustrated for the correlogram at bottom).', 'The fact that the shape of the\\ncorrelogram peak above baseline matches the EPSP derivative has been\\nempirically confirmed for large EPSPs in cat motoneurons 4.', 'This relation\\nimplies that the height of the correlogram peak above baseline is proportional\\nto the EPSP rate of rise.', 'The integral of this relationship predicts that the area\\nbetween the correlogram peak and baseline is proportional to the EPSP\\namplitude.', 'This linear relation further implies that the effects of\\nsimultaneously arriving EPSPs will add linearly.', 'The presence of additional background synaptic \"noise\", which is\\nnormally produced by randomly occurring synaptic inputs, tends to make the\\ncorrelogram peak broader than the duration of the EPSP risetime.', 'This\\nbroadening is produced by membrane potential fluctuations which cause\\nadditional threshold crossings during the decay of the EPSP by trajectories\\nthat would have missed the EPSP (e.g., the dashed trajectory in Fig.', '1,\\nmiddle).', 'On the basis of indirect empirical comparisons it has been proposed\\n6,7 that the broader correlogram peaks can be described by the sum of two\\nlinear functions of e(t):\\n\\n-v,\\n\\nf(t)\\n\\n=fo + a e(t) + b deldt\\n\\n-v;\\n\\n(2)\\n\\nThis relation provides a reasonable match when the coefficients (a and b) can\\nbe optimized for each case 5,7, but direct empirical comparisons 2,4 indicate\\nthat the difference between the correlogram peak and the derivative is\\ntypically briefer than the EPSP.', 'The effect of synaptic noise on the transform -between EPSP and\\ncorrelogram peak has not yet been analytically derived (except for the case of\\nHowever the threshold-crossing process has been\\nGaussian noise1).', 'simulated by a computer model which adds synaptic noise to the trajectories\\nintercepting the EPSP 1.', \"The correlograms generated by the simulation match\\nthe correlograms measured empirically for small EPSP's in motoneurons 2,\\nconfirming the validity of the model.\", 'Although synaptic noise distributes the triggered firings over a wider\\npeak, the area of the correlogram peak, i.e., the number of motoneuron firings\\nproduced by an EPSP, is essentially preserved and remains proportional to\\nEPSP amplitude for moderate noise levels.', \"For unitary EPSP's (produced by\\n\\n\\x0c273\\n\\na single afferent fiber) in cat motoneurons, the number of firings triggered per\\nEPSP (Np) was linearly related to the amplitude (h) of the EPSP 2:\\nNp = (O.l/mv)?\", 'h (mv) + .003\\n\\n(3)\\n\\nThe fact that the number of triggered spikes increases in proportion to EPSP\\namplitude has also been confirmed for neocortical neurons 10; for cells\\nrecorded in sensorimotor cortex slices (probably pyramidal cells) the\\ncoefficient of h was very similar: 0.07/mv.', 'This means that a typical unitary\\nEPSP with amplitude of 100 Ilv, raises the probability that the postsynaptic\\ncell fires by less than .01.', 'Moreover, this increase occurs during a specific\\ntime interval corresponding to the rise time of the EPSP - on the order of 1 - 2\\nmsec.', 'The net increase in firing rate of the postsynaptic cell is calculated by\\nthe proportional decrease in interspike intervals produced by the triggered\\nspikes 4.', \"(While the above values are typical, unitary EPSP's range in size\\nfrom several hundred IlV down to undetectable levels of severalllv., and\\nhave risetimes of.2 - 4 msec.)\", 'Inhibitory connections between cells, mediated by inhibitory\\npostsynaptic potentials (IPSPs), produce a trough in the cross-correlogram.', 'This reduction of firing probability below baseline is followed by a\\nsubsequent broad, shallow peak, representing the spikes that have been\\ndelayed during the IPSP.', \"Although the effects of inhibitory connections\\nremain to be analyzed more quantitatively, preliminary results indicate that\\nsmall IPSP's in synaptic noise produce decreases in firing probability that are\\nsimilar to the increases produced by EPSP's 4,5.\", 'DISYNAPTIC LINKS\\n\\nThe effects of polysynaptic links between neurons can be understood\\nas combinations of the underlying monosynaptic connections.', 'A\\nmonosynaptic connection from cell A to cell B would produce a first-order\\ncross-correlation peak P1(BIA,t), representing the conditional probability that\\nneuron B fires above chance at time t, given a spike in cell A at time t = O.', 'As\\nnoted above, the shape of this first-order correlogram peak is largely\\nproportional to the EPSP derivative (for cells whose interspike interval\\nexceeds the duration of the EPSP).', 'The latency of the peak is the conduction\\ntime from A to B (Fig.', '2 top left).', 'In contrast, several types of disynaptic linkages betw.een A and B,\\nmediated by a third neuron C, will produce a second-order correlation peak\\nbetween A and B.', 'A disynaptic link may be produced by two serial\\nmonosynaptic connections, from A to C and from C to B (Fig.', '2, bottom left),\\nor by a common synaptic input from C ending on both A and B (Fig.', '2,\\nbottom right).', 'In both cases, the second-order correlation between A and B\\nproduced by the disynaptic link would be the convolution of the two firstorder correlations between the monosynaptically connected cells:\\n\\n(4)\\n\\n\\x0c274\\n\\nAs indicated by the diagram, the cross-correlogram peak P2(BIA,t) would be\\nsmaller and more dispersed than the peaks of the underlying first-order\\ncorrelation peaks.', 'For serial connections the peak would appear to the right\\nof the origin, at a latency that is the sum of the two monosynaptic latencies.', 'The peak produced by a common input typically straddles the origin, since its\\ntiming reflects the difference between the underlying latencies.', \"=>\\n\\nMonosynaptic connection\\n\\n-----..'t-\\n\\nI \\\\\\nt \\\\\\n\\n@\\n\\nFirst-order correlation\\n\\n~(AIB,t)\\n\\nLJA,,-_~_(_B_I_A_'t_)_\\n\\nDisynaptic connection\\n\\n=\\n\\n~(~IA,-t)\\n\\n1\\n~\\n\\nSerial connection\\n\\nSecond-order correlation\\nCommon input\\n\\nr--A---t\\nt\\nI\\n\\nt \\\\\\nI \\\\\\n\\n: \\\\.\", 'A\\n\\n~ (C I A)\\n\\n@\\n\\nt\\nt\\nt\\nI\\n\\n:\\n\\nll(AIC)\\n\\n:\"\\n\\nV\\n\\nH\\\\~\\nt\\n\\n:\\n\\nj\\n\\n\\\\\\n\\nI\\'\"\\nt \\\\\\n\\n\\\\\\n\\\\\\n\\n\\\\\\n\\\\\\n\\nt\\n\\n\\\\~(BIC)\\nP(BIA)\\n\\n_________ ~,_2__----\\n\\n@\\n\\n\\\\.', 'P(BIC)\\n\\n1 \\\\/\\\\ \\'\\n\\nJ t\"-_------\\n\\n_ _ _/\\\\.~(BIA)\\n-L\\n\\nFig.', '2.', 'Correlational effects of monosynaptic and disynaptic links between two neurons.', 'Top: monosynaptic excitatory link from A to B produces an increase in firing probability of B\\nafter A (left).', 'As with all correlograms this is the time-inverted probability of increased firing\\nin A relative to B (right).', 'Bottom: Two common disynaptic links between A and B are a\\nserial connection via C (left) and a common input from C. In both cases the effect of the\\ndisynaptic link is the convolution of the underlying monosynaptic links.', '275\\n\\nThis relation means that the probability that a spike in cell A will\\nproduce a correlated spike in cell B would be the product of the two\\nprobabilities for the intervening monosynaptic connections.', 'Given a typical\\nNp of .Ol/EPSP, this would reduce the effectiveness of a given disynaptic\\nlinkage by two orders of magnitude relative to a monosynaptic connection.', 'However, the net strength of all the disynaptic linkages between two given\\ncells is proportional to the number of mediating intemeurons (C}, since the\\neffects of parallel pathways add.', 'Thus, the net potency of all the disynaptic\\nlinkages between two cells could approach that of a monosynaptic linkage if\\nthe number of mediating interneurons were sufficiently large.', 'It should also\\nbe noted that some intemeurons may fire more than once per EPSP and have\\na higher probability of being triggered to fire than motoneurons 11.', 'For completeness, two other possible disynaptic links between A and B\\ninvolving a third cell C may be considered.', 'One is a serial connection from B\\nto C to A, which is the reverse of the serial connection from A to B.', 'This\\nwould produce a P2(BIA) with peak to the left of the origin.', 'The fourth\\ncircuit involves convergent connections from both A and B to C; this is the\\nonly combination that would not produce any causal link between A and B.', 'The effects of still higher-order polysynaptic linkages can be computed\\nsimilarly, by convolving the effects produced by the sequential connections.', 'For example, trisynaptic linkages between four neurons are equivalent to\\ncombinations of disynaptic and monosynaptic connections.', 'The cross-correlograms between two cells have a certain symmetry,\\ndepending on which is the reference cell.', 'The cross-correlation histogram of\\ncell B referenced to A is identical to the time-inverted correlogram of A\\nreferenced to B.', 'This is illustrated for the monosynaptic connection in Fig.2,\\ntop right, but is true for all correlograms.', 'This symmetry represents the fact\\nthat the above-chance probability of B firing after A is the same as the\\nprobability of A firing before B:\\nP(BIA, t)\\n\\n= P(AIB, -t)\\n\\n(5)\\n\\nAs a consequence, polysynaptic correlational links can be computed as the\\nsame convolution integral (Eq.', '4), independent of the direction of impulse\\npropagation.', 'P ARALLEL PATHS AND FEEDBACK LOOPS\\nIn addition to the simple combinations of pair-wise connections\\nbetween neurons illustrated above, additional connections between the same\\ncells may form circuits with various kinds of loops.', 'Recurrent connections\\ncan produce feedback loops, whose correlational effects are also calculated by\\nconvolving effects of the underlying synaptic links.', 'Parallel feed-forward\\npaths can form multiple pathways between the same cells.', 'These produce\\ncorrelational effects that are the sum of the effects of the individual\\nunderlying connections.', 'The simplest feedback loop is formed by reciprocal connections\\nbetween a pair of cells.', 'The effects of excitatory feedback can be computed by\\n\\n\\x0c276\\n\\nsuccessive cO?1volutions of the underlying monosynaptic connections (Fig.', '3\\ntop).', 'Note that such a positive feedback loop would be capable of sustaining\\nactivity only if the connections were sufficiently potent to ensure\\npostsynaptic firing.', 'Since the probabilities of triggered firings at a single\\nsynapse are considerably less than one, reverberating activity can be\\nsustained only if the number of interacting cells is correspondingly increased.', 'Thus, if the probability for a single link is on the order of .01, reverberating\\nactivity can be sustained if A and B are similarly interconnected with at least\\na hundred cells in parallel.', 'Connections between three neurons may produce various kinds of\\nloops.', 'Feedforward parallel pathways are formed when cell A is\\nmonosynaptically connected to B and in addition has a serial disynaptic\\nconnection through C, as illustrated in Fig.', '3 (bottom left); the correlational\\neffects of the two linkages from A to B would sum linearly, as shown for\\nexcitatory connections.', 'Again, the effect of a larger set of cells {C} would be\\nadditive.', 'Feedback loops could be formed with three cells by recurrent\\nconnections between any pair; the correlational consequences of the loop\\nagain are the convolution of the underlying links.', 'Three cells can form\\nanother type loop if both A and B are monosynaptically connected, and\\nsimultaneously influenced by a common interneuron C (Fig.', '3 bottom right).', 'In this case the expected correlogram between A and B would be the sum of\\nthe individual components -- a common input peak around the origin plus a\\ndelayed peak produced by the serial connection.', \"Feedback loop\\n1 - - - - -..\\n'l;---~\\n\\n...\\n\\n..../\\n\\n-',\\n\\n\\\\....\\n\\n.... ....\\n\\n..\", 'I:\\n\\n\\'\\n\\n....\\n\\n.......\\n\\n...........\"\\n;\\'\"\\n........', '..?.:....', \"....\\n-',\\n\\n....\", '?????\\n\\n\\'\"', '???.', 'Parallel jeedfOrward path\\n\\nI\\nI\\n:\\n\\nt\\nt\\nt\\n\\nCommon input loop\\n\\nI\\nt\\nPI (BIA) +P 2 (BIA)\\n\\nPI (BIA)+P\\n\\n2\\n\\n(BIA)\\n\\n:/\\\\ ____\\n\\n___.', ';.J\\n\\nl~\\n\\nFig.', '3.', 'Correlational effects of parallel connections between two neurons.', 'Top: feedback\\nloop between two neurons A and B produces higher-order effects equivalent to convolution\\nof mono~aptic effects.', 'Bottom: Loops formed by parallel feed forward paths (left) and by a\\ncommon mput concurrent with a monosynaptic link (right) produce additive effects.', '277\\n\\nCONCLUSIONS\\nThus, a simple computational algebra can be used to derive the\\ncorrelational effects of a given network structure.', 'Effects of sequential\\nconnections can be computed by convolution and effects of parallel paths by\\nsummation.', 'The inverse problem, of deducing the circuitry from the\\ncorrelational data is more difficult, since similar correlogram features may be\\nproduced by different circuits 9.', 'The fact that monosynaptic links produce small correlational effects on\\nthe order of .01 represents a significant constraint in the mechanisms of\\ninformation processing in real neural nets.', 'For example, secure propagation\\nof activity through serial polysynaptic linkages requires that the small\\nprobability of triggered firing via a given link is compensated by a\\nproportional increase in the number of parallel links.', 'Thus, reliable serial\\nconduction would require hundreds of neurons at each level, with\\nappropriate divergent and convergent connections.', 'It should also be noted\\nthat the effect of intemeurons can be modulated by changing their activity.', 'The intervening cells need to be active to mediate the correlational effects.', 'As\\nindicated by eq.', 'I, the size of the correlogram peak is proportional to the\\nfiring rate (fo) of the postsynaptic cell.', 'This allows dynamic modulation of\\npolysynaptic linkages.', 'The greater the number of links, the more susceptible\\nthey are to modulation.', 'Acknowledgements: The author thanks Mr. Garrett Kenyon for stimulating\\ndiscussions and the cited colleagues for collaborative efforts.', 'This work was\\nsupported in part by Nll-I grants NS 12542 and RR00166.', 'REFERENCES\\n1.', 'Bishop, B., Reyes, A.D., and Fetz E.E., Soc.', 'for Neurosci Abst.', '11:157 (1985).', '2.', 'Cope, T.C., Fetz, E.E., and Matsumura, M., J. Physiol.', '390:161-18 (1987).', '3.', 'Fetz, E.E.', 'and Cheney, P.D., J. Neurophysiol.', '44:751-772 (1980).', '4.', 'Fetz, E.E.', 'and Gustafsson, B., J. Physiol.', '341:387-410 (1983).', '5.', 'Gustafsson, B., and McCrea, D., J. Physiol.', '347:431-451 (1984).', '6.', 'Kirkwood, P.A., J. Neurosci.', 'Meth.', '1:107-132 (1979).', '7.', 'Kirkwood, P.A., and Sears, T._ J. Physiol.', '275:103-134 (1978).', '8.', 'Knox, C.K., Biophys.', 'J.', '14: 567-582 (1974).', '9.', 'Moore, G.P., Segundo, J.P., Perkel, D.H. and Levitan, H., Biophys.', 'J.', '10:876900 (1970).', '10.', 'Reyes, A.D., Fetz E.E.', 'and Schwindt, P.C., Soc.', 'for Neurosci Abst.', '13:157\\n(1987).', '11.', 'Surmeier, D.J.', 'and Weinberg, R.J., Brain Res.', '331:180-184 (1985).', '\\nENCODING GEOMETRIC INVARIANCES IN\\nHIGHER-ORDER NEURAL NETWORKS\\nC.L.', 'Giles\\nAir Force Office of Scientific Research, Bolling AFB, DC 20332\\nR.D.', 'Griffin\\nNaval Research Laboratory, Washington, DC\\n\\n20375-5000\\n\\nT. Maxwell\\nSachs-Freeman Associates, Landover, MD 20785\\nABSTRACT\\nWe describe a method of constructing higher-order neural\\nnetworks that respond invariantly under geometric transformations on\\nthe input space.', 'By requiring each unit to satisfy a set of\\nconstraints on the interconnection weights, a particular structure is\\nimposed on the network.', 'A network built using such an architecture\\nmaintains its invariant performance independent of the values the\\nweights assume, of the learning rules used, and of the form of the\\nnonlinearities in the network.', 'The invariance exhibited by a firstorder network is usually of a trivial sort, e.g., responding only to\\nthe average input in the case of translation invariance, whereas\\nhigher-order networks can perform useful functions and still exhibit\\nthe invariance.', 'We derive the weight constraints for translation,\\nrotation, scale, and several combinations of these transformations,\\nand report results of simulation studies.', 'INTRODUCTION\\nA persistent difficulty for pattern recognition systems is the\\nrequirement that patterns or objects be recognized independent of\\nirrelevant parameters or distortions such as orientation (position,\\nrotation, aspect), scale or size, background or context, doppler\\nshift, time of occurrence, or signal duration.', 'The remarkable\\nperformance of humans and other animals on this problem in the visual\\nand auditory realms is often taken for granted, until one tries to\\nbuild a machine with similar performance.', 'Thoufh many methods have\\nbeen developed for dealing with these problems, we have classified\\nthem into two categories: 1) preprocessing or transformation\\n(inherent) approaches, and 2) case-specific or \"brute force\"\\n(learned) approaches.', 'Common transformation techniques include:\\nFourier, Hough, and related transforms; moments; and Fourier\\ndescriptors of the input signal.', 'In these approaches the signal is\\nusually transformed so that the subsequent processing ignores\\narbitrary parameters such as scale, translation, etc.', 'In addition,\\nthese techniques are usually computationally expensive and are\\nsensitive to noise in the input signal.', 'The \"brute force\" approach\\nis exemplified by training a device, such as a perceptron, to\\nclassify a pattern independent of it\\'s position by presenting the\\n\\n@\\n\\nAmerican Institute of Physics 1988\\n\\n\\x0c302\\n\\ntraining pattern at all possible positions.', 'MADALINE machines 2 have\\nbeen shown to perform well using such techniques.', 'Often, this type\\nof invariance is pattern specific, does not easily generalize to\\nother patterns, and depends on the type of learning algorithm\\nemployed.', 'Furthermore, a great deal of time and energy is spent on\\nlearning the invariance, rather than on learning the signal.', 'We\\ndescribe a method that has the advantage of inherent invariance but\\nuses a higher-order neural network approach that must learn only the\\ndesired signal.', 'Higher-order units have been shown to have unique\\ncomputational strengths and are quite amenable to the encoding of a\\npriori know1edge.', '3 - 7\\nMATHEMATICAL DEVELOPMENT\\nOur approach is similar to the group invariance approach,8,10\\nalthough we make no appeal to group theory to obtain our results.', 'We\\nbegin by selecting a transformation on the input space, then require\\nthe output of the unit to be invariant to the transformation.', 'The\\nresulting equations yield constraints on the interconnection weights,\\nand thus imply a particular form or structure for the network\\narchitecture.', 'For the i-th unit Yi of order M defined on a discrete input\\nspace, let the output be given by\\nYi[YiM(X),P(x)] - f( WiO + ~ Wi 1 (X1) P(x1)\\n\\n+ ~~ Wi 2 (X1,X2) P(x1) P(x2) + ...\\n+~ ... ~ Wi M(X1,?', '?XM) P(x1)?', '?P(XM) ),\\n\\n(1)\\n\\nwhere p(x) is the input pattern or signal function (sometimes called\\na pixel) evaluated at position vector x, wim(xl, ... Xm) is the weight\\nof order m connecting the outputs of units at Xl, x2, .. Xm to the ith unit, i.e., it correlates m values, f(u) is some threshold or\\nsigmoid output function, and the summations extend over the input\\nspace.', 'YiM(X) represents the entire set of weights associated with\\nthe i-th unit.', 'These units are equivalent to the sigma-pi units a\\ndefined by Rumelhart, Hinton, and Williams.', '7 Systems built from\\nthese units suffer from a combinatorial explosion of terms, hence are\\nmore complicated to build and train.', 'To reduce the severity of this\\nproblem, one can limit the range of the interconnection weights or\\nthe number of orders, or impose various other constraints.', 'We find\\nthat, in addition to the advantages of inherent invariance, imposing\\nan invariance constraint on Eq.', '(1) reduces the number of allowed\\naThe sigma-pi neural networks are multi-layer networks with\\nhigher-order terms in any layer.', 'As such, most of the neural\\nnetworks described here can be considered as a special case of the\\nsigma-pi units.', 'However, the sigma-pi units as originally formulated\\ndid not have invariant weight terms, though it is quite simple to\\nincorporate such invariances in these units.', '303\\n\\nweights, thus simplifying the architecture and shortening the\\ntraining time.', 'We now define what we mean by invariance.', 'The output of a unit\\nis invariant with respect to the transformation T on the input\\npattern if 9\\n(2)\\n\\nAn example of the class of invariant response defined by Eq.', '(2)\\nwould be invariant detection of an object in the receptive field of a\\npanning or zooming camera.', 'An example of a different class would be\\ninvariant detection of an object that is moving within the field of a\\nfixed camera.', 'One can think of this latter case as consisting of a\\nfixed field of \"noise\" plus a moving field that contains only the\\nobject of interest.', 'If the detection system does not respond to the\\nfixed field, then this latter case is included in Eq.', '(2).', 'To illustrate our method we derive the weight constraints for\\none-dimensional translation invariance.', 'We will first switch to a\\ncontinuous formulation, however, for reasons of simplicity and\\ngenerality, and because it is easier to grasp the physical\\nsignificance of the results, although any numerical simulation\\nrequires a discrete formulation and has significant implications for\\nthe implementation of our results.', 'Instead of an index i, we now\\nkeep track of our units with the continuous variable u.', 'With these\\nchanges Eq.', '(2) now becomes\\ny[u;wM(x),p(X)] = f( wO + JrdXl Wl(U;Xl) P(xl) + ...\\n\\n+\\n\\nf??', 'Jr dXl?', '.dXM\\n\\nwM(U;Xl,?', '?XM) P(Xl)?', '.P(XM) ),\\n\\n(3)\\n\\nThe limits on the integrals are defined by the problem and are\\ncrucial in what follows.', 'Let T be a translation of the input pattern\\nby -xO, so that\\n\\n(4)\\n\\nT[p(x)] - p(x+XO)\\nwhere xo is the translation of the input pattern.', 'Ty[u;wM(x) ,p(x)] - y[u;YM(x),p(x+XO?)', '=\\n\\nThen, from eq (2),\\n\\ny[u;wM(x),p(x)]\\n\\n(5)\\n\\nSince p(x) is arbitrary we must impose term-by-term equality in the\\nargument of the threshold function; i.e.,\\n\\nf dXl Wl(U;Xl)\\n\\nP(xl) =\\n\\nf dxl Wl(U;Xl)\\n\\nP(xl+XO),\\n\\n(Sa)\\n\\nJr fdxl dX2 W2 (U;Xl,X2) P(xl) P(x2) =\\nJr\\n\\nf dXl\\n\\ndX2 W2 (U;Xl,X2) P(xl+XO) P(x2+XO),\\netc.', '(Sb)\\n\\n\\x0c304\\n\\nMaking the substitutions xl.', 'xl-XO, x2 \",x2-XO, etc, we find that\\n\\nf dXl Wl(U;Xl)\\n\\nf f dxl\\n\\nP(xl) -\\n\\nf dxl WI(U;Xl-XO)\\n\\nP(XI) ,\\n\\n(6a)\\n\\ndX2 W2 (U;XI,X2) P(xI) P(x2) -\\n\\nf f dXI\\n\\ndX2 W2 (U;XI-XO,X2-XO) P(xI) P(x2),\\n\\n(6b)\\n\\netc.', 'Note that the limits of the integrals on the right hand side must be\\nadjusted to satisfy the change-of-variables.', 'If the limits on the\\nintegrals are infinite or if one imposes some sort of periodic\\nboundary condition, the limits of the integrals on both sides of the\\nequation can be set equal.', 'We will assume in the remainder of this\\npaper that these conditions can be met; normally this means the\\nlimits of the integrals extend to infinity.', '(In an implementation,\\nit is usually impractical or even impossible to satisfy these\\nrequirements, but our simulation results indicate that these networks\\nperform satisfactorily even though the regions of integration are not\\nidentical.', 'This question must be addressed for each class of\\ntransformation; it is an integral part of the implementation design.)', 'Since the functions p(x) are arbitrary and the regions of integration\\nare the same, the weight functions must be equal.', 'This imposes a\\nconstraint on the functional form of the weight functions or, in the\\ndiscrete implementation, limits the allowed connections and thus the\\nnumber of weights.', 'In the case of translation invariance, the\\nconstraint on the functional form of the weight functions requires\\nthat\\nw1(U;XI) - wl(u;X].-XO),\\nw2(U;XI,X2) - w2(U;XI-XO,X2-XO),\\n\\n(7a)\\n\\n(7b)\\n\\netc.', 'These equations imply that the first order weight is independent of\\ninput position, and depends only on the output position u.', 'The\\nsecond order weight is a function only of vector differences,IO i.e.,\\nw1(u;Xj) - J..(u),\\n\\n(8a)\\n\\nw2(U;X].,X2) - w2(u:X].', '-Xl)?', '(8b)\\n\\nFor a discrete implementation with N input units (pixels) fully\\nconnected to an output unit, this requirement reduces the number of\\nsecond-order weights from order N2 to order N, i.e., only weights for\\ndifferences of indexes are needed rather than all unique pair\\ncombinations.', 'Of course, this advantage is multiplied as the number\\nof fully-connected output units increases.', 'FURTHER EXAMPLES\\nWe have applied these techniques to several other\\ntransformations of interest.', 'For the case of transformation of scale\\n\\n\\x0c305\\n\\ndefine the scale operator S such that\\nSp(x) - aIlp(ax)\\n\\n(9)\\n\\nwhere a is the scale factor, and x is a vector of dimension n. The\\nfactor an is used for normalization purposes, so that a given figure\\nalways contains the same \"energy\" regardless of its scale.', \"Application of the same procedure to this transformation leads to the\\nfollowing constraints on the weights:\\nwl(u;Xjfa) -= wl(u;~,\\n\\nw2(u;X1Ia,xv'a) ..\", \"w2(u;'X.l.\", \"'~)'\\nw3(u;xlla,x2/a ,x3/a) ... w3(U;X].,X2,X3), etc.\", '(lOa)\\n(lOb)\\n(lOc)\\n\\nConsider a two-dimensional problem viewed in polar coordinates (r,t).', 'A set of solutions to these constraints is\\nJ.', '(u;q,tI) - w1(u;Q),\\n\\nw2(u;rl,r2;tl,t2) - w2(u;rllr2;tl,t2).', 'w3 (u;rl,r2,r3;tl,t2,t3) - w3 (u;(rl-r2)/r3;tl,t2,t3).', '(lla)\\n(llb)\\n(llc)\\n\\nNote that with increasing order comes increasing freedom in the\\nselection of the functional form of the weights.', 'Any solution that\\nsatisfies the constraint may be used.', 'This gives the designer\\nadditional freedom to limit the connection complexity, or to encode\\nspecial behavior into the net architecture.', 'An example of this is\\ngiven later when we discuss combining translation and scale\\ninvariance in the same network.', 'Now consider a change of scale for a two-dimensional system in\\nrectangular coordinates, and consider only the second-order weights.', 'A set of solutions to the weight constraint is:\\nW2 (U;Xl,Yl;X2,Y2)\\n\\n-\\n\\nW2 (U;Xl/Yl;X2/Y2),\\n\\n- W2 (U;Xl/X2;Yl/Y2),\\nW2 (U;Xl,Yl;X2,Y2) - w2 (U;(Xl-X2)/(Yl-Y2)), etc.', 'W2 (U;Xl,Yl;X2,Y2)\\n\\n(12a)\\n(l2b)\\n(12c)\\n\\nWe have done a simulation using the form of Eq.', '(12b).', 'The\\nsimulation was done using a small input space (8x8) and one output\\nunit.', 'A simple least-mean-square (back-propagation) algorithm was\\nused for training the network.', 'When taught to distinguish the\\nletters T and C at one scale, it distinguished them at changes of\\nscale of up to 4X with about 15 percent maximum degradation in the\\noutput strength.', 'These results are quite encouraging because no\\nspecial effort was required to make the system work, and no\\ncorrections or modifications were made to account for the boundary\\ncondition requirements as discussed near Eq.', '(6).', 'This and other\\nsimulations are discussed further later.', 'As a third example of a geometric transformation, consider the\\ncase of rotation about the origin for a two-dimensional space in\\npolar coordinates.', 'One can readily show that the weight constraints\\n\\n\\x0c306\\n\\nare satisfied if\\nwl(u;rl,tl) ~ wl(u;rl),\\nw2(u;rl,r2;tl,t2) - w2(u;rl,r2;tl-t2), etc.', '(13a)\\n(l3b)\\n\\nThese results are reminiscent of the results for translation\\ninvariance.', 'This is not uncommon: seemingly different problems\\noften have similar constraint requirements if the proper change of\\nvariable is made.', 'This can be used to advantage when implementing\\nsuch networks but we will not discuss it further here.', 'An interesting case arises when one considers combinations of\\ninvariances, e.g., scale and translation.', 'This raises the question\\nof the effect of the order of the transformations, i.e., is scale\\nfollowed by translation equivalent to translation followed by scale?', 'The obvious answer is no, yet for certain cases the order is\\nunimportant.', 'Consider first the case of change-of-scale by a,\\nfollowed by a translation XC; the constraints on the weights up to\\nsecond order are:\\nWl(U;Xl) - wl(u; (xl-xo)/a),\\nw2 (u; Xl ,x2)\\n\\n0=\\n\\nw2(u; (xl-xo)/a, (x2-xo)/a) ,\\n\\n(14a)\\n(l4b)\\n\\nand for translation followed by scale the constraints are:\\nwl(u;Xl) - wl(u; (xl/a)-xo).', 'and\\n\\n(lSa)\\n\\nw2(U;Xl,X2) = w2(u;(xl/a)-xo,(x2Ia )-XO) .', '(lSb)\\n\\nConsider only the second-order weights for the two-dimensional case.', 'Choose rectangular coordinate variables (x,y) so that the translation\\nis given by (xO,YO).', \"Then\\nW2 (U;Xl,Yl;X2,Y2) =\\nw2 (u;(xl/a)-xO,(Yl/a)-YO;(x2/a)-xO'(Y2/a)-yO)'\\n\\n(l6a)\\n\\nW2 (U;Xl,Yl;X2,Y2) w2 (U;(Xl- x o)/a, (Yl-yo)/a; (x2- xo)/a, (Y2-Yo)/a).\", '(16b)\\n\\nor\\n\\nIf we take as our solution\\nw2(U;Xl,Yl;X2,Y2) = w2(U;(X1-X2)/(Yl-Y2?,\\n\\n(17)\\n\\nthen w2 is invariant to scale and translation, and the order is\\nunimportant.', 'With higher-order weights one can be even more\\nadventurous.', 'As a final example consider the case of a change of scale by a\\nfactor a and rotation about the origin by an amount to for a twodimensional system in polar coordinates.', '(Note that the order of\\ntransformation makes no difference.)', 'The weight constraints up to\\nsecond order are:\\n(18a)\\n\\n\\x0c307\\n\\n(18b)\\nThe first-order constraint requires that wI be independent of the\\ninput variables, but for the second-order term one can obtain a more\\nuseful solution:\\n\\n(19)\\nThis implies that with second-order weights, one can construct a unit\\nthat is insensitive to changes in scale and rotation of the input\\nspace.', 'How useful it is depends upon the application.', 'SIMULATION RESULTS\\nWe have constructed several higher-order neural networks that\\ndemonstrated invariant response to transformations of scale and of\\ntranslation of the input patterns.', 'The systems were small,\\nconsisting of less than 100 input units, were constructed from\\nsecond-and first-order units, and contained only one, two, or three\\nlayers.', 'We used a back-propagation algorithm modified for the\\nhigher-order (sigma-pi) units.', 'The simulation studies are still in\\nthe early stages, so the performance of the networks has not been\\nthoroughly investigated.', 'It seems safe to say, however, that there\\nis much to be gained by a thorough study of these systems.', 'For\\nexample, we have demonstrated that a small system of second-order\\nunits trained to distinguish the letters T and C at one scale can\\ncontinue to distinguish them over changes in scale of factors of at\\nleast four without retraining and with satisfactory performance.', 'Similar performance has been obtained for the case of translation\\ninvariance.', 'Even at this stage, some interesting facets of this approach are\\nbecoming clear: 1) Even with the constraints imposed by the\\ninvariance, it is usually necessary to limit the range of connections\\nin order to restrict the complexity of the network.', 'This is often\\ncited as a problem with higher-order networks, but we take the view\\nthat one can learn a great deal more about the nature of a problem by\\nexamining it at this level rather than by simply training a network\\nthat has a general-purpose architecture.', '2) The higher-order\\nnetworks seem to solve problems in an elegant and simple manner.', 'However, unless one is careful in the design of the network, it\\nperforms worse than a simpler conventional network when there is\\nnoise in the input field.', '3) Learning is often \"quicker\" than in a\\nconventional approach, although this is highly dependent on the\\nspecific problem and implementation design.', 'It seems that a tradeoff\\ncan be made: either faster learning but less noise robustness, or\\nslower learning with more robust performance.', 'DISCUSSION\\nWe have shown a simple way to encode geometric invariances into\\nneural networks (instead of training them), though to be useful the\\nnetworks must be constructed of higher-order units.', 'The invariant\\nencoding is achieved by restricting the allowable network\\n\\n\\x0c308\\n\\narchitectures and is independent of learning rules and the form of\\nthe sigmoid or threshold functions.', 'The invariance encoding is\\nnormally for an entire layer, although it can be on an individual\\nunit basis.', 'It is easy to build one or more invariant layers into a\\nmulti-layer net, and different layers can satisfy different\\ninvariance requirements.', 'This is useful for operating on internal\\nfeatures or representations in an invariant manner.', 'For learning in\\nsuch a net, a multi-layered learning rule such as generalized backpropagation 7 must be used.', 'In our simulations we have used a\\ngeneralized back-propagation learning rule to train a two-layer\\nsystem consisting of a second-order, translation-invariant input\\nlayer and a first-order output layer.', 'Note that we have not shown\\nthat one can not encode invariances into layered first-order\\nnetworks, but the analysis in this paper implies that such invariance\\nwould be dependent on the form of the sigmoid function.', 'When invariances are encoded into higher-order neural networks,\\nthe number of interconnections required is usually reduced by orders\\nof powers of N where N is the size of the input.', 'For example, a\\nfully connected, first-order, single-layer net with a single output\\nunit would have order N interconnections; a similar second-order net,\\norder N2 .', 'If this second-order net (or layer) is made shift\\ninvariant, the order is reduced to N. The number of multiplies and\\nadds is still of order N2 .', 'We have limited our discussion in this paper to geometric\\ninvariances, but there seems to be no reason why temporal or other\\ninvariances could not be encoded in a similar manner.', 'REFERENCES\\n1.', 'D.H. Ballard and C.M.', 'Brown, Computer Vision (Prentice-Hall,\\nEnglewood Cliffs, NJ, 1982).', '2.', 'B. Widrow, IEEE First Int1.', 'Conf.', 'on Neural Networks, 87TH019l7, Vol.', '1, p. 143, San Diego, CA, June 1987.', '3.', 'J.A.', 'Feldman, Biological Cybernetics 46, 27 (1982).', '4.', 'C.L.', 'Giles and T. Maxwell, App1.', 'Optics 26, 4972 (1987).', '5.', 'G.E.', 'Hinton, Proc.', '7th IntI.', 'Joint Conf.', 'on Artificial\\nIntelligence, ed.', 'A. Drina, 683 (1981).', '6.', 'Y.C.', 'Lee, G. Doolen, H.H.', 'Chen, G.Z.', 'Sun, T. Maxwell, H.Y.', 'Lee,\\nC.L.', 'Giles, Physica 22D, 276 (1986).', '7.', 'D.E.', 'Rume1hart, G.E.', 'Hinton, and R.J. Williams, Parallel\\nDistributed Processing, Vol.', '1, Ch.', '8, D.E.', 'Rume1hart and J.L.', 'McClelland, eds., (MIT Press, Cambridge, 1986).', '309\\n\\n8.', 'T .', 'Maxwell, C.L.', 'Giles, Y.C.', 'Lee, and H.H.', 'Chen, Proc.', 'IEEE\\nIntI.', 'Conf.', 'on Systems, Man, and Cybernetics, 86CH2364-8, p.\\n627, Atlanta, GA, October 1986.', '9.', 'W. Pitts and W.S.', 'McCulloch, Bull.', 'Math.', 'Biophys.', '9, 127\\n(1947).', '10.', 'M. Minsky and S, Papert, Perceptrons (MIT Press, Cambridge,\\nMass., 1969).', '\\n\\nNetwork Generality, Training Required,\\nand PrecisIon Required\\nJohn S. Denker and Ben S. Wittner\\nAT&T Bell Laboratories\\nHolmdel, New Jersey 07733\\n\\n1\\n\\nKeep your hand on your wallet.', '- Leon Cooper, 1987\\n\\nAbstract\\nWe show how to estimate (1) the number of functions that can be implemented by a\\nparticular network architecture, (2) how much analog precision is needed in the connections in the network, and (3) the number of training examples the network must see\\nbefore it can be expected to form reliable generalizations.', 'Generality versus Training Data Required\\nConsider the following objectives: First, the network should be very powerful and versatile, i.e., it should implement any function (truth table) you like, and secondly, it\\nshould learn easily, forming meaningful generalizations from a small number of training\\nexamples.', 'Well, it is information-theoretically impossible to create such a network.', 'We\\nwill present here a simplified argument; a more complete and sophisticated version can\\nbe found in Denker et al.', '(1987).', 'It is customary to regard learning as a dynamical process: adjusting the weights (etc.)', 'in a single network.', 'In order to derive the results of this paper, however, we take\\na different viewpoint, which we call the ensemble viewpoint.', 'Imagine making a very\\nlarge number of replicas of the network.', 'Each replica has the same architecture as the\\noriginal, but the weights are set differently in each case.', 'No further adjustment takes\\nplace; the \"learning process\" consists of winnowing the ensemble of replicas, searching\\nfor the one( s) that satisfy our requirements.', 'Training proceeds as follows: We present each item in the training set to every network\\nin the ensemble.', 'That is, we use the abscissa of the training pattern as input to the\\nnetwork, and compare the ordinate of the training pattern to see if it agrees with the\\nactual output of the network.', 'For each network, we keep a score reflecting how many\\ntimes (and how badly) it disagreed with a training item.', 'Networks with the lowest score\\nare the ones that agree best with the training data.', 'If we had complete confidence in\\nlCurrently at NYNEX Science and Technology, 500 Westchester Ave., White Plains, NY 10604\\n\\n@) American Institute of Physics 1988\\n\\n\\x0c220\\n\\nthe reliability of the training set, we could at each step simply throwaway all networks\\nthat disagree.', 'For definiteness, let us consider a typical network architecture, with No input wires and\\nNt units in each processing layer I, for I E {I??', '?L}.', 'For simplicity we assume NL = 1.', 'We recognize the importance of networks with continuous-valued inputs and outputs,\\nbut we will concentrate for now on training (and testing) patterns that are discrete,\\nwith N == No bits of abscissa and N L = 1 bit of ordinate.', 'This allows us to classify the\\nnetworks into bins according to what Boolean input-output relation they implement,\\nand simply consider the ensemble of bins.', 'There are 22N jossible bins.', 'If the network architecture is completely general and\\npowerful, all 22 functions will exist in the ensemble of bins.', 'On average, one expects\\nthat each training item will throwaway at most half of the bins.', 'Assuming maximal\\nefficiency, if m training items are used, then when m ~ 2N there will be only one bin\\nremaining, and that must be the unique function that consistently describes all the\\ndata.', 'But there are only 2N possible abscissas using N bits.', 'Therefore a truly general\\nnetwork cannot possibly exhibit meaningful generalization - 100% of the possible data\\nis needed for training.', 'Now suppose that the network is not completely general, so that even with all possible\\nsettings of the weights we can only create functions in 250 bins, where So < 2N.', 'We call\\nSo the initial entropy of the network.', 'A more formal and general definition is given in\\nDenker et al.', '(1987).', 'Once again, we can use the training data to winnow the ensemble,\\nand when m ~ So, there will be only one remaining bin.', 'That function will presumably\\ngeneralize correctly to the remaining 2N - m possible patterns.', 'Certainly that function\\nis the best we can do with the network architecture and the training data we were given.', 'The usual problem with automatic learning is this: If the network is too general, So\\nwill be large, and an inordinate amount of training data will be required.', 'The required\\namount of data may be simply unavailable, or it may be so large that training would be\\nprohibitively time-consuming.', 'The shows the critical importance of building a network\\nthat is not more general than necessary.', 'Estimating the Entropy\\nIn real engineering situations, it is important to be able to estimate the initial entropy\\nof various proposed designs, since that determines the amount of training data that will\\nbe required.', 'Calculating So directly from the definition is prohibitively difficult, but we\\ncan use the definition to derive useful approximate expressions.', \"(You wouldn't want to\\ncalculate the thermodynamic entropy of a bucket of water directly from the definition,\\neither. )\", '221\\n\\nSuppose that the weights in the network at each connection i were not continuously\\nadjustable real numbers, but rather were specified by a discrete code with bi bits.', 'Then\\nthe total number of bits required to specify the configuration of the network is\\n\\n(1)\\nNow the total number offunctions that could possibly be implemented by such a network\\narchitecture would be at most 2B.', 'The actual number will always be smaller than this,\\nsince there are various ways in which different settings of the weights can lead to identical\\nfunctions (bins).', 'For one thing, for each hidden layer 1 E {1???', 'L-1}, the numbering of\\nthe hidden units can be permuted, and the polarity of the hidden units can be flipped,\\nwhich means that 2 50 is less than 2B by a factor (among others) of III Nl!', '2N ,.', 'In\\naddition, if there is an inordinately large number of bits bi at each connection, there\\nwill be many settings where small changes in the connection will be immaterial.', 'This\\nwill make 2 so smaller by an additional factor.', 'We expect aSO/abi ~ 1 when bi is small,\\nand aSO/ab i ~ 0 when bi is large; we must now figure out where the crossover occurs.', 'The number of \"useful and significant\" bits of precision, which we designate b*, typically\\nscales like the logarithm of number of connections to the unit in question.', 'This can be\\nunderstood as follows: suppose there are N connections into a given unit, and an input\\nsignal to that unit of some size A is observed to be significant (the exact value of A\\ndrops out of the present calculation).', 'Then there is no point in having a weight with\\nmagnitude much larger than A, nor much smaller than A/N.', 'That is, the dynamic\\nrange should be comparable to the number of connections.', '(This argument is not exact,\\nand it is easy to devise exceptions, but the conclusion remains useful.)', 'If only a fraction\\n1/ S of the units in the previous layer are active (nonzero) at a time, the needed dynamic\\nrange is reduced.', 'This implies b* ~ log(N/S).', 'Note: our calculation does not involve the dynamics of the learning process.', 'Some\\nnumerical methods (including versions of back propagation) commonly require a number\\nof temporary \"guard bits\" on each weight, as pointed out by llichard Durbin (private\\ncommunication).', 'Another log N bits ought to suffice.', 'These bits are not needed after\\nlearning is complete, and do not contribute to So.', 'If we combine these ideas and apply them to a network with N units in each layer, fully\\nconnected, we arrive at the following expression for the number of different Boolean\\nfunctions that can be implemented by such a network:\\n\\n(2)\\nwhere\\nB ~ LN 2 log N\\n\\n(3)\\n\\nThese results depend on the fact that we are considering only a very restricted type of\\nprocessing unit: the output is a monotone function of a weighted sum of inputs.', 'Cover\\n\\n\\x0c222\\n\\n(1965) discussed in considerable depth the capabilities of such units.', 'Valiant (1986) has\\nexplored the learning capabilities of various models of computation.', 'Abu-Mustafa has emphasized the principles of information and entropy and applied\\nthem to measuring the properties of the training set.', 'At this conference, formulas\\nsimilar to equation 3 arose in the work of Baum, Psaltis, and Venkatesh, in the context\\nof calculating the number of different training patterns a network should be able to\\nmemorize.', 'We originally proposed equation 2 as an estimate of the number of patterns\\nthe network would have to memorize before it could form a reliable generalization.', 'The\\nbasic idea, which has numerous consequences, is to estimate the number of (bins of)\\nnetworks that can be realized.', 'References\\n1.', 'Vasser Abu-Mustafa, these proceedings.', '2.', 'Eric Baum, these proceedings.', '3.', 'T. M. Cover, \"Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition,\" IEEE Trans.', 'Elec.', 'Comp., EC-14,\\n326-334, (June 1965)\\n4.', 'John Denker, Daniel Schwartz, Ben Wittner, Sara Solla, John Hopfield, Richard\\nHoward, and Lawrence Jackel, Complex Systems, in press (1987).', '5.', 'Demetri Psaltis, these proceedings.', '6.', '1.', 'G. Valiant, SIAM J. Comput.', '15(2), 531 (1986), and references therein.', '7.', 'Santosh Venkatesh, these proceedings.', '\\n\\nLEARNING A COLOR ALGORITHM FROM EXAMPLES\\nAnya C. Hurlbert and Tomaso A. Poggio\\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences,\\nMassachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA\\nABSTRACT\\nA lightness algorithm that separates surface reflectance from illumination in a\\nMondrian world is synthesized automatically from a set of examples, pairs of input\\n(image irradiance) and desired output (surface reflectance).', 'The algorithm, which resembles a new lightness algorithm recently proposed by Land, is approximately equivalent to filtering the image through a center-surround receptive field in individual chromatic channels.', 'The synthesizing technique, optimal linear estimation, requires only\\none assumption, that the operator that transforms input into output is linear.', 'This\\nassumption is true for a certain class of early vision algorithms that may therefore be\\nsynthesized in a similar way from examples.', 'Other methods of synthesizing algorithms\\nfrom examples, or \"learning\", such as backpropagation, do not yield a significantly different or better lightness algorithm in the Mondrian world.', 'The linear estimation and\\nbackpropagation techniques both produce simultaneous brightness contrast effects.', 'The problems that a visual system must solve in decoding two-dimensional images\\ninto three-dimensional scenes (inverse optics problems) are difficult: the information\\nsupplied by an image is not sufficient by itself to specify a unique scene.', 'To reduce\\nthe number of possible interpretations of images, visual systems, whether artificial\\nor biological, must make use of natural constraints, assumptions about the physical\\nproperties of surfaces and lights.', 'Computational vision scientists have derived effective\\nsolutions for some inverse optics problems (such as computing depth from binocular\\ndisparity) by determining the appropriate natural constraints and embedding them in\\nalgorithms.', 'How might a visual system discover and exploit natural constraints on its\\nown?', 'We address a simpler question: Given only a set of examples of input images and\\ndesired output solutions, can a visual system synthesize.', 'or \"learn\", the algorithm that\\nconverts input to output?', 'We find that an algorithm for computing color in a restricted\\nworld can be constructed from examples using standard techniques of optimal linear\\nestimation.', 'The computation of color is a prime example of the difficult problems of inverse\\noptics.', \"We do not merely discriminate betwN'n different wavelengths of light; we assign\\n\\n@ American Institute of Physics 1988\\n\\n\\x0c623\\n\\nroughly constant colors to objects even though the light signals they send to our eyes\\nchange as the illumination varies across space and chromatic spectrum.\", 'The computational goal underlying color constancy seems to be to extract the invariant surface\\nspectral reflectance properties from the image irradiance, in which reflectance and iI-\"\\nlumination are mixed 1 ?', 'Lightness algorithms 2-8, pioneered by Land, assume that the color of an object\\ncan be specified by its lightness, or relative surface reflectance, in each of three independent chromatic channels, and that lightness is computed in the same way in each\\nchannel.', 'Computing color is thereby reduced to extracting surface reflectance from the\\nimage irradiance in a single chromatic channel.', \"The image irra.diance, s', is proportional to the product of the illumination intensity e' and the surface reflectance r' in that channel:\\n\\ns' (x, y) = r' (x, y )e' (x, y).\", \"(1 )\\nThis form of the image intensity equation is true for a Lambertian reflectance model,\\nin which the irradiance s' has no specular components, and for appropriately chosen\\ncolor channels 9.\", \"Taking the logarithm of both sides converts it to a sum:\\ns(x, y) = rex, y)\\n\\n+ e(x,y),\\n\\n(2)\\n\\nwhere s = loges'), r = log(r') and e = log(e').\", 'Given s(x,y) alone, the problem of solving Eq.', '2 for r(x,y) is underconstrained.', \"Lightness algorithms constrain the problem by restricting their domain to a world of\\nMondrians, two-dimensional surfaces covered with patches of random colors 2 and by\\nexploiting two constraints in that world: (i) r'(x,y) is unifonn within patches but\\nhas sharp discontinuities at edges between patches and (ii) e' (x, y) varies smoothly\\nacross the Mondrian.\", 'Under these constraints, lightness algorithms can recover a good\\napproximation to r( x, y) and so can recover lightness triplets that label roughly constant\\ncolors 10.', 'We ask whether it is possible to synthesize from examples an algorithm that ex?', 'tracts reflectance from image irradiance.', 'and whether the synthesized algorithm will resemble existing lightness algorithms derived from an explicit analysis of the constraints.', 'We make one assumption, that the operator that transforms irradiance into reflectance\\nis linear.', 'Under that assumption, motivated by considerations discussed later, we use\\noptimal linear estimation techniques to synthesize an operator from examples.', 'The\\nexamples are pairs of images: an input image of a Mondrian under illumination that\\nvaries smoothly across space and its desired output image that displays the reflectance\\nof the Mondrian without the illumination.', 'The technique finds the linear estimator\\nthat best maps input into desired output.', 'in the least squares sense.', 'For computational convenience we use one-dimensional \"training vectors\" that\\nrepresent vertical scan lines across the ~londrian images (Fig.', '1).', 'We generate many\\n\\n\\x0c624\\n\\n1S0t?', '-~\\n?', \"~\\n\\n100 ,\\n\\n'.\", 'a\\n\\n,\\n\\n~~~--------------~~--~--~~\\nSO\\n100\\n110\\n100\\nllO\\n100\\nlilt.,\\n\\nInput d.t.', 'i;[:2:=:0\\na\\n\\nSO\\n\\n101\\n\\nISO\\n\\nZOO\\n\\nUI\\n\\n:~kfhJfEirQ b\\n0\\n\\n)01\\n\\nII\\n\\n100\\n\\nISO\\n\\n100\\n\\n110\\n\\np/Jte\\'\\n\\nf~l\\no\\n\\n50\\n\\n100\\n\\nUO\\n\\nZOO\\n\\nZSO\\n\\n100\\n\\nI\"\\'\"\\n\\nc\\n\\n)00\\n\\n.1\\n\\np,xe\\'\\n\\n100\\n\\nISO\\n\\n100\\n\\n110\\n\\nlot\\n\\np\\'.\"', 'OlltPllt lIlll.l .', '.ll.a\\n\\nFig.', '1.', '(a) The input data, a one-dimensional vector 320 pixels long.', 'Its random\\nMondrian reflectance pattern is superimposed on a linear illumination gradient with\\na random slope and offset.', '(b) shows the corresponding output solution, on the left\\nthe illumination and on the right reBectance.', 'We used 1500 such pairs of inputoutput examples (each different from the others) to train the operator shown in Fig.', '2.', '(c) shows the result obtained by the estimated operator when it acts on the input\\ndata (a), not part of the training set.', 'On the left is the illumination and on the\\nright the reflectance, to be compared with (b).', 'This result is fairly typical: in some\\ncases the prediction is even better, in others it is worse.', 'different input vectors s by adding together different random T and e vectors, according\\nto Eq.', '2.', 'Each vector r represents a pattern of step changes across space, corresponding\\nto one column of a reHectance image.', 'The step changes occur at random pixels and\\nare of random amplitude between set minimum and maximum values.', 'Each vector t\\nrepresents a smooth gradient across space with a random offset and slope, correspondin~\\nto one column of an illumination image.', 'We th~n arrange the training vectors sand r\\nas the columns of two matrices Sand R, resp~ti?..', 'ely.', 'Our goal is then to compute the\\noptimal solution L of\\n\\nLS\\n\\n=R\\n\\nwhere L is a linear operator represented as a matrix.', '625\\n\\nIt is well known that the solution of this equation that is optimal in the least\\n\\nsquares sense is\\n\\n( 4)\\nwhere S+ is the Moore-Penrose pseudoinverse 11.', 'We compute the pseudoinverse by\\noverconstraining the problem - using many more training vectors than there are number\\nof pixels in each vector - and using the straightforward formula that applies in the\\noverconstrained case 12: S+ ST(SST)-l.\\nThe operator L computed in this way recovers a good approximation to the correct\\noutput vector r when given a new s, not part of the training set, as input (Fig.', 'Ic).', 'A second operator, estimated in the same way, recovers the illumination e. Acting on\\na random two-dimensional Mondrian L also yields a satisfactory approximation to the\\ncorrect output image.', 'Our estimation scheme successfully synthesizes an algorithm that performs the\\nlightness computation in a Mondrian world.', 'What is the algorithm and what is its\\nrelationship to other lightness algorithms?', 'To answer these questions we examine the\\nstructure of the matrix L. We assume that, although the operator is not a convolution\\noperator, it should approximate one far from the boundaries of the image.', 'That is,\\nin its central part, the operator should be space-invariant, performing the same action\\non each point in the image.', 'Each row in the central part of L should therefore be\\nthe same as the row above but displaced by one element to the right.', 'Inspection of\\nthe matrix confirmes this expectation.', 'To find the form of L in its center, we thus\\naverage the rows there, first shifting them appropriately.', 'The result, shown in Fig.', '2,\\nis a space-invariant filter with a narrow positive peak and a broad, shallow, negative\\nsurround.', \"Interestingly, the filter our scheme synthesizes is very similar to Land's most recent\\nretinex operator 5, which divides the image irradiance at each pixel by a weighted\\naverage of the irradiance at all pixels in a large surround and takes the logarithm of\\nthat result to yield lightness 13.\", 'The lightness triplets computed by the retinex operator\\nagree well with human perception in a Mondrian world.', \"The retinex operator and our\\nmatrix L both differ from Land's earlier retinex algorithms, which require a non-linear\\nthresholding step to eliminate smooth gradients of illumination.\", 'The shape of the filter in Fig.', '2, particularly of its large surround, is also suggestive of the \"nonclassical\" receptive fields that have been found in V4, a cortical area\\nimplicated in mechanisms underlying color constancy 14-17.', 'The form of the space-invariant filter is similar to that derived in our earlier formal\\nanalysis of the lightness problem 8.', 'It is qualitatively the same as that which results\\nfrom the direct application of regularization methods exploiting the spatial constraints\\non reflectance and illumination described above 9.18.19.', 'The Fourier transform of the\\nfilter of Fig.', '2 is approximately a bandpass filter that cuts out low frequencies due\\n\\n=\\n\\n\\x0c626\\n\\n~\\n\\n0\\n\\n( .)', \"~\\n\\nC'\\nC\\n\\n-\\n\\n-80\\n\\ns::.\", \"0\\nPi xe Is\\n\\n.2'\\n\\n~\\n\\na\\n\\n-80\\n\\n-----------\\n\\no\\n\\n+80\\n\\nPixels\\nFig.\", '2.', 'The space-invariant part of the estimated operator, obtained by shifting and\\naveraging the rows of a 160-pixel-wide central square of the matrix L, trained on a set\\nof 1500 examples with linear illumination gradients (see Fig.', '1).', 'When logarithmic\\nillumination gradients are used , a qualitatively similar receptive field is obtained.', 'In\\na separate experiment we use a training set of one-dimensional Mondrians with either\\nlinear illumination gradients or slowly varying sinusoidal illumination components\\nwith random wavelength, phase and amplitude.', 'T he resulting filter is shown in\\nthe inset.', 'The surrounds of both filters extend beyond the range we can estimate\\nreliably, the range we show here.', 'to slow gradients of illumination and preserves intennediate frequencies due to step\\nchanges in reflectance.', 'In contrast, the operator that recovers the illumination, e.\\ntakes the form of a low-pass filter.', '\\\\Ve stress that the entire operator L is not a\\nspace-invariant filter.', 'In this context, it is clear that the shape of the estimated operator should vary with\\nthe type of illumination gradient in the training set.', 'We synthesize a second operator\\nusing a new set of examples that contain equal numbers of vectors with random, sinusoidally varying illumination components and VE\"(tors with random, linear illumination\\ngradients.', \"Whereas the first operator, synthE.>Sized from examples with strictly linear\\nillumination gradients, has a broad negative surround that remains virtually constant\\nthroughout its extent, the new operator's surround (Fig .\", '2, inset) has a smaller ext(,111\\n\\n\\x0c627\\n\\nand decays smoothly towards zero from its peak negative value in its center.', 'We also apply the operator in Fig.', '2 to new input vectors in which the density\\nand amplitude of the step changes of reflectance differ greatly from those on which the\\noperator is trained.', 'The operator performs well, for example, on an input vector representing one column of an image of a small patch of one reflectance against a uniform\\nbackground of a different reflectance, the entire image under a linear illumination gradient.', 'This result is consistent with psychophysical experiments that show that color\\nconstancy of a patch holds when its Mondrian background is replaced by an equivalent\\ngrey background 20.', 'The operator also produces simultaneous brightness contrast, as expected from the\\nshape and sign of its surround.', 'The output reflectance it computes for a patch of fixed\\ninput reflectance decreases linearly with increasing average irradiance of the input test\\nvector in which the patch appears.', 'Similarly, to us, a dark patch appears darker when\\nagainst a light background than against a dark one.', 'This result takes one step towards explaining such illusions as the Koffka Ring 21.', 'A uniform gray annulus against a bipartite background (Fig.', '3a) appears to split into\\ntwo halves of different lightnesses when the midline between the light and dark halves\\nof the background is drawn across the annulus (Fig.', '3b).', 'The estimated operator\\nacting on the Koffka Ring of Fig.', '3b reproduces our perception by assigning a lower\\noutput reflectance to the left half of the annulus (which appears darker to us) than to\\nthe right half 22.', 'Yet the operator gives this brightness contrast effect whether or not\\nthe midline is drawn across the annulus (Fig.', '3c).', \"Becau~e the opf'rator can perform\\nonly a linear transformation between the input and output images, it is not surprising\\nthat the addition of the midline in the input evokes so little change in the output.\", 'These results demonstrate that the linear operator alone cannot compute lightness in\\nall worlds and suggest that an additional operator might be necessary to mark and\\nguide it within bounded regions.', 'Our estimation procedure is motivated by our previous observation 9.23,18 that\\nstandard regularization algorithms 19 in early vision define linear mappings between\\ninput and output and therefore can be estimated associatively under certain condi?', 'tions.', 'The technique of optimal linear estimation that we use is closely related to\\noptimal Bayesian estimation 9.', 'If we were to assume from the start that the optimal\\nlinear operator is space-invariant, we could considerably simplify (and streamline) the\\ncomputation by using standard correlation te<:hniques 9.24.', 'How does our estimation technique compare with other methods of \"learning\" a\\nlightness algorithm?', 'We can compute the r~ularized pseudoinverse using gradient\\ndescent on a \"neural\" network 25 with linf\\'ar units.', 'Since the pseudoinverse is lhf\"\\nunique best linear approximation in the L1 norm.', 'a gradient descent method that\\n\\n\\x0c628\\n\\nminimizes the square error between the actual output and desired output of a fully\\nconnected linear network is guaranteed to converge, albeit slowly.', 'Thus gradient descent in weight space converges to the same result as our first technique, the global\\nminimum.', 'b\\n\\na\\n\\nc\\n\\n.n\\n\\nsa\\n\\n0\\n.It\\n\\n.sa\\n\\n_\\n\\nut\\n\\n_\\n\\ninput data\\n\\npixel\\n\\n...~~\\n\\n~:==\\n:iu II ...\\n\\n,,\\n\\ne.\\n\\nsa\\n\\n.', '~\\n\\n-\\n\\n~\\n\\n.', '_\\n\\n.', \"ut\\n\\n_\\n\\noutput reflectance - with edge\\n\\n~'~\\n...:i'I~~\\nI'\\n_ .\", \".'\", '- ..\\n\\n=..\\ne. \"\\n~.', '.', '_\\nI\\n\\n~\\n\\n~\\n\\n(\\n\\nut\\n\\n,\\n\\n_\\n\\noutput reflectance - without edge\\n\\nFig.', '3.', '(a) Koffka Ring.', '(b) Koftka Ring with\\nmidline drawn across annulus.', '(c) Horizontal\\nscan lines across Koffka Ring.', 'Top: Scan\\nline starting at arrow in (b).', 'Middle: Scan\\nline at corresponding location in the output of\\nlinear operator acting on (b).', 'Bottom: Scan line\\nat same location in the output of operator acting\\non (a).', '629\\n\\nWe also compare the linear estimation technique with a \"backpropagation\" network: gradient descent on a 2-layer network with sigmoid units 25 (32 inputs, 32\\n\"hidden units\", and 32 linear outputs), using training vectors 32 pixels long.', 'The network requires an order of magnitude more time to converge to a stable configuration\\nthan does the linear estimator for the same set of 32-pixel examples.', \"The network's\\nperformance is slightly, yet consistently, better, measured as the root-mean-square error in output, averaged over sets of at least 2000 new input vectors.\", 'Interestingly, the\\nbackpropagation network and the linear estimator err in the same way on the same\\ninput vectors.', 'It is possible that the backpropagation network may show considerable\\ninprovement over the linear estimator in a world more complex than the Mondrian one.', 'We are presently examining its performance on images with real-world features such\\nas shading, shadows, and highlights26.', 'We do not think that our results mean that color constancy may be learned during\\na critical period by biological organisms.', 'It seems more reasonable to consider them\\nsimply as a demonstration on a toy world that in the course of evolution a visual system\\nmay recover and exploit natural constraints hidden in the physics of the world.', 'The\\nsignificance of our results lies in the facts that a simple statistical technique may be used\\nto synthesize a lightness algorithm from examples; that the technique does as well as\\nother techniques such as backpropagation; and that a similar technique may be used for\\nother problems in early vision.', \"Furthermore, the synthesized operator resembles both\\nLand's psychophysically-tested retinex operator and a neuronal nonclassical receptive\\nfield.\", \"The operator's properties suggest that simultaneous color (or brightness) contrast\\nmight be the result of the visual system's attempt to discount illumination gradients\\n27\\n\\nREFERENCES AND NOTES\\n1.\", 'Since we do not have perfect color constancy, our visual system must not extract\\nreflectance exactly.', 'The limits on color constancy might reveal limits on the underlying\\ncomputation.', '2.', '3.', '4.\\nand S.\\n5.', '6.', 'E.H. Land, Am.', 'Sci.', '52,247 (1964).', 'E.H. Land and J.J. McCann, J. Opt.', 'Soc.', 'Am.', '61, 1 {1971}.', 'E.H. Land, in Central and Peripheral Mechanisms of Colour Vision, T. Ottoson\\nZeki, Eds., (Macmillan, New York, 1985), pp.', '5-17.', 'E.H. Land, Proc.', 'Nat.', 'Acad.', 'Sci.', 'USA 83, 3078 (1986).', 'B.K.P.', 'Hom, Computer Graphics and Image Processing 3, 277 (1974).', '630\\n\\n7.', 'A. Blake, in Central and Peripheral Mechanisms of Colour Vision, T. Ottoson\\nand S. Zeki, Eds., (Macmillan, New York, 1985), pp.', '45-59.', '8.', 'A. Hurlbert, J. Opt.', 'Soc.', 'Am.', 'A 3,1684 (1986).', '9.', 'A. Hurlbert and T. Poggio, ArtificiaLIntelligence Laboratory Memo 909, (M.LT.,\\nCambridge, MA, 1987).', \"10. r'{x,y) can be recovered at best only to within a constant, since Eq.\", \"1\\nis invariant under the transformation of r' int.o ar' and e' into a-ie', where a is a\\nconstant.\", '11.', 'A. Albert, Regression and the Moore-Penrose Pseudoinllerse, (Academic Press,\\nNew York, 1972).', '12.', 'The pseudoinverse, and therefore L, may also be computed by recursive techniques that improve its form as more data become available l l .', '13.', \"Our synthesized filter is not exactly identical with Land's: the filter of Fig.\", '2 subtracts from the value at each point the average value of the logarithm of irradiance at all pixels, rather than the logarithm of the average values.', \"The estimated\\noperator is therefore linear in the logarithms, whereas Land's is not.\", 'The numerical\\ndifference between the outputs of the two filters is small in most cases (Land, personal\\ncommunication), and both agree well with psychophysical results.', '14.', 'R. Desimone, S.J.', 'Schein, J. Moran and L.G.', 'Ungerleider, Vision Res.', '25,\\n441 (1985).', '15.', 'H.M. Wild, S.R.', 'Butler, D. Carden and J.J. Kulikowski, Nature (London) 313,\\n133 (1985).', '16.', 'S.M.', 'Zeki, Neuroscience 9, 741 (1983).', '17.', 'S.M.', 'Zeki, Neuroscience 9, 767 (1983).', '18.', 'T. Poggio, et.', \"al, in Proceedings Image Understanding Workshop, L. Baumann, Ed., (Science Applications International Corporation, McLean, VA, 1985), pp.'\", '25-39.', '19.', 'T. Poggio, V. Torre and C. Koch, Nature (London) 317,314 (1985).', '20.', 'A. Valberg and B. Lange-Malecki, Investigative Ophthalmology and Visual\\nScience Supplement 28, 92 (1987).', '21.', 'K. Koffka, Principles of Gestalt Psychology, (Harcourt, Brace and Co., New\\nYork, 1935).', '22.', 'Note that the operator achieves this effect by subtracting a non-existent illumination gradient from the input signal.', '23.', 'T. Poggio and A. Hurlbert, Artificial Intelligence Laboratory Working Paper\\n264, (M.LT., Cambridge, MA, 1984).', '24.', 'Estimation of the operator on two-dimensional examples is possible, but computationally very expensive if done in the same way.', 'The present computer simulations\\nrequire several hours when run on standard serial computers.', 'The two-dimensional case\\n\\n\\x0c631\\n\\nwill need much more time (our one-dimensional estimation scheme runs orders of magnitude faster on a CM-1 Connection Machine System with 16K-processors).', '25.', 'D. E. Rumelhart, G.E.', 'Hinton and R.J. Williams, Nature (London) 323, 533\\n(1986 ).', '26.', 'A. Hurlbert, The Computation of Color, Ph.D. Thesis, M.l.', 'T., Cambridge,\\nMA, in preparation.', '2i.', 'We are grateful to E. Land, E. Hildreth, .J.', 'Little, F. Wilczek and D. Hillis\\nfor reading the draft and for useful discussions.', 'A. Rottenberg developed the routines\\nfor matrix operations that we used on the Connection Machine.', 'T. Breuel wrote the\\nbackpropagation simulator.', '\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURAL NETWORKS\\nPaolo Gaudiano\\nDept.', 'of Aerospace Engineering Sciences,\\nUniversity of Colorado, Boulder CO 80309, USA\\nJanuary 5, 1988\\n\\nAbstract\\nPatterns of activity over real neural structures are known to exhibit timedependent behavior.', 'It would seem that the brain may be capable of utilizing\\ntemporal behavior of activity in neural networks as a way of performing functions\\nwhich cannot otherwise be easily implemented.', 'These might include the origination\\nof sequential behavior and the recognition of time-dependent stimuli.', 'A model is\\npresented here which uses neuronal populations with recurrent feedback connections in an attempt to observe and describe the resulting time-dependent behavior.', 'Shortcomings and problems inherent to this model are discussed.', 'Current models\\nby other researchers are reviewed and their similarities and differences discussed.', 'METHODS / PRELIMINARY RESULTS\\nIn previous papers,[2,3] computer models were presented that simulate a net consisting of two spatially organized populations of realistic neurons.', 'The populations are\\nrichly interconnected and are shown to exhibit internally sustained activity.', 'It was\\nshown that if the neurons have response times significantly shorter than the typical unit\\ntime characteristic of the input patterns (usually 1 msec), the populations will exhibit\\ntime-dependent behavior.', 'This will typically result in the net falling into a limit cycle.', 'By a limit cycle, it is meant that the population falls into activity patterns during which\\nall of the active cells fire in a cyclic, periodic fashion.', 'Although the period of firing of\\nthe individual cells may be different, after a fixed time the overall population activity\\nwill repeat in a cyclic, periodic fashion.', 'For populations organized in 7x7 grids, the\\nlimit cycle will usually start 20~200 msec after the input is turned off, and its period\\nwill be in the order of 20-100 msec.', 'The point ofinterest is that ifthe net is allowed to undergo synaptic modifications by\\nmeans of a modified hebbian learning rule while being presented with a specific spatial\\npattern (i.e., cells at specific spatial locations within the net are externally stimulated),\\nsubsequent presentations of the same pattern with different temporal characteristics\\nwill cause the population to recall patterns which are spatially identical (the same cells\\nwill be active) but which have different temporal qualities.', 'In other words, the net can\\nfall into a different limit cycle.', 'These limit cycles seem to behave as attractors in that\\nsimilar input patterns will result in the same limit cycle, and hence each distinct limit\\ncycle appears to have a basin of attraction.', 'Hence a net which can only learn a small\\n\\n?', 'American Institute of Physics 1988\\n\\n\\x0c298\\n\\nnumber of spatially distinct patterns can recall the patterns in a number of temporal\\nmodes.', 'If it were possible to quantitatively discriminate between such temporal modes,\\nit would seem reasonable to speculate that different limit cycles could correspond to\\ndifferent memory traces.', 'This would significantly increase estimates on the capacity of\\nmemory storage in the net.', 'It has also been shown that a net being presented with a given pattern will fall and\\nstay into a limit cycle until another pattern is presented which will cause the system\\nto fall into a different basin of attraction.', 'If no other patterns are presented, the net\\nwill remain in the same limit cycle indefinitely.', 'Furthermore, the net will fall into the\\nsame limit cycle independently of the duration of the input stimulus, so long as the\\ninput stimulus is presented for a long enough time to raise the population activity level\\nbeyond a minimum necessary to achieve self-sustained activity.', 'Hence, if we suppose\\nthat the net \"recognizes\" the input when it falls into the corresponding limit cycle, it\\nfollows that the net will recognize a string of input patterns regardless of the duration of\\neach input pattern, so long as each input is presented long enough for the net to fall into\\nthe appropriate limit cycle.', 'In particular, our system is capable of falling into a limit\\ncycle within some tens of milliseconds.', 'This can be fast enough to encode, for example, a\\nstring of phonemes as would typically be found in continuous speech.', \"It may be possible,\\nfor instance, to create a model similar to Rumelhart and McClelland's 1981 model on\\nword recognition by appropriately connecting multiple layers of these networks.\", 'If the\\nresponse time of the cells were increased in higher layers, it may be possible to have\\nthe lowest level respond to stimuli quickly enough to distinguish phonemes (or some\\nsub-phonemic basic linguistic unit), then have populations from this first level feed into\\na slower, word-recognizing population layer, and so On.', 'Such a model may be able to\\nperform word recognition from an input consisting of continuous phoneme strings even\\nwhen the phonemes may vary in duration of presentation.', 'SHORTCOMINGS\\nUnfortunately, it was noticed a short time ago that a consistent mistake had been\\nmade in the process of obtaining the above-mentioned results.', 'Namely, in the process\\nof decreasing the response time of the cells I accidentally reached a response time below\\nthe time step used in the numerical approximation that updates the state of each cell\\nduring a simulation.', 'The equations that describe the state of each cell depend on the\\nstate of the cell at the previous time step as well as on the input at the present time.', 'These equations are of first order in time, and an explicit discrete approximation is\\nused in the model.', 'Unfortunately it is a known fact that care must be taken in selecting\\nthe size of the time step in order to obtain reliable results.', 'It is infact the case that\\nby reducing the time step to a level below the response time of the cells the dynamics\\nof the system varied significantly.', 'It is questionable whether it would be possible to\\nadjust some of the population parameters within reson to obtain the same results with\\na smaller step size, but the following points should be taken into account: 1) other\\nresearchers have created similar models that show such cyclic behavior (see for example\\nSilverman, Shaw and Pearson[7]).', '2) biological data exists which would indicate the\\nexistance of cyclic or periodic bahvior in real neural systems (see for instance Baird[1]).', 'As I just recently completed a series of studies at this university, I will not be able\\nto perform a detailed examination of the system described here, but instead I will more\\n\\n\\x0c299\\n\\nthan likely create new models on different research equipment which will be geared more\\nspecifically towards the study of temporal behavior in neural networks.', 'OTHER MODELS\\nIt should be noted that in the past few years some researchers have begun investigating the possibility of neural networks that can exhibit time-dependent behavior,\\nand I would like to report on some of the available results as they relate to the topic of\\ntemporal patterns.', \"Baird[l] reports findings from the rabbit's olfctory bulb which indicate the existance of phase-locked oscillatory states corresponding to olfactory stimuli\\npresented to the subjects.\", 'He outlines an elegant model which attributes pattern recognition abilities to competing instabilities in the dynamic activity of neural structures.', 'He further speculates that inhomogeneous connectivity in the bulb can be selectively\\nmodified to achieve input-sensitive oscillatory states.', 'Silverman, Shaw and Pearson[7] have developed a model based on a biologically-inspired\\nidealized neural structure, which they call the trion.', 'This unit represents a localized\\ngroup of neurons with a discrete firing period.', 'It was found that small ensembles of trions with symmetric connections can exhibit quasi-stable periodic firing patterns which\\ndo not require pacemakers or external driving.', 'Their results are inspired by existing\\nphysiological data and are consistent with other works.', 'Kleinfeld[6], and Sompolinsky and Kanter[8] independently developed neural network\\nmodels that can generate and recognize sequential or cyclic patterns.', 'Both models rely\\non what could be summarized as the recirculation of information through time-delayed\\nchannels.', 'Very similar results are presented by Jordan[4] who extends a typical connectionist or\\nPDP model to include state and plan units with recurrent connections and feedback\\nfrom output units through hidden units.', 'He employs supervised learning with fuzzy\\nconstraints to induce learning of sequences in the system.', 'From a slightly different approach, Tank and Hopfield[9] make USe of patterned sets\\nof delays which effectively compress information in time.', 'They develop a model which\\nrecognizes patterns by falling into local minima of a state-space energy function.', 'They\\nsuggest that a systematic selection of delay functions can be done which will allow for\\ntime distortions that would be likely to occur in the input.', 'Finally, a somewhat different approach is taken by Homma, Atlas and Marks[5], who\\ngeneralize a network for spatial pattern recognition to one that performs spatio-temporal\\npatterns by extending classical principles from spatial networks to dynamic networks.', 'In particular, they replace multiplication with convolution, weights with transfer functions, and thresholding with non linear transforms.', 'Hebbian and Delta learning rules\\nare similarly generalized.', 'The resulting models are able to perform temporal pattern\\nrecognition.', 'The above is only a partial list of some of the relevant work in this field, and there\\nare probably various other results I am not aware of.', 'DISCUSSION\\nAll of the above results indicate the importance of temporal patterns in neural networks.', 'The need is apparent for further formal models which can successfully quantify\\ntemporal behavior in neural networks.', 'Several questions must be answered to further\\n\\n\\x0c300\\n\\nclarify the role and meaning of temporal patterns in neural nets.', 'For instance, there\\nis an apparent difference between a model that performs sequential tasks and one that\\nperforms recognition of dynamic patterns.', 'It seems that appropriate selection of delay\\nmechanisms will be necessary to account for many types of temporal pattern recognition.', 'The question of scaling must also be explored: mechanism are known to exist in\\nthe brain which can cause delays ranging from the millisecond-range (e.g.', 'variations\\nin synaptic cleft size) to the tenth of a second range (e.g.', 'axonal transmission times).', 'On the other hand, the brain is capable of rec\"Ignizing sequences of stimuli that can be\\nmuch longer than the typical neural event, such as for instance being able to remember\\na song in its entirety.', 'These and other questions could lead to interesting new aspects\\nof brain function which are presently unclear.', 'References\\n[1] Baird, B., \"Nonlinear Dynamics of Pattern Formation and Pattern Recognition in\\nthe Rabbit Olfactory Bulb\".', 'Physica 22D, 150-175.', '1986.', '[2] Gaudiano, P., \"Computer Models of Neural Networks\".', \"Unpublished Master's Thesis.\", 'University of Colorado.', '1987.', '[3] Gaudiano, P., MacGregor, R.J., \"Dynamic Activity and Memory Traces in\\nComputer-Simulated Recurrently-Connected Neural Networks\".', 'Proceedings of the\\nFirst International Conference on Neural Networks.', '2:177-185.', '1987.', '[4] Jordan, M.I., \"Attractor Dynamics and Parallelism in a Connectionist Sequential\\nMachine\".', 'Proceedings of the Eighth Annual Conference of the Cognitive Sciences\\nSociety.', '1986.', '[5] Homma, T., Atlas, L.E., Marks, R.J.II, \"An Artificial Neural Network for SpatioTemporal Bipolar Patterns: Application to Phoneme Classification\".', 'To appear in\\nproceedings of Neural Information Processing Systems Conference (AlP).', '1987.', '[6] Kleinfeld, D., \"Sequential State Generation by Model Neural Networks\".', 'Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA.', '83: 9469-9473.', '1986.', '[7] Silverman, D.l., Shaw, G.L., Pearson, l.C.', '\"Associative Recall Properties of the\\nTrion Model of Cortical Organization\".', 'Biol.', 'Cybern.', '53:259-271.', '1986.', '[8] Sompolinsky, H., Kanter, I.', '\"Temporal Association in Asymmetric Neural Networks\".', 'Phys.', 'Rev.', 'Let.', '57:2861-2864.', '1986.', '[9] Tank, D.W., Hopfield, l.l.', '\"Neural Computation by Concentrating Information in\\nTime\".', 'Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA.', '84:1896-1900.', '1987.', '\\n\\nUSING NEURAL NETWORKS TO IMPROVE\\nCOCHLEAR IMPLANT SPEECH PERCEPTION\\nManoel F. Tenorio\\nSchool of Electrical Engineering\\nPurdue University\\nWest Lafayette, IN 47907\\n\\nABSTRACT\\n\\n-\\n\\nAn increasing number of profoundly deaf patients suffering from sensorineural deafness are using cochlear implants as prostheses.', 'Mter the\\nimplant, sound can be detected through the electrical stimulation of the\\nremaining peripheral auditory nervous system.', 'Although great progress has\\nbeen achieved in this area, no useful speech recognition has been attained\\nwith either single or multiple channel cochlear implants.', 'Coding evidence suggests that it is necessary for any implant which\\nwould effectively couple with the natural speech perception system to simulate the temporal dispersion and other phenomena found in the natural\\nreceptors, and currently not implemented in any cochlear implants.', 'To this\\nend, it is presented here a computational model using artificial neural networks (ANN) to incorporate the natural phenomena in the artificial\\ncochlear.', 'The ANN model presents a series of advantages to the implementation\\nof such systems.', 'First, the hardware requirements, with constraints on\\npower, size, and processing speeds, can be taken into account together with\\nthe development of the underlining software, before the actual neural structures are totally defined.', 'Second, the ANN model, since it is an abstraction\\nof natural neurons, carries the necessary ingredients and is a close mapping\\nfor implementing the necessary functions.', 'Third, some of the processing,\\nlike sorting and majority functions, could be implemented more efficiently,\\nrequiring only local decisions.', 'Fourth, the ANN model allows function\\nmodifications through parametric modification (no software recoding), which\\npermits a variety of fine-tuning experiments, with the opinion of the\\npatients, to be conceived.', \"Some of those will permit the user some freedom\\nin system modification at real-time, allowing finer and more subjective\\nadjustments to fit differences on the condition and operation of individual's\\nremaining peripheral auditory system.\", '1.', 'INTRODUCTION\\n\\nThe study of the model of sensory receptors can be carried out either\\nvia trying to understand how the natural receptors process incoming signals\\nand build a representation code, or via the construction of artificial replacements.', 'In the second case, we are interested in to what extent those\\nartificial counterparts have the ability to replace the natural receptors.', 'Several groups are now carrying out the design of artificial sensors.', 'Artificial cochleas seem to have a number of different designs and a tradition\\nof experiments.', 'These make them now available for widespread use as\\nprostheses for patients who have sensorineural deafness caused by hair cell\\ndamage.', '?', 'American Institute of Physics 1988\\n\\n\\x0c784\\n\\nAlthough surgery is required for such implants, their performance has\\nreached a level of maturity to induce patients to seek out these devices\\nvoluntarily.', 'Unfortunately, only partial acoustic information is obtained by\\nseverely deaf patients with cochlear prosthesis.', \"Useful patterns for speech\\ncommunication are not yet 'fully recognizable through auditory prostheses.\", 'This problem with artificial receptors is true for both single implants, that\\nstimulate large sections of the cochlea with signals that cover a large portion\\nof the spectrum [4,5], and multi channel implants, that stimulate specific\\nregions of the cochlea with specific portions of the auditory spectrum [3,13].', 'In this paper, we tackle the problem of artificial cochlear implants\\nthrough the used of neurocomputing tools.', 'The receptor model used here\\nwas developed by Gerald Wasserman of the Sensory Coding Laboratory,\\nDepartment of Psychological Sciences, Purdue University [20], and the\\nimplants were performed by Richard Miyamoto of the Department of Otolaryngology, Indiana University Medical School [11].', 'The idea is to introduce with the cochlear implant, the computation\\nthat would be performed otherwise by the natural receptors.', 'It would therefore be possible to experimentally manipulate the properties of the implant\\nand measure the effect of coding variations on behavior.', 'The model was\\nconstrained to be portable, simple to implant, fast enough computationally\\nfor on-line use, and built with a flexible paradigm, which would allow for\\nmodification of the different parts of the model, without having to reconstruct it entirely.', 'In the next section, we review parts of the receptor model,\\nand discuss the block diagram of the implant.', 'Section 3 covers the limitations associated with the technique, and discusses the r.e sults obtained with\\na single neuron and one feedback loop.', 'Section 4 discusses the implementations of these models using feedforward neural networks, and the computational advantages for doing so.', '2.', 'COCHLEAR IMPLANTS AND THE NEURON MODEL\\nAlthough patients cannot reliably recognize randomly chosen spoken\\nwords to them (when implanted with either multichannel or single channel\\ndevices), this is not to say that no information is extracted from speech.', 'If\\nthe vocabulary is reduced to a limited set of words, patients perform\\nsignificantly better than chance, at associating the word with a member of\\nthe set.', 'For these types of experiments, single channel implants correspond to\\nreported performance of 14% to 20% better than chance, with 62% performance being the highest reported.', 'For multiple channels, performances of\\n95% were reported.', 'So far no one has investigated the differences in performance between the two types of implants.', 'Since the two implants have so\\nmany differences, it is difficult to point out the cause for the better performance in the multiple channel case.', 'The results of such experiments are encouraging, and point to the fact\\nthat cochlea implants need only minor improvement to be able to mediate\\nad-lib speech perception successfully.', 'Sensory coding studies have suggested\\na solution to the implant problem, by showing that the representation code\\ngenerated by the sensory system is task dependent.', 'This evidence came\\nfrom comparison of intracellular recordings taken from a single receptor of\\nintact subjects.', 'This coding evidence suggests that the temporal dispersion (time\\nintegration) found in natural receptors would be a necessary part of any\\n\\n\\x0c785\\n\\ncochlear implant.', 'Present cochlear implants have no dispersion at all.', 'Figure 2 shows the block diagram for a representative cochlear implant, the\\nHouse-Urban stimulator.', 'The acoustic signal is picked up by the microphone, which sends it to an AM oscillator.', 'This modulation step is necessary to induce an electro-magnetic coupling between the external and internal coil.', 'The internal coil has been surgically implanted, and it is connected\\nto a pair of wires implanted inside and outside the cochlea.', 'Just incorporating the temporal dispersion model to an existing device\\nwould not replicate the fact that in natural receptors, temporal dispersion\\nappears in conjunction to other operations which are strongly non linear.', 'There are operations like selection of a portion of the spectrum, rectification,\\ncompression, and time-dispersion to be considered.', 'In figure 3, a modified implant is shown, which takes into consideration\\nsome of these operations.', 'It is depicted as a single-channel implant,\\nalthough the ultimate goal is to make it multichannel.', 'Details of the operation of this device can be found elsewhere [21].', 'Here, it is important to mention that the implant would also have a compression/rectification function,\\nand it would receive a feedback from the integrator stage in order to control\\nits gain.', '3.', 'CHARACTERISTICS AND RESULTS OF THE IMPLANTS\\nThe above model has been implemented as an off-line process, and then\\nthe patients were exposed to a preprocessed signal which emulated the\\noperation of the device.', 'It is not easy to define the amount of feedback\\nneeded in the system or the amount of time dispersion.', 'It could also be that\\nthese parameters are variable across different conditions.', 'Another variance\\nin the experiment is the amount of damage (and type) among different individuals.', 'So, these parameters have to be determined clinically.', 'The coupling between the artificial receptor and the natural system also\\npresents problems.', 'If a physical connection is used, it increases the risk of\\ninfections.', 'When inductive methods are used, the coupling is never ideal.', 'If\\nportability and limited power is of concern in the implementation, then the\\nlimited energy available for coupling has to be used very effectively.', 'The computation of the receptor model has to be made in a way to\\nallow for fast implementation.', 'The signal transformation is to be computed\\non-line.', 'Also, the results from clinical studies should be able to be incorpora ted fairly easily without having to reengineer the implant.', 'Now we present the results of the implementation of the transfer function of figure 4.', 'Patients, drawn from a population described elsewhere\\n[11,12,14], were given spoken sentences processed off-line, and simultaneously\\npresented with a couple of words related to the context.', 'Only one of them\\nwas the correct answer.', 'The patient had two buttons, one for each alternative; he/she was to press the button which corresponded to the correct alternative.', 'The results are shown in the tables below.', 'Patient 1 (Average of the population)\\nPercentage of correct alternatives\\nDispersion\\nNo disp.', '0.1 msec\\n0.3 msec\\n\\n67%\\n78%\\n85%\\n\\nBest performance\\n\\n\\x0c786\\n\\n1 msec\\n3 msec\\n\\n76%\\n72%\\n\\nTable I: Phoneme discrimination in\\n\\nd.\\n\\ntwo-alternate task.', 'Patient 2\\n.', 'D ?lsperSlOn\\n\\nNo disp.', '1.0 msec\\n\\nPercentage of correct alternatives\\n50%\\n76%\\n\\nBest performance\\n\\nTable II: Sentence comprehension in a two-alternative task.', 'There were quite a lot of variations in the performance of the different\\npatients, some been able to perform better at different dispersion and\\ncompression amounts than the average of the population.', 'Since one cannot\\ncontrol the amount of damage in the system of each patient or differences in\\nindividuals, it is hard to predict the ideal values for a given patient.', 'Nevertheless, the improvements observed are of undeniable value in improving speech perception.', '4.', 'THE NEUROCOMPUTING MODEL\\nIn studying the implementation of such a system for on-line use, yet\\nflexible enough to produce a carry-on device, we look at feedforward neurocomputer models as a possible answer.', 'First, we wanted a model that easily\\nproduced a parallel implementation, so that the model could be expanded in\\na multichannel environment without compromising the speed of the system.', 'Figure 5 shows the initial idea for the implementation of the device as a Single Instruction Multiple Data (SIMD) architecture.', 'The implant would be similar to the one described in Figure 4, except\\nthat the transfer function of the receptor would be performed by a two layer\\nfeed forward network (Figure 6).', 'Since there is no way of finding out the\\nvalues of compression and dispersion a part from clinical trials, or even if\\nthese values do change in certain conditions, we need to create a structure\\nthat is flexible enough to modify the program structure by simple manipulation of parameters.', 'This is also the same problem we would face when trying to expand the system to a multichannel implant.', 'Again, neuromorphic\\nmodels provided a nice paradigm in which the dataflow and the function of\\nthe program could be altered by simple parameter (weight) change.', 'For this first implementation we chose to use the no-contact inductive\\ncoupling method.', 'The drawback of this method is that all the information\\nhas to be compressed in a single channel for reliable transmission and cross\\ntalk elimination.', 'Since the inductive coupling of the implant?', 'is critical at every cycle, the\\nmost relevant information must be picked out of the processed signal.', 'This\\ninformation is then given all the available energy, and after all the coupling\\nloss, it should be sufficient to provide for speech pattern discrimination.', 'In a\\nmultichannel setting, this corresponds to doing a sorting of all the n signals\\nin the channels, selecting the m highest signals, and adding them up for\\nmodulation.', 'In a naive single processor implementation, this could\\ncorrespond to n 2 comparisons, and in a multiprocessor implementation,\\nlog(n) comparisons.', 'Both are dependent on the number of signals to be\\n\\n\\x0c787\\n\\nsorted.', 'We needed a scheme in which the sorting time would be constant with\\nthe number of channels, and would be easily implementable in analog circuitry, in case this became a future route.', 'Our scheme is shown in Figure 7.', 'Each channel is connected to a threshold element, whose threshold can be\\nvaried externally.', 'A monotonically decreasing function scans the threshold\\nvalues, from the highest possible value of the output to the lowest.', 'The output of these elements will be high corresponding to the values that are the\\nhighest first.', 'These output are summed with a quasi-integrator with threshold set to m. This element, when high, disables the scanning functions; and\\nit corresponds to having found the m highest signals.', 'This sorting is\\nindependent of the number of channels.', 'The output of the threshold units are fed into sigma-pi units which\\ngates the signals to be modulated.', 'The output of these units are summed\\nand correspond to the final processed signal (Figure 8).', 'The user has full control of the characteristics of this device.', 'The\\nnumber of channels can be easily altered; the number of components allowed\\nin the modulation can be changed; the amount of gain, rectificationcompression, and dispersion of each channel can also be individually controlled.', 'The entire system is easily implementable in analog integrated circuits, once the clinical tests have determine the optimum operational\\ncharacteristics.', '6.', 'CONCLUSION\\nWe have shown that the study of sensory implants can enhance our\\nunderstanding of the representation schemes used for natural sensory receptors.', 'In particular, implants can be enhanced significantly if the effects of\\nthe sensory processing and transfer functions are incorporated in the model.', 'We have also shown that neuromorphic computing paradigm provides a\\nparallel and easily modifiable framework for signal processing structures,\\nwith advantages that perhaps cannot be offered by other technology.', 'We will soon start the use of the first on-line portable model, using a\\nsingle processor.', 'This model will provide a testbed for more extensive clinical trials of the implant.', 'We will then move to the parallel implementation,\\nand from there, possibly move toward analog circuitry implementation.', 'Another route for the use of neuromorphic computing in this domain is\\npossibly the use of sensory recordings from healthy animals to train selforganizing adaptive learning networks, in order to design the implant\\ntransfer functions.', 'REFERENCES\\n\\n[1]\\n\\n[2]\\n\\nBilger, R.C.', '; Black, F.O.', '; Hopkinson, N.T.', '; and Myers, E.N.,\\n\"Implanted auditory prosthesis: An evaluation of subjects presently\\nfitted with cochlear implants,\" Otolaryngology, 1977, Vol.', '84, pp.', '677682.', 'Bilger, R.C.', '; Black, F.O.', '; Hopkinson, N.T.', '; ~~ers, E.~.', '; Payne, !.L.', ';\\nStenson, N.R.', '; Vega, A.; and Wolf, R.V., EvaluatiOn of subJects\\npresently fitted with implanted auditory prostheses,\" Annals of Otology, Rhinology, and Laryngology, 1977, Vol.', '86(Supp.', '38), pp.', '1-176.', '788\\n\\n[3]\\n\\n[4]\\n\\n[5]\\n[6]\\n\\nEddington, D.K.', '; Dobelle, W.H.', '; Brackmann, D.E.', '; Mladejovsky, M.G.', ';\\nand Parkin, J., \"Place and periodicity pitch by stimulation of multiple\\nscala tympani electrodes in deaf volunteers,\" American Society for\\nArtificial Internal Organs, Transactions, 1978, Vol.', '24, pp.', '1-5.', 'House, W.F.', '; Berliner, .K.', '; Crary, W.; Graham, M.; Luckey, R;; Norton,\\nN.; Selters, W.; Tobm, H.; Urban, J.; and Wexler, M., Cochlear\\nimplants,\" Annals of Otology, Rhinology and Laryngology, 1976, Vol.', '85(Supp.', '27), pp.', '1-93.', 'House, W.F.', 'and Urban, J., \"Long term results of electrode implantation and electronic stimulation of the cochlea in man,\" Annals of Otology, Rhinology and Laryngology, 1973, Vol.', '82, No.2, pp.', '504-517.', 'Ifukube, T. and White, R.L., \"A speech processor with lateral inhibition for an eight channel cochlear implant and its evaluation,\" IEEE\\nTrans.', 'on Biomedical Engineering, November 1987, Vol.', 'BME-34, No.', '11.', '[7]\\n[8]\\n[9]\\n[10]\\n\\n[11]\\n[12]\\n\\n[13]\\n\\n[14]\\n\\n[15]\\n\\n[16]\\n\\nKong, K.-L., and Wasserman, G.S., \"Changing response measures\\nalters temporal summation in the receptor and spike potentials of the\\nLimulus lateral eye,\" Sensory Processes, 1978, Vol.', '2, pp.', '21-31.', '(a)\\nKong, K.-L., and Wasserman, G.S., \"Temporal summation in the receptor potential of the Limulus lateral eye: Comparison between retinula\\nand eccentric cells,\" Sensory Processes, 1978, Vol.', '2, pp.', '9-20.', '(b)\\nMichelson, R.P., \"The results of electrical stimulation of the cochlea in\\nhuman sensory deafness,\" Annals of Otology, Rhinology and Laryngology, 1971, Vol.', '80, pp.', '914-919.', 'Mia dej ovsky, M.G.', '; Eddington, D.K.', '; Dobelle, W.H.', '; and Brackmann,\\nD.E., \"Artificial hearing for the deaf by cochlear stimulation: Pitch\\nmodulation and some parametric thresholds,\" American Society for\\nArtificial Internal Organs, Transactions, 1974, Vol.', '21, pp.', '1-7.', 'Miyamoto, R.T.; Gossett, S.K.', '; Groom, G.L.', '; Kienle, M.L.', '; Pope, M.L.', ';\\nand Shallop, J.K., \"Cochlear implants: An auditory prosthesis for the\\ndeaf,\" Journal of the Indiana State Medical Association, 1982, Vol.', '75,\\npp.', '174-177.', 'Miyamoto, R.T.; Myres, W.A.', '; Pope, M.L.', '; and Carotta, C.A.,\\n\"Cochlear implants for deaf children,\" Laryngoscope, 1986, Vol.', '96, pp.', '990-996.', 'Pialoux, P.; Chouard, C.H.', '; Meyer, B.; and Fu,?ain, C., \"Indications\\nand results of the multichannel cochlear implant,\\' Acta Otolaryngology,\\n1979, Vo .. 87, pp.', '185-189.', 'Robbins, A.M.i Osberger, M.J.; Miyamoto, R.T.; Kienle, M.J.; and\\nMyres, W.A., ?', 'Speech-tracking performance in single-channel cochlear\\nimplant subjects,\" JourntLl of Speech and Hearing Research, 1985, Vol.', '28, pp.', '565-578.', 'Russell, I.J.', 'and Sellick, P.M., \"The tuning properties of cochlear hair\\ncells,\" in E.F. Evans and J.P. Wilson (eds.', '), Psychophysics and Physiology 0 f Hearing, London: Academic Press, 1977.', 'Wasserman, G.S., \"Limulus psychophysics: Temporal summation in the\\nventral eye,\" Journal of Experimental Psychology: General, 1978, Vol.', '107, pp.', '276-286.', '789\\n\\n[17]\\n[18]\\n\\n[19]\\n\\n[20]\\n[21]\\n\\nWasserman, G.S., \"Limulus psychophysics: Increment threshold,\" Perception & Psychophysics, 1981, Vol.', '29, pp.', '251-260.', 'Wasserman, G.S.', '; Felsten, G.; and Easland, G.S., \"Receptor saturation\\nand the psychophysical function,\" Investigative Ophthalmology and\\nVisual Science, 1978, Vol.', '17, p. 155 (Abstract).', 'Wasserman, G.S.', '; Felsten, G.; and Easland, G.S., \"The psychophysical\\nfunction: Harmonizing Fechner and Stevens,\" Science, 1979, Vol.', '204,\\npp.', '85-87.', 'Wasserman, G.S., \"Cochlear implant codes and speech perception in\\nprofoundly deaf,\" Bulletin of Psychonomic Society, Vol.', '(18)3, 1987.', 'Wasserman, G.S.', '; Wang-Bennett, L.T.', '; and Miyamoto, R.T., \"Temporal dispersion in natural receptors and pattern discrimination mediated by artificial receptor,\" Proc.', 'of the Fechner Centennial Symposium, Hans Buffart (Ed.', '), Elsevier/North Holland, Amsterdam, 1987.\\n\\nr\\nI\\nI\\n\\n7~\\n\\n9\\n\\n~ SENSORY CODING DATA ~ - ,\\n\\n13\\n\\nRECEPTOR\\nSIGNAL\\n\\nSTIMULUS\\n\\nI\\nI\\n\\nCENTRAL\\nANALYSIS\\n\\nBEHAVIOR\\n\\n11\\n\\nI\\n\\n15\\n\\nr----------~\\n\\nI\\n\\nL\\n\\n..: PROSTHETIC :\\n-\\n\\n-\\n\\nI\\n\\nSIGNAL\\n\\n:- -\\n\\n...... -r:-.', '__ .', 'I\\n\\nI\\n\\n-\\n\\n..J\\n\\n?', '17\\n\\nFig.', '1.', 'Path of Natural and Prosthetic Signals.', 'Sound\\n\\nCentral Nervous System\\n\\nFig.', '2.', 'The House-Urban Cochlear Implant.', '790\\n\\nAMPLIFICATION\\n\\n-----1~\\n\\nCOMPRESSIVE RECTIFIER\\n\\nDISPERSION\\n\\n-----1~\\n\\nINTEGRATOR\\n\\nFig.', '3.', 'Receptor Model\\n\\nSound\\n\\nCentral Nervous System\\n\\nFig.', '4.', 'Modified Implant Model.', '791\\n\\nPORTABLE PARALLEL NEUROCOMpuTER\\n\\n16KHz AM\\nMODULATED OUTPUT\\n\\nm\\n\\nEXTERNAL\\nUSER CONTROLLED\\nPARAMETERS\\n\\nFig.', '5.', 'Initial Concept for a SIMD Architecture.', 'EXTERNALLY CONTROLLED\\nAMPUFICATION\\n\\nDISPERSION\\n\\nNEURON\\nMODEL\\n\\nSORTER\\n\\nOF\\nN SIGNALS\\n\\nNEURON\\nMODEL\\n\\nFig.', '6.', 'Feedforward Neuron Model Implant.', '792\\n\\nSORTER OF n SIGNALS IN 0(1)\\n\\nI--~ RESET SCANNING\\n\\nSIGNALS\\nINPUTS\\n\\nFUNCTION\\n\\nTHRESHOLD\\nSETOFn,\\nEXTERNALLY\\nCONTROLLED\\nTHRESHOLD CONTROL:\\nSCANNING FUNCTION FROM I imax TO I j min\\n\\nI j max\\n\\nI I. min\\n\\nFig.', '7.', 'Signal Sorting Circuit.', 'SIGNAL SELECTORS\\n\\nt---~.', '0 -leS\\n1\\n\\n1\\n\\n1\\n\\nJ----\" OUTPUT SIGNAL\\nIn\\n\\n---+---~.c\\n\\nFig.', '8.', 'Sigma-Pi Units for Signal Composition.', '793\\n\\nUSER CONTROLLED PARAMETERS\\n\\ntJ4\\n\\n13121114131\\n\\nBEST\\nMATCHES\\n\\nDISPERSION\\n\\n10101312111\\nGAIN\\n\\n~\\nFILTER\\nBYPASS\\n\\n~\\nPROCESSOR\\nBYPASS\\n\\n~\\n\\nSINGLE\\nNEURON\\nPROCESSING\\n\\nMICROPHONE\\n\\nFig.', '9.', 'Parameter Controls for Clinical Studies.', '\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS*\\nChristopher L. Scofield\\nCenter for Neural Science and Physics Department\\nBrown University\\nProvidence, Rhode Island 02912\\nand\\nNestor, Inc., 1 Richmond Square, Providence, Rhode Island,\\n02906.', 'ABSTRACT\\nA single cell theory for the development of selectivity and\\nocular dominance in visual cortex has been presented previously\\nby Bienenstock, Cooper and Munrol.', 'This has been extended to a\\nnetwork applicable to layer IV of visual cortex 2 .', 'In this paper\\nwe present a mean field approximation that captures in a fairly\\ntransparent manner the qualitative, and many of the\\nquantitative, results of the network theory.', 'Finally, we consider\\nthe application of this theory to artificial neural networks and\\nshow that a significant reduction in architectural complexity is\\npossible.', 'A SINGLE LAYER NETWORK AND THE MEAN FIELD\\nAPPROXIMATION\\nWe consider a single layer network of ideal neurons which\\nreceive signals from outside of the layer and from cells within\\nthe layer (Figure 1).', 'The activity of the ith cell in the network is\\nc\\'1 -- m\\'1 d + \"\"\\'\\n~ T .. c\\'\\n~J J\\'\\n\\nJ\\n\\n(1)\\n\\nEach cell\\nd is a vector of afferent signals to the network.', \"receives input from n fibers outside of the cortical network\\nthrough the matrix of synapses mi' Intra-layer input to each cell\\nis then transmitted through the matrix of cortico-cortical\\nsynapses L.\\n?\", 'American Institute of Physics 1988\\n\\n\\x0c684\\n\\nAfferent\\nSignals\\n\\n>\\n\\n... ..\\n\\nm2\\n\\nm1\\n\\nmn\\n\\n~\\n\\nr;.', '\",...-\\n\\nd\\n\\n.L\\n:\\n\\n1\\n\\n,~\\n\\n2\\n\\n... ..\\n\\n, ...c.. ,\\n\\n~\\n\\n~\\n\\nFigure 1: The general single layer recurrent\\nnetwork.', 'Light circles are the LGN -cortical\\nsynapses.', 'Dark circles are the (nonmodifiable) cortico-cortical synapses.', 'We now expand the response of the i th cell into individual\\nterms describing the number of cortical synapses traversed by\\nthe signal d before arriving through synapses Lij at cell i.', \"Expanding Cj in (1), the response of cell i becomes\\nci\\n\\n=mi d + l: ~j mj d + l: ~jL Ljk mk d + 2: ~j 2Ljk L Lkn mn d +... (2)\\nJ\\n\\nJ\\n\\nK\\n\\nJ\\n\\nK' n\\n\\nNote that each term contains a factor of the form\\n\\nThis factor describes the first order effect, on cell q, of the\\ncortical transformation of the signal d.\\nThe mean field\\napproximation consists of estimating this factor to be a constant,\\nindependant of cell location\\n(3)\\n\\n\\x0c685\\n\\nThis assumption does not imply that each cell in the network is\\nselective to the same pattern, (and thus that mi = mj).\", 'Rather,\\nthe assumption is that the vector sum is a constant\\n\\nThis amounts to assuming that each cell in the network is\\nsurrounded by a population of cells which represent, on average,\\nall possible pattern preferences.', 'Thus the vector sum of the\\nafferent synaptic states describing these pattern preferences is a\\nconstant independent of location.', 'Finally, if we assume that the lateral connection strengths are\\na function only of i-j then Lij becomes a circular matrix so that\\n\\nr. Lij ::: ~J Lji = Lo = constan t.\\n1\\n\\nThen the response of the cell i becomes\\n(4)\\n\\nfor I\\n\\n~\\n\\nI <1\\n\\nwhere we define the spatial average of cortical cell activity C = in\\nd, and N is the average number of intracortical synapses.', \"Here, in a manner similar to that in the theory of magnetism,\\nwe have replaced the effect of individual cortical cells by their\\naverage effect (as though all other cortical cells can be replaced\\nby an 'effective' cell, figure 2).\", \"Note that we have retained all\\norders of synaptic traversal of the signal d.\\nThus, we now focus on the activity of the layer after\\n'relaxation' to equilibrium.\", 'In the mean field approximation we\\ncan therefore write\\n(5)\\n\\nwhere the mean field\\n\\na\\nwith\\n\\n=am\\n\\n\\x0c686\\n\\nand we asume that\\ninhibitory).', 'Afferent\\nSignals\\nd\\n\\nLo < 0 (the network is,\\n\\non\\n\\naverage,\\n\\n>\\n\\nFigure 2: The single layer mean field network.', \"Detailed connectivity between all cells of the\\nnetwork is replaced with a single (nonmodifiable) synapse from an 'effective' cell.\", 'LEARNING IN THE CORTICAL NETWORK\\n\\nWe will first consider evolution of the network according to a\\nsynaptic modification rule that has been studied in detail, for\\nsingle cells, elsewhere!?', '3.', 'We consider the LGN - cortical\\nsynapses to be the site of plasticity and assume for maximum\\nsimplicity that there is no modification of cortico-cortical\\nsynapses.', 'Then\\n(6)\\n\\n.', 'Lij = O.', 'In what follows c denotes the spatial average over cortical cells,\\nwhile Cj denotes the time averaged activity of the i th cortical cell.', 'The function cj> has been discussed extensively elsewhere.', 'Here\\nwe note that cj> describes a function of the cell response that has\\nboth hebbian and anti-hebbian regions.', '687\\n\\nThis leads to a very complex set of non-linear stochastic\\nequations that have been analyzed partially elsewhere 2 .', 'In\\ngeneral, the afferent synaptic state has fixed points that are\\nstable and selective and unstable fixed points that are nonselective!, 2.', 'These arguments may now be generalized for the\\nnetwork.', 'In the mean field approximation\\n(7)\\n\\nThe mean field, a has a time dependent component m. This\\nvaries as the average over all of the network modifiable\\nsynapses and, in most environmental situations, should change\\nslowly compared to the change of the modifiable synapses to a\\nsingle cell.', 'Then in this approximation we can write\\n\\n?', \"(mi(a)-a) = cj>[mi(a) - a] d.\\n\\n(8)\\n\\nWe see that there is a mapping\\nmi' <-> mica) - a\\n\\n(9)\\n\\nsuch that for every mj(a) there exists a corresponding (mapped)\\npoint mj' which satisfies\\n\\nthe original equation for the mean field zero theory.\", 'It can be\\nshown 2, 4 that for every fixed point of mj( a = 0), there exists a\\ncorresponding fixed point mj( a) with the same selectivity and\\nstability properties.', 'The fixed points are available to the\\nneurons if there is sufficient inhibition in the network (ILo I is\\nsufficiently large).', 'APPLICATION OF THE MEAN FIELD NETWORK TO\\nLAYER IV OF VISUAL CORTEX\\nNeurons in the primary visual cortex of normal adult cats are\\nsharply tuned for the orientation of an elongated slit of light and\\nmost are activated by stimulation of either eye.', 'Both of these\\nproperties--orientation selectivity and binocularity--depend on\\nthe type of visual environment experienced during a critical\\n\\n\\x0c688\\n\\nperiod of early postnatal development.', 'For example, deprivation\\nof patterned input during this critical period leads to loss of\\norientation selectivity while monocular deprivation (MD) results\\nin a dramatic shift in the ocular dominance of cortical neurons\\nsuch that most will be responsive exclusively to the open eye.', 'The ocular dominance shift after MD is the best known and most\\nintensively studied type of visual cortical plasticity.', 'The behavior of visual cortical cells in various rearing\\nconditions suggests that some cells respond more rapidly to\\nenvironmental changes than others.', 'In monocular deprivation,\\nfor example, some cells remain responsive to the closed eye in\\nspite of the very large shift of most cells to the open eye- Singer\\net.', 'al.', '5 found, using intracellular recording, that geniculo-cortical\\nsynapses on inhibitory interneurons are more resistant to\\nmonocular deprivation than are synapses on pyramidal cell\\ndendrites.', 'Recent work suggests that the density of inhibitory\\nGABAergic synapses in kitten striate cortex is also unaffected by\\nMD during the cortical period 6, 7.', 'These results suggest that some LGN -cortical synapses modify\\nrapidly, while others modify relatively slowly, with slow\\nmodification of some cortico-cortical synapses.', 'Excitatory LGNcortical synapses into excitatory cells may be those that modify\\nprimarily.', 'To embody these facts we introduce two types of\\nLGN -cortical synapses:\\nthose (mj) that modify and those (Zk)\\nthat remain relatively constant.', 'In a simple limit we have\\n\\nand\\n\\n(10)\\n\\nWe assume for simplicity and consistent with the above\\nphysiological interpretation that these two types of synapses are\\nconfined to two different classes of cells and that both left and\\nright eye have similar synapses (both m i or both Zk) on a given\\ncell.', 'Then, for binocular cells, in the mean field approximation\\n(where binocular terms are in italics)\\n\\n\\x0c689\\n\\nwhere dl(r) are the explicit left (right) eye time averaged signals\\narriving form the LGN.', 'Note that a1(r) contain terms from\\nmodifiable and non-modifiable synapses:\\nal(r) =\\n\\na (ml(r) + zl(r?).', 'Under conditions of monocular deprivation, the animal is reared\\nwith one eye closed.', 'For the sake of analysis assume that the\\nright eye is closed and that only noise-like signals arrive at\\ncortex from the right eye.', 'Then the environment of the cortical\\ncells is:\\nd = (di, n)\\n\\n(12)\\n\\nFurther, assume that the left eye synapses have reached their\\n1\\n\\nr\\n\\nselective fixed point, selective to pattern d 1 ?', \"Then (mi' m i )\\n(m:*, xi) with IXil ?lm!*1.\", 'linear analysis of the\\nthe closed eye\\n\\n<I> -\\n\\n=\\n\\nFollowing the methods of BCM, a local\\nfunction is employed to show that for\\n\\nXi =\\n\\na (1 - }..a)-li.r.', '(13)\\n\\nwhere A.', '= NmIN is the ratio of the number modifiable cells to the\\ntotal number of cells in the network.', 'That is, the asymptotic\\nstate of the closed eye synapses is a scaled function of the meanfield due to non-modifiable (inhibitory) cortical cells.', 'The scale\\nof this state is set not only by the proportion of non-modifiable\\ncells, but in addition, by the averaged intracortical synaptic\\nstrength Lo.', 'Thus contrasted with the mean field zero theory the deprived\\neye LGN-cortical synapses do not go to zero.', 'Rather they\\napproach the constant value dependent on the average inhibition\\nproduced by the non-modifiable cells in such a way that the\\nasymptotic output of the cortical cell is zero (it cannot be driven\\nby the deprived eye).', 'However lessening the effect of inhibitory\\nsynapses (e.g.', 'by application of an inhibitory blocking agent such\\nas bicuculine) reduces the magnitude of a so that one could once\\nmore obtain a response from the deprived eye.', '690\\n\\nWe find, consistent with previous theory and experiment,\\nthat most learning can occur in the LGN-cortical synapse, for\\ninhibitory (cortico-cortical) synapses need not modify.', 'Some\\nnon-modifiable LGN-cortical synapses are required.', 'THE MEAN FIELD APPROXIMATION AND\\nARTIFICIAL NEURAL NETWORKS\\nThe mean field approximation may be applied to networks in\\nwhich the cortico-cortical feedback is a general function of cell\\nactivity.', 'In particular, the feedback may measure the difference\\nbetween the network activity and memories of network activity.', 'In this way, a network may be used as a content addressable\\nmemory.', 'We have been discussing the properties of a mean\\nfield network after equilibrium has been reached.', 'We now focus\\non the detailed time dependence of the relaxation of the cell\\nactivity to a state of equilibrium.', 'Hopfield8 introduced a simple formalism for the analysis of\\nthe time dependence of network activity.', \"In this model,\\nnetwork activity is mapped onto a physical system in which the\\nstate of neuron activity is considered as a 'particle' on a potential\\nenergy surface.\", \"Identification of the pattern occurs when the\\nactivity 'relaxes' to a nearby minima of the energy.\", 'Thus\\nmlmma are employed as the sites of memories.', 'For a Hopfield\\nnetwork of N neurons, the intra-layer connectivity required is of\\norder N2.', 'This connectivity is a significant constraint on the\\npractical implementation of such systems for large scale\\nproblems.', 'Further, the Hopfield model allows a storage capacity\\nwhich is limited to m < N memories 8, 9.', \"This is a result of the\\nproliferation of unwanted local minima in the 'energy' surface.\", 'Recently, Bachmann et al.', \"l 0, have proposed a model for the\\nrelaxation of network activity in which memories of activity\\npatterns are the sites of negative 'charges', and the activity\\ncaused by a test pattern is a positive test 'charge'.\", \"Then in this\\nmodel, the energy function is the electrostatic energy of the\\n(unit) test charge with the collection of charges at the memory\\nsites\\n\\nE = -IlL ~ Qj I J-l- Xj I - L,\\nJ\\n\\n(14)\\n\\n\\x0c691\\n\\nwhere Jl (0) is a vector describing the initial network activity\\ncaused by a test pattern, and Xj' the site of the jth memory.\", 'L is\\na parameter related to the network size.', \"This model has the advantage that storage density is not\\nrestricted by the the network size as it is in the Hopfield model,\\nand in addition, the architecture employs a connectivity of order\\nm x N.\\nNote that at each stage in the settling of Jl (t) to a memory\\n(of network activity) Xj' the only feedback from the network to\\neach cell is the scalar\\n~\\n\\nJ\\n\\nQ. I Jl- X?\", 'I - L\\nJ\\n\\nJ\\n\\n(15)\\n\\nThis quantity is an integrated measure of the distance of the\\ncurrent network state from stored memories.', 'Importantly, this\\nmeasure is the same for all cells; it is as if a single virtual cell\\nwas computing the distance in activity space between the\\ncurrent state and stored states.', 'The result of the computation is\\nThis is a\\nthen broadcast to all of the cells in the network.', 'generalization of the idea that the detailed activity of each cell in\\nthe network need not be fed back to each cell.', \"Rather some\\nglobal measure, performed by a single 'effective' cell is all that is\\nsufficient in the feedback.\", 'DISCUSSION\\n\\nWe have been discussing a formalism for the analysis of\\nnetworks of ideal neurons based on a mean field approximation\\nof the detailed activity of the cells in the network.', 'We find that\\na simple assumption concerning the spatial distribution of the\\npattern preferences of the cells allows a great simplification of\\nthe analysis.', \"In particular, the detailed activity of the cells of\\nthe network may be replaced with a mean field that in effect is\\ncomputed by a single 'effective' cell.\", 'Further, the application of this formalism to the cortical layer\\nIV of visual cortex allows the prediction that much of learning in\\ncortex may be localized to the LGN-cortical synaptic states, and\\nthat cortico-cortical plasticity is relatively unimportant.', 'We find,\\nin agreement with experiment, that monocular deprivation of\\nthe cortical cells will drive closed-eye responses to zero, but\\nchemical blockage of the cortical inhibitory pathways would\\nreveal non-zero closed-eye synaptic states.', '692\\n\\nFinally, the mean field approximation allows the development\\nof single layer models of memory storage that are unrestricted\\nin storage density, but require a connectivity of order mxN.', 'This\\nis significant for the fabrication of practical content addressable\\nmemories.', 'ACKNOWLEOOEMENTS\\nI would like to thank Leon Cooper for many helpful discussions\\nand the contributions he made to this work.', '*This work was supported by the Office of Naval Research and\\nthe Army Research Office under contracts #NOOOI4-86-K-0041\\nand #DAAG-29-84-K-0202.', 'REFERENCES\\n[1] Bienenstock, E. L., Cooper, L. N & Munro, P. W. (1982) 1.', 'Neuroscience 2, 32-48.', '[2] Scofield, C. L. (I984) Unpublished Dissertation.', '[3] Cooper, L. N, Munro, P. W. & Scofield, C. L. (1985) in Synaptic\\nModification, Neuron Selectivity and Nervous System\\nOrganization, ed.', 'C. Levy, J.', 'A. Anderson & S. Lehmkuhle,\\n(Erlbaum Assoc., N.', 'J.).', '[4] Cooper, L. N & Scofield, C. L. (to be published) Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA ..\\n[5] Singer, W. (1977) Brain Res.', '134, 508-000.', '[6] Bear, M. F., Schmechel D. M., & Ebner, F. F. (1985) 1.', 'Neurosci.', '5, 1262-0000.', '[7] Mower, G. D., White, W. F., & Rustad, R. (1986) Brain Res.', '380,\\n253-000.', '[8] Hopfield, J. J.', '(1982) Proc.', 'Natl.', 'A cad.', 'Sci.', 'USA 79, 2554-2558.', '[9] Hopfield, J. J., Feinstein, D. 1., & Palmer, R. O.', '(1983) Nature\\n304, 158-159.', '[10] Bachmann, C. M., Cooper, L. N, Dembo, A.', '& Zeitouni, O.', '(to be\\npublished) Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA.', '\\nMICROELECTRONIC IMPLEMENTATIONS OF CONNECTIONIST\\nNEURAL NETWORKS\\nStuart Mackie, Hans P. Graf, Daniel B. Schwartz, and John S. Denker\\nAT&T Bell Labs, Holmdel, NJ 07733\\n\\nAbstract\\nIn this paper we discuss why special purpose chips are needed for useful\\nimplementations of connectionist neural networks in such applications as pattern\\nrecognition and classification.', 'Three chip designs are described: a hybrid\\ndigital/analog programmable connection matrix, an analog connection matrix with\\nadjustable connection strengths, and a digital pipe lined best-match chip.', 'The common\\nfeature of the designs is the distribution of arithmetic processing power amongst the\\ndata storage to minimize data movement.', 'RAMs\\n?????/.....', \"Distributed\\n/ '.\", \"'.\", 'co mputati on\\nchips\\n\\n... 0\\n\\n/ ......\\n\\nQ)Q)\\n\\n,c\"C\\n\\nE~\\n\\n\\'\\'iiit:::::::;:::::,\\n\\n::::S,....\\n\\nZO\\n\\n???', '..', 'Conventional\\nCPUs\\n??', '??..', '1\\n1\\n\\n10 3\\n10 6\\n10 9\\nNode Complexity\\n(No.', 'of Transistors)\\n\\nFigure 1.', 'A schematic graph of addressable node complexity and size for conventional\\ncomputer chips.', 'Memories can contain millions of very simple nodes each\\nwith a very few transistors but with no processing power.', 'CPU chips are\\nessentially one very complex node.', 'Neural network chips are in the\\ndistributed computation region where chips contain many simple fixed\\ninstruction processors local to data storage.', '(After Reece and Treleaven 1 )\\n?', 'American Institute of Physics 1988\\n\\n\\x0c516\\n\\nIntroduction\\nIt is clear that conventional computers lag far behind organic computers when it\\ncomes to dealing with very large data rates in problems such as computer vision and\\nspeech recognition.', 'Why is this?', 'The reason is that the brain performs a huge number\\nof operations in parallel whereas in a conventional computer there is a very fast\\nprocessor that can perform a variety of instructions very quickly, but operates on only\\ntwo pieces of data at a time.', 'The rest of the many megabytes of RAM is idle during any instruction cycle.', 'The\\nduty cycle of the processor is close to 100%, but that of the stored data is very close to\\nzero.', 'If we wish to make better use of the data, we have to distribute processing\\npower amongst the stored data, in a similar fashion to the brain.', 'Figure 1 illustrates\\nwhere distributed computation chips lie in comparison to conventional computer chips\\nas regard number and complexity of addressable nodes per chip.', 'In order for a distributed strategy to work, each processing element must be small\\nin order to accommodate many on a chip, and communication must be local and hardwired.', 'Whereas the processing element in a conventional computer may be able to\\nexecute many hundred different operations, in our scheme the processor is hard-wired\\nto perform just one.', 'This operation should be tailored to some particular application.', 'In neural network and pattern recognition algorithms, the dot products of an input\\nvector with a series of stored vectors (referred to as features or memories) is often\\nrequired.', 'The general calculation is:\\n\\nSum of Products\\n\\nv .', 'F(i) = L. v.J f..IJ\\nJ\\n\\nwhere V is the input vector and F(i) is one of the stored feature vectors.', 'Two\\nvariations of this are of particular interest.', 'In feature extraction, we wish to find all the\\nfeatures for which the dot product with the input vector is greater than some threshold\\nT, in which case we say that such features are present in the input vector.', 'Feature Extraction\\n\\nv .', 'F(i) =\\n\\nL. v.J f..IJ\\nJ\\n\\nIn pattern classification we wish to find the stored vector that has the largest dot\\nproduct with the input vector, and we say that the the input is a member of the class\\nrepresented by that feature, or simply that that stored vector is closest to input vector.', 'Classification\\n\\nmax(V. F(i) =\\n\\nLV.', 'f..\\n.', 'J IJ\\nJ\\n\\nThe chips described here are each designed to perform one or more of the above\\nfunctions with an input vector and a number of feature vectors in parallel.', 'The overall\\nstrategy may be summed up as follows: we recognize that in typical pattern recognition\\napplications, the feature vectors need to be changed infrequently compared to the input\\n\\n\\x0c517\\n\\nvectors, and the calculation that is perfonned is fixed and low-precision, we therefore\\ndistribute simple fixed-instruction processors throughout the data storage area, thus\\nminimizing the data movement and optimizing the use of silicon.', 'Our ideal is to have\\nevery transistor on the chip doing something useful during every instruction cycle.', 'Analog Sum-or-Products\\nU sing an idea slightly reminiscent of synapses and neurons from the brain, in two\\nof the chips we store elements of features as connections from input wires on which the\\nelements of the input vectors appear as voltages to summing wires where a sum-ofproducts is perfonned.', 'The voltage resulting from the current summing is applied to\\nthe input of an amplifier whose output is then read to determine the result of the\\ncalculation.', 'A schematic arrangement is shown in Figure 2 with the vertical inputs\\nconnected to the horizontal summing wires through resistors chosen such that the\\nconductance is proportional to the magnitude of the feature element.', 'When both\\npositive and negative values are required, inverted input lines are also necessary.', 'Resistor matrices have been fabricated using amorphous silicon connections and metal\\nlinewidths.', 'These were programmed during fabrication by electron beam lithography\\nto store names using the distributed feedback method described by Hopfield2 ,3.', 'This\\nwork is described more fully elsewhere.', '4 ,5 Hard-wired resistor matrices are very\\ncompact, but also very inflexible.', 'In many applications it is desirable to be able to\\nreprogram the matrix without having to fabricate a new chip.', 'For this reason, a series\\nof programmable chips has been designed.', 'Input lines\\nFeature 4\\n\\n-t-----tI----4t---t---f--.---1\\n\\nFeature 3 -+--4II--I--\\'--+---+--4~\\n\\noc:\\n\\n--\\n\\n\"\\'C\\n\\nc:\\n\\nFeature 2 ~--+-\"\"\\'-+--4---1~--I\\n\\n( I)\\n\\nFeature 1\\n\\nFigure 2.', 'A schematic arrangement for calculating parallel sum-of-products with a\\nresistor matrix.', 'Features are stored as connections along summing wires and\\nthe input elements are applied as voltages on the input wires.', 'The voltage\\ngenerated by the current summing is thresholded by the amplifer whose\\noutput is read out at the end of the calculation.', 'Feedback connections may be\\n\\n\\x0c518\\n\\nmade to give mutual inhibition and allow only one feature amplifier to tum\\non, or allow the matrix to be used as a distributed feedback memory.', 'Programmable Connection Matrix\\nFigure 3 is a schematic diagram of a programmable connection using the contents of\\ntwo RAM cells to control current sinking or sourcing into the summing wire.', \"The\\nswitches are pass transistors and the 'resistors' are transistors with gates connected to\\ntheir drains.\", \"Current is sourced or sunk if the appropriate RAM cell contains a '1' and\\nthe input Vi is high thus closing both switches in the path.\", 'Feature elements can\\ntherefore take on values (a,O,-b) where the values of a and b are determined by the\\nconductivities of the n- and p-transistors obtained during processing.', 'A matrix with\\n2916 such connections allowing full interconnection of the inputs and outputs of 54\\namplifiers was designed and fabricated in 2.5Jlm CMOS (Figure 4).', 'Each connection\\nis about 100x100Jlm, the chip is 7x7mm and contains about 75,000 transistors.', 'When\\nloaded with 49 49-bit features (7x7 kernel), and presented with a 49-bit input vector,\\nthe chip performs 49 dot products in parallel in under 1Jls.', 'This is equivalent to 2.4\\nbillion bit operations/sec.', 'The flexibility of the design allows the chip to be operated in\\nseveral modes.', 'The chip was programmed as a distributed feedback memory\\n(associative memory), but this did not work well because the current sinking capability\\nof the n-type transistors was 6 times that of the p-types.', \"An associative memory was\\nimplemented by using a 'grandmother cell' representation, where the memories were\\nstored along the input lines of amplifiers, as for feature extraction, but mutually\\ninhibitory connections were also made that allowed only one output to tum on.\", 'With\\n10 stored vectors each 40 bits long, the best match was found in 50-600ns, depending\\non the data.', 'The circuit can also be programmed to recognize sequences of vectors and\\nto do error correction when vectors were omitted or wrong vectors were inserted into\\nthe sequences.', 'The details of operation of the chip are described more fully\\nelsewhere 6 .', 'This chip has been interfaced to a UNIX minicomputer and is in everyday\\nuse as an accelerator for feature extraction in optical character recognition of handwritten numerals.', 'The chip speeds up this time consuming calculation by a factor of\\nmore than 1000.', 'The use of the chip enables experiments to be done which would be\\ntoo time consuming to simulate.', 'Experience with this device has led to the design of four new chips, which are\\ncurrently being tested.', 'These have no feedback capability and are intended exclusively\\nfor feature extraction.', 'The designs each incorporate new features which are being\\ntested separately, but all are based on a connection matrix which stores 46 vectors each\\n96 bits long.', 'The chip will perform a full parallel calculation in lOOns.', '519\\n\\nVDD\\n\\n~,\\nOutput(!)', '<:]\\n\\nVj\\n\\n~\\n\\nExcitatory\\n\\nInhibitory\\nV?', 'J\\n\\nIvss\\nFigure 3.', 'Schematic diagram of a programmable connection.', \"A current sourcing or\\nsinking connection is made if a RAM cell contains a '1' and the input Vi is\\nhigh.\", 'The currents are summed on the input wire of the amplifier.', '?1 Pads ?', 'Row Decoders\\nr:--3 Connections\\nITII1 Amplifie rs\\n\\nFigure 4.', 'Programmable connection matrix chip.', 'The chip contains 75,000 transistors\\nin 7x7mm, and was fabricated using 2.5Jlm design rules.', '520\\n\\nAdaptive Connection Matrix\\nMany problems require analog depth in the connection strengths, and this is\\nespecially important if the chip is to be used for learning, where small adjustments are\\nrequired during training.', 'Typical approaches which use transistors sized in powers of\\ntwo to give conductance variability take up an area equivalent to the same number of\\nminimum sized transistors as the dynamic range, which is expensive in area and\\nenables only a few connections to be put on a chip.', 'We have designed a fully analog\\nconnection based on a DRAM structure that can be fabricated using conventional\\nCMOS technology.', 'A schematic of a connection and a connection matrix is shown in\\nFigure 5.', 'The connection strength is represented by the difference in voltages stored\\non two MOS capacitors.', 'The capacitors are 33Jlm on edge and lose about 1% of their\\ncharge in five minutes at room temperature.', 'The leakage rate can be reduced by three\\norders of magnitude by cooling the the capacitors to -50?C and by five orders of\\nmagnitude by cooling to -100?C.', 'The output is a current proportional to the product of\\nthe input voltage and the connection strength.', 'The output currents are summed on a\\nwire and are sent off chip to external amplifiers.', 'The connection strengths can be\\nadjusted using transferring charge between the capacitors through a chain of transistors.', 'The connections strengths may be of either polarity and it is expected that the\\nconnections will have about 7 bits of analog depth.', 'A chip has been designed in\\n1.25Jlm CMOS containing 1104 connections in an array with 46 inputs and 24 outputs.', \"Input\\n\\nWeight update and decay\\nby shifting charge\\n\\n.1\\n\\n..L\\n\\n1..\\n\\n'-1'--,\\n\\n02\\n\\n:or:\\n\\n.4111.\", '..\\n\\n\\'\"\\n\\n,\\n\\n...\\n\\n1\"\\n\\n\\'r\"\\n\\n...-\\n\\n........\\n....\\n...\\n........\\n...\\n........\\n......\\n......\\n\\n...', 'Input\\n\\nw (l (01-02)\\nOutput=w*lnput\\n\\n!', '....\\n...-\\n\\nOutput through external amplifiers\\n\\nFigure 5.', 'Analog connection.', 'The connection strength is represented by the difference\\nin voltages stored on two capacitors.', 'The output is a current proprtional to\\nthe product of the input voltage and the connection strength.', 'Each connection is 70x240Jlm.', 'The design has been sent to foundry, and testing is\\nexpected to start in April 1988.', 'The chip has been designed to perform a network\\ncalculation in <30ns, i.e., the chip will perform at a rate of 33 billion multiplies/sec.', 'It\\ncan be used simply as a fast analog convolver for feature extraction, or as a learning\\n\\n\\x0c521\\n\\nengine in a gradient descent algorithm using external logic for connection strength\\nadjustment.', 'Because the inputs and outputs are true analog, larger networks may be\\nformed by tiling chips, and layered networks may be made by cascading through\\namplifiers acting as hidden units.', 'Digital Classifier Chip\\nThe third design is a digital implementation of a classifier whose architecture is not\\na connectionist matrix.', 'It is nearing completion of the design stage, and will be\\nfabricated using 1.25Jlm CMOS.', 'It calculates the largest five V?P(i) using an alldigital pipeline of identical processors, each attached to one stored word.', 'Each\\nprocessor is also internally pipelined to the extent that no stage contains more than two\\ngate delays.', 'This is important, since the throughput of the processor is limited by the\\nspeed of the slowest stage.', 'Each processor calculates the Hamming distance (number\\nof difference bits) between an input word and its stored word, and then compares that\\ndistance with each of the smallest 5 values previously found for that input word.', 'An\\nupdated list of 5 best matches is then passed to the next processor in the pipeline.', 'At\\nthe end of the pipeline the best 5 matches overall are output.', '(1 ) Features stored in\\n\\nData\\npipeline\\n\\nring shift register\\n\\n:{ it\\n\\n~\\n\\n~\\n\\nBest match list\\npipeline\\nTag register\\n\\n+\\n\\n~ ~\\n\\nit {{ : : : Ii :::::;::::::.', 'f:: Jr }/ :{ it :r : : :: :mIfl\\n::t:t if::::: t::}; {}} [,\\n:i::\\n\\nII:: ::::::::I; : I::::::I::;:: : : {::::::I:\\n; ;: : :I[HI tm.', ': ~L!.~\\\\.._-...-t!~[1\\n\\n/ Pf --\\n\\n(2) Input and feature\\n(3) Accumulator\\nare compared\\ndumps\\ndistance\\nbit-serially\\ninto comparison\\nregister at end\\nof input word\\n\\nPig.', '6\\n\\n,\\n\\n(4) Comparator inserts\\nnew match and tag into\\nlist when better than\\nold match\\n\\nSchematic of one of the 50 processors in the digital classifier chip.', 'The\\nHamming distance of the input vector to the feature vector is calculated, and\\nif better than one of the five best matches found so far, is inserted into the\\nmatch list together with the tag and passed onto the next processor.', 'At the\\nend of the pipeline the best five matches overall are output\\n\\n\\x0c522\\n\\nThe data paths on chip are one bit wide and all calculations are bit serial.', 'This\\nmeans that the processing elements and the data paths are compact and maximizes the\\nnumber of stored words per chip.', 'The layout of a single processor is shown in\\nFig.', '6.', 'The features are stored as 128-bit words in 8 16-bit ring shift registers and\\nassociated with each feature is a 14-bit tag or name string that is stored in a static\\nregister.', 'The input vector passes through the chip and is compared bit-by-bit to each\\nstored vector, whose shift registers are cycled in tum.', 'The total number of bits\\ndifference is summed in an accumulator.', 'After a vector has passed through a processor,\\nthe total Hamming distance is loaded into the comparison register together with the tag.', 'At this time, the match list for the input vector arrives at the comparator.', 'It is an\\nordered list of the 5 lowest Hamming distances found in the pipeline so far, together\\nwith associated tag strings.', 'The distance just calculated is compared bit-serially with\\neach of the values in the list in turn.', 'If the current distance is smaller than one of the\\nones in the list, the output streams of the comparator are switched, having the effect of\\ninserting the current match and tag into the list and deleting the previous fifth best\\nmatch.', 'After the last processor in the pipeline, the list stream contains the best five\\ndistances overall, together with the tags of the stored vectors that generated them.', 'The\\ndata stream and the list stream are loaded into 16-bit wide registers ready for output.', 'The design enables chips to be connected together to extend the pipeline if more than 50\\nstored vectors are required.', 'The throughput is constant, irrespective of the number of\\nchips connected together; only the latency increases as the number of chips increases.', 'The chip has been designed to operate with an on-chip clock frequency of at least\\nl00MHz.', 'This high speed is possible because stage sizes are very small and data paths\\nhave been kept short.', 'The computational efficiency is not as high as in the analog chips\\nbecause each processor only deals with one bit of stored data at a time.', 'However, the\\noverall throughput is high because of the high clock speed.', 'Assuming a clock\\nfrequency of l00MHz, the chip will produce a list of 5 best distances with tag strings\\nevery 1.3Jls, with a latency of about 2.5Jls.', 'Even if a thousand chips containing\\n50,000 stored vectors were pipelined together, the latency would be 2.5ms, low\\nenough for most real time applications.', 'The chip is expected to perform 5 billion bit\\noperation/sec.', 'While it is important to have high clock frequencies on the chip, it is also important\\nto have them much lower off the chip, since frequencies above 50MHz are hard to deal\\non circuit boards.', 'The 16-bit wide communication paths onto and off the chip ensure\\nthat this is not a problem here.', 'Conclusion\\nThe two approaches discussed here, analog and digital, represent opposites in\\ncomputational approach.', 'In one, a single global computation is performed for each\\nmatch, in the other many local calculations are done.', 'Both the approaches have their\\nadvantages and it remains to be seen which type of circuit will be more efficient in\\napplications, and how closely an electronic implementation of a neural network should\\nresemble the highly interconnected nature of a biolOgical network.', 'These designs represent some of the first distributed computation chips.', 'They are\\ncharacterized by having simple processors distributed amongst data storage.', 'The\\noperation performed by the processor is tailored to the application.', 'It is interesting to\\nnote some of the reasons why these designs can now be made: minimum linewidths on\\n\\n\\x0c523\\n\\ncircuits are now small enough that enough processors can be put on one chip to make\\nthese designs of a useful size, sophisticated design tools are now available that enable a\\nsingle person to design and simulate a complete circuit in a matter of months, and\\nfabrication costs are low enough that highly speculative circuits can be made without\\nrequiring future volume production to offset prototype costs.', 'We expect a flurry of similar designs in the coming years, with circuits becoming\\nmore and more optimized for particular applications.', 'However, it should be noted that\\nthe impressive speed gain achieved by putting an algorithm into custom silicon can only\\nbe done once.', 'Further gains in speed will be closely tied to mainstream technological\\nadvances in such areas as transistor size reduction and wafer-scale integration.', 'It\\nremains to be seen what influence these kinds of custom circuits will have in useful\\ntechnology since at present their functions cannot even be simulated in reasonable time.', 'What can be achieved with these circuits is very limited when compared with a three\\ndimensional, highly complex biological system, but is a vast improvement over\\nconventional computer architectures.', 'The authors gratefully acknowledge the contributions made by L.D.', 'Jackel, and\\nR.E.', 'Howard\\n\\nReferences\\n\\n1\\n\\nM. Reece and P.C.', 'Treleaven, \"Parallel Architectures for Neural Computers\", Neural\\nComputers, R. Eckmiller and C. v.d.', 'Malsburg, eds (Springer-Verlag, Heidelberg,\\n1988)\\n\\n2\\n\\nJ.I.', 'Hopfield, Proc.', 'Nat.', 'Acad.', 'Sci.', '79.2554 (1982).', '3\\n\\nJ.S.', 'Denker, Physica 22D, 216 (1986).', '4\\n\\nR.E.', 'Howard, D.B.', 'Schwartz, J.S.', 'Denker, R.W.', 'Epworth, H.P.', 'Graf, W .E.', 'Hubbard, L.D.', 'Jackel, B.L.', 'Straughn, and D.M.', 'Tennant, IEEE Trans.', 'Electron\\nDevices ED-34, 1553, (1987)\\n\\n5\\n\\nH.P.', 'Oraf and P. deVegvar, \"A CMOS Implementation of a Neural Network\\nModel\", in \"Advanced Research in VLSI\", Proceedings of the 1987 Stanford\\nConference, P. Losleben (ed.', '), (MIT Press 1987).', '6\\n\\nH.P.', 'Oraf and P. deVegvar, \"A CMOS Associative Memory Chip Based on Neural\\nNetworks\", Tech.', 'Digest, 1987 IEEE International Solid-State Circuits Conference.', '\\nLEARNING ON A GENERAL NETWORK\\n\\nAmir F. Atiya\\nDepartment of Electrical Engineering\\nCalifornia Institute of Technology\\nCa 91125\\n\\nAbstract\\nThis paper generalizes the backpropagation method to a general network containing feedback t;onnections.', 'The network model considered consists of interconnected groups of neurons,\\nwhere each group could be fully interconnected (it could have feedback connections, with possibly asymmetric weights), but no loops between the groups are allowed.', 'A stochastic descent\\nalgorithm is applied, under a certain inequality constraint on each intra-group weight matrix\\nwhich ensures for the network to possess a unique equilibrium state for every input.', 'Introduction\\nIt has been shown in the last few years that large networks of interconnected \"neuron\" -like\\nelemp.nts are quite suitable for performing a variety of computational and pattern recognition\\ntasks.', 'One of the well-known neural network models is the backpropagation model [1]-[4].', 'It\\nis an elegant way for teaching a layered feedforward network by a set of given input/output\\nexamples.', 'Neural network models having feedback connections, on the other hand, have also\\nbeen devised (for example the Hopfield network [5]), and are shown to be quite successful in\\nperforming some computational tasks.', 'It is important, though, to have a method for learning\\nby examples for a feedback network, since this is a general way of design, and thus one can\\navoid using an ad hoc design method for each different computational task.', 'The existence\\nof feedback is expected to improve the computational abilities of a given network.', 'This is\\nbecause in feedback networks the state iterates until a stable state is reached.', 'Thus processing\\nis perforrr:.ed on several steps or recursions.', 'This, in general allows more processing abilities\\nthan the \"single step\" feedforward case (note also the fact that a feedforward network is\\na special case of a feedback network).', 'Therefore, in this work we consider the problem of\\ndeveloping a general learning algorithm for feedback networks.', 'In developing a learning algorithm for feedback networks, one has to pay attention to the\\nfollowing (see Fig.', '1 for an example of a configuration of a feedback network).', 'The state of\\nthe network evolves in time until it goes to equilibrium, or possibly other types of behavior\\nsuch as periodic or chaotic motion could occur.', 'However, we are interested in having a steady\\nand and fixed output for every input applied to the network.', 'Therefore, we have the following\\ntwo important requirements for the network.', 'Beginning in any initial condition, the state\\nshould ultimately go to equilibrium.', 'The other requirement is that we have to have a unique\\n\\n?', 'American Institute of Physics 1988\\n\\n\\x0c23\\n\\nequilibrium state.', 'It is in fact that equilibrium state that determines the final output.', 'The\\nobjective of the learning algorithm is to adjust the parameters (weights) of the network in small\\nsteps, so as to move the unique equilibrium state in a way that will result finally in an output\\nas close as possible to the required one (for each given input).', 'The existence of more than op.e\\nequilibrium state for a given input causes the following problems.', 'In some iterations one might\\nbe updating the weights so as to move one of the equilibrium states in a sought direction, while\\nin other iterations (especially with different input examples) a different equilibrium state is\\nmoved.', 'Another important point is that when implementing the network (after the completion\\noflearning), for a fixed input there can be more than one possible output.', 'Independently, other\\nwork appeared recently on training a feedback network [6],[7],[8].', 'Learning algorithms were\\ndeveloped, but solving the problem of ensuring a unique equilibrium was not considered.', 'This\\nproblem is addressed in this paper and an appropriate network and a learning algorithm are\\nproposed.', 'neuron 1\\n\\noutputs\\n\\ninputs\\n\\nFig .', '1\\nA recurrent network\\n\\nThe Feedback Network\\nConsider a group of n neurons which could be fully inter-connected (see Fig.', '1 for an\\nexample).', 'The weight matrix W can be asymmetric (as opposed to the Hopfield network).', 'The inputs are also weighted before entering into the network (let V be the weight matrix).', 'Let x and y be the input and output vectors respectively.', 'In our model y is governed by the\\nfollowing set of differential equations, proposed by Hopfield [5]:\\n\\ndu\\n\\nTdj = Wf(u) - u\\n\\n+ Vx,\\n\\ny = f(u)\\n\\n(1)\\n\\n\\x0c24\\n\\nwhere f(u) = (J(ud, ... , f(un)f, T denotes the transpose operator, f is a bounded and\\ndifferentiable function, and.,.', 'is a positive constant.', 'For a given input, we would like the network after a short transient period to give a steady\\nand fixed output, no matter what the initial network state was.', 'This means that beginning\\nany initial condition, the state is to be attracted towards a unique equilibrium.', \"This leads to\\nlooking for a condition on the matrix W.\\n\\nTheorem: A network (not necessarily symmetric) satisfying\\n\\nL L w'fi\\ni\\n\\n< l/max(J')2,\\n\\ni\\n\\nexhibits no other behavior except going to a unique equilibrium for a given input.\", 'Proof : Let udt) and U2(t) be two solutions of (1).', 'Let\\n\\nwhere \" II is the two-norm.', 'Differentiating J with respect to time, one obtains\\n\\nUsing (1) , the expression becomes\\n\\ndJ(t) = --lluI(t)\\n2\\n-d- u2(t))11 2\\nt\\n\\n1\"\\n\\n2\\n+ -(uI(t)\\n- U2(t)) T W [f( uI(t) ) - f ( uz(t) )] .', '.,.', \"Using Schwarz's Inequality, we obtain\\n\\nAgain, by Schwarz's Inequality,\\n\\ni = 1, ... ,n\\nwhere\\n\\nWi\\n\\ndenotes the\\n\\nith\\n\\nrow of W. Using the mean value theorem, we get\\n\\nIlf(udt)) - f(U2(t))II ~ (maxl!\", \"'I)IIUl(t) - uz(t)ll.\", 'Using (2),(3), and the expression for J(t), we get\\n\\nd~~t) ~\\nwhere\\n\\n-aJ(t)\\n\\n(4)\\n\\n(3)\\n\\n(2)\\n\\n\\x0c25\\n\\nBy hypothesis of the Theorem, a is strictly positive.', 'Multiplying both sides of (4) by exp( at),\\nthe inequality\\n\\nresults, from which we obtain\\n\\nJ(t)\\n\\n~\\n\\nJ(O)e- at .', 'From that and from the fact that J is non-negative, it follows that J(t) goes to zero as t -+ <Xl.', 'Therefore, any two solutions corresponding to any two initial conditions ultimately approach\\neach other.', 'To show that this asymptotic solution is in fact an equilibrium, one simply takes\\nU2(t) = Ul(t + T), where T is a constant, and applies the above argument (that J(t) -+ 0 as\\nt -+ <Xl), and hence Ul(t + T) -+ udt) as t -+ <Xl for any T, and this completes the proof.', \"For example, if the function\\n\\nI\\n\\nis of the following widely used sigmoid-shaped form,\\n1\\n\\nI(u) = l+e- u\\n\\n'\\n\\nthen the sum of the square of the weights should be less than 16.\", 'Note that for any function\\nI, scaling does not have an effect on the overall results.', 'We have to work in our updating\\nscheme subject to the constraint given in the Theorem.', 'In many cases where a large network\\nis necessary, this constraint might be too restrictive.', 'Therefore we propose a general network,\\nwhich is explained in the next Section.', 'The General Network\\nWe propose the following network (for an example refer to Fig.', '2).', 'The neurons are\\npartitioned into several groups.', 'Within each group there are no restrictions on the connections\\nand therefore the group could be fully interconnected (i.e.', 'it could have feedback connections) .', 'The groups are connected to each other, but in a way that there are no loops.', 'The inputs to\\nthe whole network can be connected to the inputs of any of the groups (each input can have\\nseveral connections to several groups).', 'The outputs of the whole network are taken to be the\\noutputs (or part of the outputs) of a certain group, say group I.', 'The constraint given in the\\nTheorem is applied on each intra-group weight matrix separately.', 'Let (qa, s\"), a = 1, .. .', ', N be\\nthe input/output vector pairs of the function to be implemented.', 'We would like to minimize\\nthe sum of the square error, given by\\n\\na=l\\n\\nwhere\\nM\\n\\ne\"\\n\\n=\\n\\nI)y{ -\\n\\nsi}2,\\n\\ni=l\\n\\nand yf is the output vector of group f upon giving input qa, and M is the dimension of vector\\ns\".', 'The learning process is performed by feeding the input examples qU sequentially to the\\nnetwork, each time updating the weights in an attempt to minimize the error.', '26\\n\\ninputs\\n\\nJ---V\\n\\noutputs\\n\\nFig.', '2\\nAn example of a general network\\n(each group represents a recurrent network)\\n\\nNow, consider a single group l. Let Wi be the intra-group weight matrix of group l, vrl\\nbe the matrix of weights between the outputs of group,.', 'and the inputs of group l, and yl be\\nthe output vector of group I.', \"Let the respective elements be w~i' V[~., and y~.\", \"Furthermore,\\nlet\\nbe the number of neurons of group l. Assume that the time constant l' is sufficiently\\nsmall so as to allow the network to settle quickly to the equilibrium state, which is given by\\nthe solution of the equation\\n\\nn,\\n\\nyl = f(W'yl +\\n\\nL vrlyr) .\", '(5)\\n\\nr?A I\\n\\nwhere A, is the set of the indices of the groups whose outputs a.re connected to the inputs of\\ngroup ,.', 'We would like each iteration to update the weight matrices Wi and vrl so as to move\\nthe equilibrium in a direction to decrease the error.', 'We need therefore to know the change in\\nthe error produced by a small change in the weight matrices.', 'Let .', \":;';, , and aa~~, denote the\\nmatrices whose (i, j)th element are :~'.'\", 'and ::~ respectively.', \"Let ~ be the column vector\\n'1\\n'1\\n:r\\nwhose ith element is ~.\", 'We obtain the following relations:\\nuy.', \"8e a =\\n8W'\\n8e a\\n8V tl\\n\\n[A' _ (W')T] -1 8ea (\\n\\n8yl Y\\n\\n')T\\n\\n,\\n\\na ( r)T\\n= [A' _ (W')T] -1 8e\\n8yl y\\n,\\n\\nwhere A' is the diagonal matrix whose ith diagonal element is l/f'(Lk w!kY~ + LrLktJ[kyk)\\nfor a derivation refer to Appendix).\", 'The vector ~ associated with groUp l can be obtained\\nin terms of the vectors ~, fEB\" where B, is the set of the indices of the groups whose inputs\\nare connected to the outputs of group ,.', 'We get (refer to Appendix)\\n\\n8e a\\n8yl\\n\\n= \\'\" (V\\'i)T[Ai _ (Wi{r 1 8e\"..\\n~\\n\\n8y3\\n\\nJlBI\\n\\n(6)\\n\\nThe matrix A\\' ~ (W\\')T for any group l can never be singular, so we will not face any\\nproblem in the updating process.', \"To prove that, let z be a vector satisfying\\n\\n[A' - (W'f]z = o.\", '27\\n\\nWe can write\\nzdmaxlf\\' I ~\\n\\nLW~.Zk\\'\\n\\ni\\n\\n= I, ... , nl\\n\\nk\\n\\nwhere\\n\\nZi is the ,\"th element of z.', \"Using Schwarz's Inequality, we obtain\\n\\ni = I, ... ,nl\\nSquaring both sides and adding the inequalities for i\\n\\n= I, ... , nl, we get\\n\\nL/; ~ max(J')2(Lz~) LL(w~i)2.\\ni\\n\\nk\\n\\nSince the condition\\n\\n(7)\\n\\nk\\n\\nLL(W!k)2 < I/max(J')2),\\nk\\n\\nis enforced, it follows that (7) cannot be satisfied unless z is the zero vector.\", \"Thus, the matrix\\n\\nA' - (W')T cannot be singular.\", 'For each iteration we begin by updating the weights of group f (the group contammg\\nthe final outputs).', 'For that group ~ equals simply 2(y{ - SI, ... , yf.t - SM, 0, ... , O)T).', 'Then\\nwe move backwards to the groups connected to that group and obtain their corresponding\\nvectors using (6), update the weights, and proceed in the same manner until we complete\\nupdating all the groups.', 'Updating the weights is performed using the following stochastic\\ndescent algorithm for each group,\\n\\n!', '!J:\\n\\n8e a\\n\\nt:.', \"V = -a3 8V\\n\\n+ a4 ea R ,\\n\\nwhere R is a noise matrix whose elements are characterized by independent zero-mean unityvariance Gaussian densities, and the a's are parameters.\", 'The purpose of adding noise is to\\nallow escaping local minima if one gets stuck in any of them.', 'Note that the control parameter\\nis taken to be ea.', 'Hence the variance of the added noise tends to decrease the more we\\napproach the ideal zero-error solution.', 'This makes sense because for a large error, i.e.', 'for an\\nunsatisfactory solution, it pays more to add noise to the weight matrices in order to escape\\nlocal minima.', 'On the other hand, if the error is small, then we are possibly near the global\\nminimum or to an acceptable solution, and hence we do not want too much noise in order\\nnot to be thrown out of that basin.', 'Note that once we reach the ideal zero-error solution the\\nadded noise as well as the gradient of ea become zero for all a and hence the increments of the\\nweight matrices become zero.', \"If after a certain iteration W happens to violate the constraint\\nLiiwlj ~ constant < I/max(J')2, then its elements are scaled so as to project it back onto\\nthe surface of the hypershere.\", 'Implementation Example\\nA pattern recognition example is considered.', 'Fig.', '3 shows a set of two-dimensional\\ntraining patterns from three classes.', 'It is required to design a neural network recognizer with\\n\\n\\x0c28\\n\\nthree output neurons.', 'Each of the neurons should be on if a sample of the corresponding class is\\npresented, and off otherwise, i.e.', 'we would like to design a \"winner-take-all\" network.', 'A singlelayer three neuron feedback network is implemented.', 'We obtained 3.3% error.', 'Performing the\\nsame experiment on a feedforward single-layer network with three neurons, we obtained 20%\\nerror.', 'For satisfactory results, a feedforward network should be two-layer.', 'With one neuron\\nin the first layer and three in the second layer, we got 36.7% error.', 'Finally, with two neurons\\nin the first layer and three in the second layer, we got a match with the feedback case, with\\n3.3% error .', 'z\\nz z\\nz\\n\\nz\\n\\nz\\n\\nz\\n\\nz\\nz\\n\\nz\\nz\\n\\nz\\n\\nz\\n\\nz z\\nz\\n\\nzil\\n\\n1\\n33\\n\\n3\\n3\\n3\\n\\n1\\n\\n3\\n3\\n\\n33\\n3\\n3\\n\\n3\\n\\n~\\n\\n3\\n\\n3\\n3\\n\\n3\\n\\n3\\n\\n3\\n\\nFig.', '3\\nA pattern recognition example\\n\\nConclusion\\n\\nA way to extend the backpropagation method to feedback networks has been proposed .', 'A condition on the weight matrix is obtained, to insure having only one fixed point, so as\\nto prevent having more than one possible output for a fixed input.', 'A general structure for\\nnetworks is presented, in which the network consists of a number of feedback groups connected\\nto each other in a feedforward manner.', 'A stochastic descent rule is used to update the weights.', 'The lJ!ethod is applied to a pattern recognition example.', 'With a single-layer feedback network\\nit obtained good results.', 'On the other hand, the feedforward backpropagation method achieved\\ngood resuls only for the case of more than one layer, hence also with a larger number of neurons\\nthan the feedback case.', '29\\nAcknow ledgement\\nThe author would like to gratefully acknowledge Dr .', 'Y. Abu-Mostafa for the useful\\ndiscussions.', 'This work is supported by Air Force Office of Scientific Research under Grant\\nAFOSR-86-0296.', 'Appendix\\nDifferentiating (5), one obtains\\n\\naI\\n\\n- Yj\\na\\nI\\n=\\nw kp\\n\\naI\\n\\nk,p = 1, ... ,n,\\n\\nI\\nYm\\np jk ,\\nf \\'(\\')(,,\",\\nZj L..,Wjm-a\\nI\\n+Y\\'6)\\n\\nw kp\\n\\nm\\n\\nwhere\\nif j = k\\notherwise,\\n\\nand\\n\\nWe can write\\n\\na~\\'\\n\\n=\\n\\n(A\\' _ Wi) -lbkz>\\n\\n(A - 1)\\n\\naw kp\\n\\nwhere b kp is the nt-dimensional vector whose\\n\\nb~l> = {y~\\n?', '0\\n\\nith\\n\\ncomponent is given by\\n\\nifi = k\\notherwise.', 'By the chain rule,\\naea _ \"\"\\' ae a ay;\\n-a\\nI\\n-L..,-aI - a\\nI\\'\\nw kp\\nj\\nYj w kp\\n\\nwhich, upon substituting from (A - 1), can be put in the form y!,gk~\\' where gk is the\\ncolumn of (A\\' - Wt)-l. Finally, we obtain the required expression, which is\\nae\" = [At _ (WI)T]\\n\\naw\\'\\n\\n-1\\n\\nae\" ( ,)T\\n\\nayl\\n\\ny\\n\\n.', \"Regarding a()~~I' it is obtained by differentiating (5) with respect to vr~,.\", \"We get similarly\\n\\nwhere\\n\\nC kl'\\n\\nis the nt-dimensional vector whose ith component is given by\\n\\nif i = k\\notherwise.\", \"kth\\n\\n\\x0c30\\n\\nA derivation very similar to the case of :~l results in the following required expression:\\nBe a =\\nBVrl\\n8\\n\\n8\\n\\n[A' _ (w,)T] -1 Be a ( r)T.\\nBy' y\\n\\n8yJ\\n\\nj\\n\\nNow, finally consider ~.\", 'Let ~, jf.B, be the matrix whose (k,p)th element is ~.', 'The\\nelements\\nof ~\\ncan be obtained by differentiating the equation for the fixed point for group\\n.', 'uy\\nJ, as follows,\\n\\nHence,\\n\\n:~~.', \"=\\n\\n(Ai - Wi) -IV'i.\", '(A - 2)\\n\\nUsing the chain rule, one can write\\nBe a\\n\\nBy\\'\\n\\n?T\\n\\n= ~(ByJ)\\n~ Byl\\n\\nJEEr\\n\\nBe a\\nBy;\\'\\n\\nWe substitute from (A - 2) into the previous equation to complete the derivation by obtaining\\n\\nReferences\\n111 P. Werbos, \"Beyond regression: New tools for prediction and analysis in behavioral sciences\", Harvard University dissertation, 1974.', '[21 D. Parker, \"Learning logic\", MIT Tech Report TR-47, Center for Computational Research\\nin Economics and Management Science, 1985.', '[31 Y.', 'Le Cun, \"A learning scheme for asymmetric threshold network\", Proceedings of Cognitiva, Paris, June 1985.', '[41 D. Rumelhart, G.Hinton, and R. Williams, \"Learning internal representations by error\\npropagation\", in D. Rumelhart, J. McLelland and the PDP research group (Eds.', '), Parallel\\ndistributed processing: Explorations in the microstructure of cognition, Vol.', '1, MIT Press,\\nCambridge, MA, 1986.', '151 J. Hopfield, \"Neurons with graded response have collective computational properties like\\nthose of two-state neurons\", Proc.', 'N atl.', 'Acad.', 'Sci.', 'USA, May 1984.', '[61 L. Ahneida, \" A learning rule for asynchronous perceptrons with feedback in a combinatorial environment\", Proc.', 'of the First Int.', 'Annual Conf.', 'on Neural Networks, San Diego,\\nJune 1987.', '[71 R. Rohwer, and B. Forrest, \"Training time-dependence in neural networks\", Proc.', 'of the\\nFirst Int.', 'Annual Conf.', 'on Neural Networks, San Diego, June 1987.', '[81 F. Pineda, \"Generalization of back-propagation to recurrent neural networks\", Phys.', 'Rev.', 'Lett., vol.', '59, no.', '19, 9 Nov. 1987.', '\\nTHE SIGMOID NONLINEARITY IN PREPYRIFORM CORTEX\\nFrank H. Eeckman\\nUniversity of California, Berkeley, CA 94720\\nABSlRACT\\nWe report a study ?on the relationship between EEG amplitude values and unit\\nspike output in the prepyriform cortex of awake and motivated rats.', 'This relationship\\ntakes the form of a sigmoid curve, that describes normalized pulse-output for\\nnormalized wave input.', 'The curve is fitted using nonlinear regression and is\\ndescribed by its slope and maximum value.', 'Measurements were made for both excitatory and inhibitory neurons in the cortex.', 'These neurons are known to form a monosynaptic negative feedback loop.', 'Both\\nclasses of cells can be described by the same parameters.', 'The sigmoid curve is asymmetric in that the region of maximal slope is displaced\\ntoward the excitatory side.', \"The data are compatible with Freeman's model of\\nprepyriform burst generation.\", 'Other analogies with existing neural nets are being\\ndiscussed, and the implications for signal processing are reviewed.', 'In particular the\\nrelationship of sigmoid slope to efficiency of neural computation is examined.', 'INTRODUCTION\\nThe olfactory cortex of mammals generates repeated nearly sinusoidal bursts of\\nelectrical activity (EEG) in the 30 to 60 Hz.', 'range 1.', 'These bursts ride on top of a\\nslower ( 1 to 2 Hz.', '), high amplitude wave related to respiration.', 'Each burst begins\\nshortly after inspiration and terminates during expiration.', 'They are generated locally\\nin the cortex.', \"Similar bursts occur in the olfactory bulb (OB) and there is a high\\ndegree of correlation between the activity in the two structures!'\", 'The two main cell types in the olfactory cortex are the superficial pyramidal cell\\n(type A), an excitatory neuron receiving direct input from the OB, and the cortical\\ngranule cell (type B), an inhibitory interneuron.', 'These cell groups are\\nmonosynaptically connected in a negative feedback loop2.', 'Superficial pyramidal cells are mutually excitatory3, 4, 5 as well as being\\nexcitatory to the granule cells.', 'The granule cells are inhibitory to the pyramidal cells\\nas well as to each other3, 4, 6.', 'In this paper we focus on the analysis of amplitude dependent properties: How is\\nthe output of a cellmass (pulses) related to the synaptic potentials (ie.', 'waves)?', 'The\\nconcurrent recording of multi-unit spikes and EEG allows us to study these\\nphenomena in the olfactory cortex.', 'The anatomy of the olfactory system has been extensively studied beginning with\\nthe work of S. Ramon y Cajal 7.', 'The regular geometry and the simple three-layered\\narchitecture makes these structures ideally suitable for EEG recording 4, 8.', 'The EEG\\ngenerators in the various olfactory regions have been identified and their synaptic\\nconnectivities have been extensively studied9, 10,5,4, 11,6.', 'The EEG is the scalar sum of synaptic currents in the underlying cortex.', 'It can\\nbe recorded using low impedance ?', '.5 Mohm) cortical or depth electrodes.', 'Multiunit signals are recorded in the appropriate cell layers using high impedance (> .5\\nMohm) electrodes and appropriate high pass filtering.', 'Here we derive a function that relates waves (EEG) to pulses in the olfactory\\ncortex of the rat.', 'This function has a sigmoidal shape.', 'The derivative of this curve\\n?', 'American Institute of Physics 1988\\n\\n\\x0c243\\n\\ngives us the gain curve for wave-to-pulse conversion.', 'This is the forward gain for\\nneurons embedded in the cortical cellmass.', 'The product of the forward gain values of\\nboth sets of neurons (excitatory and inhibitory) gives us the feedback gain values.', 'These ultimately determine the dynamics of the system under study.', 'MATERIALS AND METI-IODS\\nA total of twenty-nine rats were entered in this study.', 'In each rat a linear array of\\n6 100 micron stainless steel electrodes was chronically implanted in the prepyriform\\n(olfactory) cortex.', 'The tips of the electrodes were electrolytically sharpened to\\nproduce a tip impedance on the order of .5 to 1 megaohm.', 'The electrodes were\\nimplanted laterally in the midcortex, using stereotaxic coordinates.', 'Their position was\\nverified electrophysiologically using a stimulating electrode in the olfactory tract.', 'This procedure has been described earlier by Freeman 12.', 'At the end of the recording\\nsession a small iron deposit was made to help in histological verification.', 'Every\\nelectrode position was verified in this manner.', 'Each rat was recorded from over a two week period following implantation.', 'All\\nanimals were awake and attentive.', 'No stimulation (electrical or olfactory) was used.', \"The background environment for recording was the animal's home cage placed in the\\nsame room during all sessions.\", 'For the present study two channels of data were recorded concurrently.', 'Channel\\n1 carried the EEG signal, filtered between 10 and 300 Hz.', 'and digitized at 1 ms\\nintervals.', 'Channel 2 carried standard pulses 5 V, 1.2 ms wide, that were obtained by\\npassing the multi-unit signal (filtered between 300 Hz.', 'and 3kHz.)', 'through a\\nwindow discriminator.', 'These two time-series were stored on disk for off-line processing using a PerkinElmer 3220 computer.', 'All routines were written in FORTRAN.', 'They were tested on\\ndata files containing standard sine-wave and pulse signals.', 'DATA PROCESSING\\nThe procedures for obtaining a two-dimensional conditional pulse probability\\ntable have been described earlier 4.', 'This table gives us the probability of occurrence\\nof a spike conditional on both time and normalized EEG amplitude value.', 'By counting the number of pulses at a fixed time-delay, where the EEG is\\nmaximal in amplitude, and plotting them versus the normalized EEG amplitudes, one\\nobtains a sigmoidal function: The Pulse probability Sigmoid Curve (PSC) 13, 14.', 'This function is normalized by dividing it by the average pulse level in the record.', 'It\\nis smoothed by passing it through a digital 1: 1: 1 filter and fitted by nonlinear\\nregression.', 'The equations are:\\nQ = Qmax ( 1- exp [ - ( ev - 1) I Qmax ]) for v> - uO\\nQ = -1\\nfor v < - uO\\n\\n(1 )\\n\\nwhere uO is the steady state voltage, and Q = (p-PO)/pO.', 'and Qmax =(Pmax-PO)/pO.', 'PO is the background pulse count, Pmax is the maximal pulse count.', 'These equations rely on one parameter only.', 'The derivation and justification for\\nthese equations were discussed in an earlier paper by Freeman 13.', '244\\n\\nRESULTS\\nData were obtained from all animals.', 'They express normalized pulse counts, a\\ndimensionless value as a function of normalized EEG values, expressed as a Z-score\\n(ie.', 'ranging from - 3 sd.', 'to + 3 sd., with mean of 0.0).', 'The true mean for the EEG\\nafter filtering is very close to 0.0 m V and the distribution of amplitude values is very\\nnearly Gaussian.', 'The recording convention was such that high EEG-values (ie.', '> 0.0 to + 3.0 sd.)', 'corresponded to surface-negative waves.', 'These in turn occur with activity at the\\napical dendrites of the cells of interest.', 'Low EEG values (ie.', 'from - 3.0 sd.', 'to < 0.0)\\ncorresponded to surface-positive voltage values, representing inhibition of the cells.', 'The data were smoothed and fitted with equation (1).', 'This yielded a Qrnax value\\nfor every data file.', 'There were on average 5 data files per animal.', 'Of these 5, an\\naverage of 3.7 per animal could be fitted succesfully with our technique.', 'In 25 % of\\nthe traces, each representing a different electrode pair, no correlations between spikes\\nand the EEG were found.', \"Besides Qmax we also calculated Q' the maximum derivative of the PSC,\\nrepresenting the maximal gain.\", 'There were 108 traces in all.', 'In the first 61 cases the Qrnax value described the\\nwave-to-pulse conversion for a class of cells whose maximum firing probability is in\\nphase with the EEG.', 'These cells were labelled type A cells 2.', 'These traces\\ncorrespond to the excitatory pyramidal cells.', 'The mean for Qmax in that group was\\n14.6, with a standard deviation of 1.84.', 'The range was 10.5 to 17.8.', 'In the remaining 47 traces the Qmax described the wave-to-pulse conversion for\\nclass B cells.', 'Class B is a label for those cells whose maximal firing probability lags\\nthe EEG maximum by approximately 1/4 cycle.', 'The mean for Qrnax in that group\\nwas 14.3, with a standard deviation of 2.05.', 'The range in this group was 11.0 to\\n18.8.', 'The overall mean for Qmax was 14.4 with a standard deviation of 1.94.', 'There is\\nno difference in Qmax between both groups as measured by the Student t-test.', 'The\\nnonparametric Wilcoxon rank-sum test also found no difference between the groups\\n( p =0.558 for the t-test; p = 0.729 for the Wilcoxon).', 'Assuming that the two groups have Qmax values that are normally distributed (in\\ngroup A, mean = 14.6, median = 14.6; in group B, mean = 14.3, median = 14.1),\\nand that they have equal variances ( st. deviation group A is 1.84; st. deviation group\\nB is 2.05) but different means, we estimated the power of the t-test to detect that\\ndifference in means.', \"A difference of 3 points between the Qmax's of the respective groups was\\nconsidered to be physiologically significant.\", 'Given these assumptions the power of\\nthe t-test to detect a 3 point difference was greater than .999 at the alpha .05 level for\\na two sided test.', 'We thus feel reasonably confident that there is no difference\\nbetween the Qmax values of both groups.', 'The first derivative of the PSC gives us the gain for wave-to-pulse conversion4.', \"The maximum value for this first derivative was labelled Q'.\", \"The location at which\\nthe maximum Q' occurs was labelled Vmax .\", 'Vmax is expressed in units of standard\\ndeviation of EEG amplitudes.', \"The mean for Q' in group A was 5.7, with a standard deviation of .67, in group B\\nit was 5.6 with standard deviation of .73.\", \"Since Q' depends on Qmax, the same\\nstatistics apply to both: there was no significant difference between the two groups\\nfor slope maxima.\", '245\\n\\nFigure 1.', 'Distribution of Qmax values\\ngroup A\\n14\\nH\\nCII\\n.Q\\n\\n~\\n\\n-\\n\\n\",,\\n\\n12\\n10\\n\\n-\\n\\n8\\n\\nI-\\n\\n6\\n\\nI-\\n\\ngroup B\\n\\n~\\n\\n\"\\' r-\\n\\n>, r-,\\n;\\n\\n.\\n\\n\"', '\\'\\n\\n,\\n,\\'\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n\\nH\\nCII\\n.Q\\n\\n~\\n\\nr\"\\':\\n;\\n;\\n\\n~\\n\\n, ,\\n\\n,~\\n\\n, ,,\\nr,\\n,\\n4 I- ~, , \\': 1\\',\\n;\\n, , \", 1\\';\\'\\n, \"\" v,\\n2 .- , ,\\n, \" v,\\n, , ,\" ; ,\\n\".\\'', 'o\\n1011121314151617181920\\nQmax values\\n,\"\\n\\n1\\',\\'\\n\\n\"\\n\\n;\\n\\n\"\\n\\n;\\n\\n;\\n\\n14\\n\\n-\\n\\n12\\n\\n-\\n\\n10\\n\\n~\\n\\n8\\n\\nI-\\n\\n6\\n\\nI-\\n\\n4\\n\\n~\\n\\n2\\n\\n.-\\n\\no\\n\\n\"\"\\n,~\\n\\n, ,\\n~\"\\n\\n, ,\\n, ,, ,\\n\\n~,\\n\\n,\" , , , ;: ,...\\n,\\n, ,, ,, , ,, \"\\n,\\n\"\\n, , , , ,\\n, , , , , \" \\' ,\\n,\"!l~.', ', , .f71.', ',\\n\\np:\\n\\n;\\n\\n1011121314151617181920\\n\\nQmax values\\n\\nThe mean for Vmax was at 2.15 sd.', '+/- .307.', 'In every case Vmax was on the\\nexcitatory side from 0.00, ie.', 'at a positive value of EEG Z-scores.', 'All values were\\ngreater than 1.00.', 'A similar phenomenon has been reported in the olfactory bulb 4,\\n14, 15.', 'Figure 2.', 'Examples of sigmoid fits.', 'A cell\\n\\nB cell\\n\\n14\\n\\n14\\n\\n12\\n\\n12\\n\\n10\\n\\n10\\n\\n8\\n\\n8\\n\\n6\\n\\n6\\n\\nCII\\n\\n4\\n\\n4\\n\\n~\\n\\n2\\n\\n2\\n\\n0\\n\\n0\\n\\n-2\\n\\n-2\\n\\n~\\n~\\n\\n?rot\\n\\n~\\n\\nCII\\nog\\n\\n11\\\\\\n\\nPo\\n\\n-4\\n-3\\n\\n-2\\n\\n-1\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n-4\\n-3\\n\\n-2\\n\\n0\\n\\n-1\\n\\n1\\n\\nnormalized EEG amplitude\\nQm = 14.0\\n\\nQm = 13.4\\n\\n2\\n\\n3\\n\\n\\x0c246\\n\\nCOMPARISON WITH DATA FROM TIIE OB\\nPreviously we derived Qrnax values for the mitral cell population in the olfactory\\nbulb14.', 'The mitral cells are the output neurons of the bulb and their axons form the\\nlateral olfactory tract (LOT).', 'The LOT is the main input to the pyramidal cells (type\\nA) in the cortex.', 'For awake and motivated rats (N = 10) the mean Qmax value was 6.34 and the\\nstandard deviation was 1.46.', 'The range was 4.41- 9.53.', 'For anesthetized animals\\n(N= 8) the mean was 2.36 and the standard deviation was 0.89.', 'The range was 1.153.62.', 'There was a significant difference between anesthetized and awake animals.', 'Furthermore there is a significant difference between the Qmax value for cortical cells\\nand the Qmaxvalue for bulbar cells (non - overlapping distributions).', 'DISCUSSION\\nAn important characteristic of a feedback loop is its feedback gain.', 'There is ample\\nevidence for the existence of feedback at all levels in the nervous system.', 'Moreover\\nspecific feedback loops between populations of neurons have been described and\\nanalyzed in the olfactory bulb and the prepyriform cortex 3, 9, 4.', 'A monosynaptic negative feedback loop has been shown to exist in the PPC,\\nbetween the pyramidal cells and inhibitory cells, called granule cells 3, 2, 6, 16.', 'Time series analysis of concurrent pulse and EEG recordings agrees with this idea.', 'The pyramidal cells are in the forward limb of the loop: they excite the granule\\ncells.', 'They are also mutually excitatory 2,4,16.', 'The granule cells are in the feedback\\nlimb: they inhibit the pyramidal cells.', 'Evidence for mutual inhibition (granule to\\ngranule) in the PPC also exists 17, 6.', 'The analysis of cell firings versus EEG amplitude at selected time-lags allows one\\nto derive a function (the PSC) that relates synaptic potentials to output in a neural\\nfeedback system.', 'The first derivative of this curve gives an estimate of the forward\\ngain at that stage of the loop.', 'The procedure has been applied to various structures in\\nthe olfactory system 4, 13, 15, 14.', 'The olfactory system lends itself well to this type\\nof analysis due to its geometry, topology and well known anatomy.', 'Examination of the experimental gain curves shows that the maximal gain is\\ndisplaced to the excitatory side.', 'This means that not only will the cells become\\nactivated by excitatory input, but their mutual interaction strength will increase.', 'The\\nresult is an oscillatory burst of high frequency ( 30- 60 Hz.)', 'activity.', 'This is the\\nmechanism behind bursting in the olfactory EEG 4, 13.', 'In comparison with the data from the olfactory bulb one notices that there is a\\nsignificant difference in the slope and the maximum of the PSC.', 'In cortex the values\\nare substantially higher, however the Vmax is similar.', 'C. Gray 15 found a mean\\nvalue of 2.14 +/- 0.41 for V max in the olfactory bulb of the rabbit (N= 6).', 'Our value\\nin the present study is 2.15 +/- .31.', 'The difference is not statistically significant.', 'There are important aspects of nonlinear coupling of the sigmoid type that are of\\ninterest in cortical functioning.', 'A sigmoid interaction between groups of elements\\n(\"neurons\") is a prominent feature in many artificial neural nets.', 'S. Grossberg has\\nextensively studied the many desirable properties of sigmoids in these networks.', 'Sigmoids can be used to contrast-enhance certain features in the stimulus.', 'Together\\nwith a thresholding operation a sigmoid rule can effectively quench noise.', 'Sigmoids\\ncan also provide for a built in gain control mechanism 18, 19.', '247\\n\\nChanging sigmoid slopes have been investigated by J. Hopfield.', 'In his network\\nchanging the slope of the sigmoid interaction between the elements affects the\\nnumber of attractors that the system can go to 20.', 'We have previously remarked\\nupon the similarities between this and the change in sigmoid slope between waking\\nand anesthetized animals 14.', 'Here we present a system with a steep slope (the PPC)\\nin series with a system with a shallow slope (the DB).', 'Present investigations into similarities between the olfactory bulb and Hopfield\\nnetworks have been reported 21, 22.', 'Similarities between the cortex and Hopfieldlike networks have also been proposed 23.', 'Spatial amplitude patterns of EEG that correlate with significant odors exist in the\\nbulb 24.', 'A transmission of \"wave-packets\" from the bulb to the cortex is known to\\noccur 25.', 'It has been shown through cofrequency and phase analysis that the bulb\\ncan drive the cortex 25, 26.', 'It thus seeems likely that spatial patterns may also exist\\nin the cortex.', 'A steeper sigmoid, if the analogy with neural networks is correct,\\nwould allow the cortex to further classify input patterns coming from the olfactory\\nbulb.', 'In this view the bulb could form an initial classifier as well as a scratch-pad\\nmemory for olfactory events.', 'The cortex could then be the second classifier, as well\\nas the more permanent memory.', 'These are at present speculations that may turn out to be premature.', 'They\\nnevertheless are important in guiding experiments as well as in modelling.', 'Theoretical studies will have to inform us of the likelihood of this kind of processing.', 'REFERENCES\\n1 S.L.', 'Bressler and W.J.', 'Freeman, Electroencephalogr.', 'Clin.', 'Neurophysiol.', '~: 19\\n(1980).', '.', '2 W.J.', 'Freeman, J. Neurophysiol.', 'll: 1 (1968).', '3 W.J.', 'Freeman, Exptl.', 'Neurol.', '.lO.', ': 525 (1964).', '4 W.J.', 'Freeman, Mass Action in the Nervous System.', '(Academic Press, N.Y.,\\n1975), Chapter 3.', '5 L.B.', 'Haberly and G.M.', 'Shepherd, Neurophys.~: 789 (1973).', '6 L.B.', 'Haberly and J.M.', 'Bower, J. Neurophysiol.', 'll: 90 (1984).', \"7 S. Ramon y Cajal, Histologie du Systeme Nerveux de l'Homme et des Vertebres.\", '( Ed.', 'Maloine, Paris, 1911) .', '8 W.J.', 'Freeman, BioI.', 'Cybernetics .', '.3..5.: 21 (1979).', '9 W. Rall and G.M.', 'Shepherd, J. Neurophysiol.ll: 884 (1968).', '10 G.M.', 'Shepherd, Physiol.', 'Rev.', '5l: 864 (1972).', '11 L.B.', 'Haberly and J.L.', 'Price, J. Compo Neurol.', '.l18.', '; 711 (1978).', '12 W.J.', 'Freeman, Exptl.', 'Neurol.', '~: 70 (1962).', '13 W.J.', 'Freeman, BioI.', 'Cybernetics.ll: 237 (1979).', '14 F.H.', 'Eeckman and W.J.', 'Freeman, AlP Proc.', 'ill: 135 (1986).', '15 C.M.', 'Gray, Ph.D. thesis, Baylor College of Medicine (Houston,1986)\\n16 L.B.', 'Haberly, Chemical Senses, .ll!', ': 219 (1985).', '17 M. Satou et aI., J. Neurophysiol.', '~: 1157 (1982).', '18 S. Grossberg, Studies in Applied Mathematics, Vol LII, 3 (MIT Press, 1973)\\np 213.', '19 S. Grossberg, SIAM-AMS Proc.', 'U: 107 (1981).', '20 J.J Hopfield, Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA 8.1: 3088 (1984).', '21 W.A.', 'Baird, Physica 2.m: 150 (1986).', '22 W.A.', 'Baird, AlP Proceedings ill: 29 (1986).', '23 M. Wilson and J. Bower, Neurosci.', 'Abstr.', '387,10 (1987).', '248\\n\\n24 K.A.', 'Grajski and W.J.', 'Freeman, AlP Proc.lS.l: 188 (1986).', '25 S.L.', 'Bressler, Brain Res.', '~: 285 (1986).', '26 S.L.', 'Bressler, Brain Res.~: 294 (1986).', '\\nTOWARDS AN ORGANIZING PRINCIPLE FOR\\nA LAYERED PERCEPTUAL NETWORK\\nRalph Linsker\\nIBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598\\n\\nAbstract\\nAn information-theoretic optimization principle is proposed for the development\\nof each processing stage of a multilayered perceptual network.', 'This principle of\\n\"maximum information preservation\" states that the signal transformation that is to be\\nrealized at each stage is one that maximizes the information that the output signal values\\n(from that stage) convey about the input signals values (to that stage), subject to certain\\nconstraints and in the presence of processing noise.', 'The quantity being maximized is a\\nShannon information rate.', 'I provide motivation for this principle and -- for some simple\\nmodel cases -- derive some of its consequences, discuss an algorithmic implementation,\\nand show how the principle may lead to biologically relevant neural architectural\\nfeatures such as topographic maps, map distortions, orientation selectivity, and\\nextraction of spatial and temporal signal correlations.', 'A possible connection between\\nthis information-theoretic principle and a principle of minimum entropy production in\\nnonequilibrium thermodynamics is suggested.', 'Introduction\\nThis paper describes some properties of a proposed information-theoretic\\norganizing principle for the development of a layered perceptual network.', 'The purpose\\nof this paper is to provide an intuitive and qualitative understanding of how the principle\\nleads to specific feature-analyzing properties and signal transformations in some simple\\nmodel cases.', 'More detailed analysis is required in order to apply the principle to cases\\ninvolving more realistic patterns of signaling activity as well as specific constraints on\\nnetwork connectivity.', 'This section gives a brief summary of the results that motivated the formulation\\nof the organizing principle, which I call the principle of \"maximum information\\npreservation.\"', 'In later sections the principle is stated and its consequences studied.', 'In previous work l I analyzed the development of a layered network of model cells\\nwith feedforward connections whose strengths change in accordance with a Hebb-type\\nsynaptic modification rule.', 'I found that this development process can produce cells that\\nare selectively responsive to certain input features, and that these feature-analyzing\\nproperties become progressively more sophisticated as one proceeds to deeper cell\\nlayers.', 'These properties include the analysis of contrast and of edge orientation, and\\nare qualitatively similar to properties observed in the first several layers of the\\nmammalian visual pathway.2\\nWhy does this happen?', \"Does a Hebb-type algorithm (which adjusts synaptic\\nstrengths depending upon correlations among signaling activities 3 ) cause a developing\\nperceptual network to optimize some property that is deeply connected with the mature\\nnetwork's functioning as an information processing system?\", '?', 'American Institute ofPhvsics 1988\\n\\n\\x0c486\\n\\nFurther analysis 4 .s has shown that a suitable Hebb-type rule causes a\\nlinear-response cell in a layered feedforward network (without lateral connections) to\\ndevelop so that the statistical variance of its output activity (in response to an ensemble\\nof inputs from the previous layer) is maximized, subject to certain constraints.', 'The\\nmature cell thus performs an operation similar to principal component analysis (PCA),\\nan approach used in statistics to expose regularities (e.g., clustering) present in\\nhigh-dimensional input data.', '(Oja 6 had earlier demonstrated a particular form of\\nHebb-type rule that produces a model cell that implements PCA exactly.)', 'Furthermore, given a linear device that transforms inputs into an output, and given\\nany particular output value, one can use optimal estimation theory to make a \"best\\nestimate\" of the input values that gave rise to that output.', 'Of all such devices, I have\\nfound that an appropriate Hebb-type rule generates that device for which this \"best\\nestimate\" comes closest to matching the input values.', '4 ?s Under certain conditions, such\\na cell has the property that its output preserves the maximum amount of information\\nabout its input values.', 's\\nMaximum Information Preservation\\nThe above results have suggested a possible organizing principle for the\\ndevelopment of each layer of a multilayered perceptual network.', 's The principle can be\\napplied even if the cells of the network respond to their inputs in a nonlinear fashion,\\nand even if lateral as well as feedforward connections are present.', '(Feedback from later\\nto earlier layers, however, is absent from this formulation.)', 'This principle of \"maximum\\ninformation preservation\" states that for a layer of cells L that is connected to and\\nprovides input to another layer M, the connections should develop so that the\\ntransformation of signals from L to M (in the presence of processing noise) has the\\nproperty that the set of output values M conveys the maximum amount of information\\nabout the input values L, subject to various constraints on, e.g., the range of lateral\\nconnections and the processing power of each cell.', 'The statistical properties of the\\nensemble of inputs L are assumed stationary, and the particular L-to-M transformation\\nthat achieves this maximization depends on those statistical properties.', 'The quantity\\nbeing maximized is a Shannon information rate.', '7\\nAn equivalent statement of this principle is: The L-to-M transformation is chosen\\nso as to minimize the amount of information that would be conveyed by the input values\\nL to someone who already knows the output values M.\\nWe shall regard the set of input signal values L (at a given time) as an input\\n\"message\"; the message is processed to give an output message M. Each message is in\\ngeneral a set of real-valued signal activities.', 'Because noise is introduced during the\\nprocessing, a given input message may generate any of a range of different output\\nmessages when processed by the same set of connections.', 'The Shannon information rate (i.e., the average information transmitted from L\\nto M per message) is7\\nR = LL LMP(L,M) log [P(L,M)/P(L)P(M)].', '(1)\\n\\nFor a discrete message space, peL) [resp.', 'P(M)] is the probability of the input (resp.', 'output) message being L (resp.', 'M), and P(L,M) is the joint probability of the input\\nbeing L and the output being M. [For a continuous message space, probabilities are\\n\\n\\x0c487\\n\\nreplaced by probability densities, and sums (over states) by integrals.]', 'This rate can be\\nwritten as\\n(2)\\n\\nwhere\\n\\nh == -\\n\\nLL P(L) log P(L)\\n\\n(3)\\n\\nis the average information conveyed by message Land\\n\\n(4)\\nis the average information conveyed by message L to someone who already knows M.\\nSince II.', 'is fixed by the properties of the input ensemble, maximizing R means\\nminimizing I LIM , as stated above.', 'The information rate R can also be written as\\n(5)\\n\\nwhere 1M and IMI L are defined by interchanging Land M in Eqns.', '3 and 4.', 'This form is\\nheuristically useful, since it suggests that one can attempt to make R large by (if\\npossible) simultaneously making 1M large and IMI L small.', 'The term 1M is largest when\\neach message M occurs with equal probability.', 'The term 1\"\\'1/.', 'is smallest when each L\\nis transformed into a unique M, and more generally is made small by \"sharpening\" the\\nP(M IL) distribution, so that for each L, P(M IL) is near zero except for a small set of\\nmessages M.\\nHow can one gain insight into biologically relevant properties of the L - M\\ntransformation that may follow from the principle of maximum information preservation\\n(which we also call the \"infomax\" principle)?', 'In a network, this L - M transformation\\nmay be a function of the values of one or a few variables (such as a connection strength)\\nfor each of the allowed connections between and within layers, and for each cell.', 'The\\nsearch space is quite large, particularly from the standpoint of gaining an intuitive or\\nqualitative understanding of network behavior.', 'We shall therefore consider a simple\\nmodel in which the dimensionalities of the Land M signal spaces are greatly reduced,\\nyet one for which the infomax analysis exhibits features that may also be important\\nunder more general conditions relevant to biological and synthetic network\\ndevelopment.', 'The next four sections are organized as follows.', '(i) A model is introduced in\\nwhich the Land M messages, and the L-to-M transformation, have simple forms.', 'The\\ninfomax principle is found to be satisfied when some simple geometric conditions (on\\nthe transformation) are met.', '(ii) I relate this model to the analysis of signal processing\\nand noise in an interconnection network.', 'The formation of topographic maps is\\ndiscussed.', '(iii) The model is applied to simplified versions of biologically relevant\\nproblems, such as the emergence of orientation selectivity.', '(iv) I show that the main\\nproperties of the infomax principle for this model can be realized by certain local\\nalgorithms that have been proposed to generate topographic maps using lateral\\ninteractions.', '488\\n\\nA Simple Geometric Model\\nIn this model, each input message L is described by a point in a low-dimensional\\nvector space, and the output message M is one of a number of discrete states.', 'For\\ndefiniteness, we will take the L space to be two-dimensional (the extension to higher\\ndimensionality is straightforward).', 'The L - M transformation consists of two steps.', \"(i) A noise process alters L to a message L' lying within a neighborhood of radius v\\ncentered on L. (ii) The altered message L' is mapped deterministically onto one of the\\noutput messages M.\\nA given L' - M mapping corresponds to a partitioning of the L space into regions\\nlabeled by the output states M. (We do not exclude a priori the possibility that multiple\\ndisjoint regions may be labeled by the same M.) Let A denote the total area of the L\\nstate space.\", 'For each M, let A (M) denote the area of L space that is labeled by M. Let\\nsCM) denote the total border length that the region(s) labeled M share with regions of\\nunlike M -label.', \"A point L lying within distance v of a border can be mapped onto either\\nM-value (because of the noise process L - L').\", 'Call this a \"borderline\" L. A point L\\nthat is more than a distance v from every border can only be mapped onto the M-value\\nof the region containing it.', 'Suppose v is sufficiently small that (for the partitionings of interest) the area\\noccupied by borderline L states is small compared to the total area of the L space.', 'Consider first the case in which peL) is uniform over L. Then the information rate R\\n(using Eqn.', '5) is given approximately (through terms of order v) by\\nR = - ~M[A(M)/A] 10g[A(M)/A] - (yv/A) ~Ms(M).', '(6)\\n\\nTo see this, note that P(M) = A(M)/A and that P(M IL) log P(M IL) is zero except for\\nborderline L (since 0 log 0 = 1 log 1 = 0).', 'Here y is a positive number whose value\\ndepends upon the details of the noise process, which determines P(M I L) for borderline\\nL as a function of distance from the border.', 'For small v (low noise) the first term (1M) on the RHS of Eqn.', '6 dominates.', \"It is\\nmaximized when the A(M) [and hence the P(M)] values are equal for all M. The second\\nterm (with its minus sign), which equals ( -~'4IL)' is maximized when the sum of the\\nborder lengths of all M regions is minimized.\", 'This corresponds to \"sharpening\" the\\nP(M IL) distribution in our earlier, more general, discussion.', 'This suggests that the\\ninfomax solution is obtained by partitioning the L space into M-regions (one for each\\nM value) that are of substantially equal area, with each M-region tending to have\\nnear-minimum border length.', 'Although this simple analysis applies to the low-noise case, it is plausible that even\\nwhen v is comparable to the spatial scale of the M regions, infomax will favor making\\nthe M regions have approximately the same extent in all directions (rather than be\\nelongated), in order to \"sharpen\" p(MI L) and reduce the probability of the noise\\nprocess mapping L onto many different M states.', 'What if peL) is nonuniform?', 'Then the same result (equal areas, minimum border)\\nis obtained except that both the area and border-length elements must now be weighted\\nby the local value of peL).', 'Therefore the infomax principle tends to produce maps in\\nwhich greater representation in the output space is given to regions of the input signal\\nspace that are activated more frequently.', 'To see how lateral interactions within the M layer can affect these results, let us\\nsuppose that the L - M mapping has three, not two, process steps: L - L\\'\\n\\n\\x0c489\\n\\n- M - M, where the first two steps are as above, and the third step changes the output\\nM into any of a number of states M (which by definition comprise the\\n\"M-neighborhood\" of M).', 'We consider the case in which this M-neighborhood relation\\nis symmetric.', 'This type of \"lateral interaction\" between M states causes the infomax principle\\nto favor solutions for which M regions sharing a border in L space are M-neighbors in\\nthe sense defined.', 'For a simple example in which each state M has n M-neighbors\\n(including itself), and each M-neighbor has an equal chance of being the final state\\n(given M), infomax tends to favor each M-neighborhood having similar extent in all\\ndirections (in L space).', 'Relation Between the Geometric Model and Network Properties\\nThe previous section dealt with certain classes of transformations from one\\nmessage space to another, and made no specific reference to the implementation of these\\ntransformations by an interconnected network of processor cells.', 'Here we show how\\nsome of the features discussed in the previous section are related to network properties.', 'For simplicity suppose that we have a two-dimensional layer of uniformly\\ndistributed cells, and that the signal activity of each cell at any given time is either 1\\n(active) or 0 (quiet).', 'We need to specify the ensemble of input patterns.', 'Let us first\\nconsider a simple case in which each pattern consists of a disk of activity of fixed radius,\\nbut arbitrary center position, against a quiet background.', 'In this case the pattern is fully\\ndefined by specifying the coordinates of the disk center.', 'In a two-dimensional L state\\nspace (previous section), each pattern would be represented by a point having those\\ncoordinates.', 'Now suppose that each input pattern consists not of a sharply defined disk of\\nactivity, but of a \"fuzzy\" disk whose boundary (and center position) are not sharply\\ndefined.', '[Such a pattern could be generated by choosing (from a specified distribution)\\na position Xc as the nominal disk center, then setting the activity of the cell at position\\nX to 1 with a probability that decreases with distance I x - Xc I . ]', 'Any such pattern can\\nbe described by giving the coordinates of the \"center of activity\" along with many other\\nvalues describing (for example) various moments of the activity pattern relative to the\\ncenter.', 'For the noise process L - L\\' we suppose that the activity of an L cell can be\\n\"misread\" (by the cells of the M layer) with some probability.', 'This set of distorted\\nactivity values is the \"message\" L\\'.', \"We then suppose that the set of output activities M\\nis a deterministic function of L'.\", 'We have constructed a situation in which (for an appropriate choice of noise level)\\ntwo of the dimensions of the L state space -- namely, those defined by the disk center\\ncoordinates -- have large variance compared to the variance induced by the noise\\nprocess, while the other dimensions have variance comparable to that induced by noise.', 'In other words, the center position of a pattern is changed only a small amount by the\\nnoise process (compared to the typical difference between the center positions of two\\npatterns), whereas the values of the other attributes of an input pattern differ as much\\nfrom their noise-altered values as two typical input patterns differ from each other.', '(Those attributes are \"lost in the noise. \")', 'Since the distance between L states in our geometric model (previous section)\\ncorresponds to the likelihood of one L state being changed into the other by the noise\\n\\n\\x0c490\\n\\nprocess, we can heuristically regard the L state space (for the present example) as a\\n\"slab\" that is elongated in two dimensions and very thin in all other dimensions.', '(In\\ngeneral this space could have a much more complicated topology, and the noise process\\nwhich we here treat as defining a simple metric structure on the L state space need not\\ndo so.', 'These complications are beyond the scope of the present discussion.)', 'This example, while simple.', 'illustrates a feature that is key to understanding the\\noperation of the infomax principle: The character of the ensemble statistics and of the\\nnoise process jointly determine which attributes of the input pattern are statistically\\nmost significant; that is, have largest variance relative to the variance induced by noise.', 'We shall see that the infomax principle selects a number of these most significant\\nattributes to be encoded by the L - M transformation.', 'We turn now to a description of the output state space M. We shall assume that\\nthis space is also of low dimensionality.', 'For example, each M pattern may also be a disk\\nof activity having a center defined within some tolerance.', 'A discrete set of discriminable\\ncenter-coordinate values can then be used as the M-region \"labels\" in our geometric\\nmodel.', 'Restricting the form of the output activity in this particular way restricts us to\\nconsidering positional encodings L - M, rather than encodings that make use of the\\nshape of the output pattern, its detailed activity values, etc.', 'However, this restriction\\non the form of the output does not determine which features of the input patterns are\\nto be encoded, nor whether or not a topographic (neighbor-preserving) mapping is to\\nbe used.', 'These properties will be seen to emerge from the operation of the infomax\\nprinciple.', 'In the previous section we saw that the infomax principle will tend to lead to a\\npartitioning of the L space into M regions having equal areas [if peL) is uniform in the\\ncoordinates of the L disk center] and minimum border length.', 'For the present case this\\nmeans that the M regions will tend to \"tile\" the two long dimensions of the L state space\\n\"slab,\" and that a single M value will represent all points ill L space that differ only in\\ntheir low-variance coordinates.', 'If peL) is nonuniform, then the area of the M region\\nat L will tend to be inversely proportional to peL).', 'Furthermore, if there are local lateral\\nconnections between M cells, then (depending upon the particular form of such\\ninteraction) M states corresponding to nearby localized regions of layer-M activity can\\nbe M-neighbors in the sense of the previous section.', 'In this case the mapping from the\\ntwo high-variance coordinates of L space to M space will tend to be topographic.', 'Examples: Orientation Selectivity and Temporal Feature Maps\\nThe simple example in the previous section illustrates how infomax can lead to\\ntopographic maps, and to map distortions [which provide greater M-space\\nrepresentation for regions of L having large peL)].', 'Let us now consider a case in which\\ninformation about input features is positionally encoded in the output layer as a result\\nof the infomax principle.', 'Consider a model case in which an ensemble of patterns is presented to the input\\nlayer L. Each pattern consists of a rectangular bar of activity (of fixed length and width)\\nagainst a quiet background.', \"The bar's center position and orientation are chosen for\\neach pattern from uniform distributions over some spatial interval for the position, and\\nover all orientation angles (i.e., from 0?\", 'to 180?).', 'The bar need not be sharply defined,\\nbut can be \"fuzzy\" in the sense described above.', 'We assume, however, that all\\n\\n\\x0c491\\n\\nproperties that distinguish different patterns of the ensemble -- except for center\\nposition and orientation -- are \"lost in the noise\" in the sense we discussed.', 'To simplify the representation of the solution, we further assume that only one\\ncoordinate is needed to describe the center position of the bar for the given ensemble.', 'For example, the ensemble could consist of bar patterns all of which have the same y\\ncoordinate of center position, but differ in their x coordinate and in orientation 0.', 'We can then represent each input state by a point in a rectangle (the L state space\\ndefined in a previous section) whose abscissa is the center-position coordinate x and\\nwhose ordinate is the angle 0.', 'The horizontal sides of this rectangle are identified with\\neach other, since orientations of 0 0 and 180 0 are identical.', '(The interior of the\\nrectangle can thus be thought of as the surface of a horizontal cylinder.)', \"The number N x of different x positions that are discriminable is given by the range\\nof x values in the input ensemble divided by the tolerance with which x can be measured\\n(given the noise process L - L'); similarly for No.\", 'The relative lengths Llx and MJ of the\\nsides of the L state space rectangle are given by Llx/ MJ = Nj No.', 'We discuss below the\\ncase in which Nx > > No; if No were> > Nx the roles of x and 0 in the resulting mappings\\nwould be reversed.', 'There is one complicating feature that should be noted, although in the interest\\nof clarity we will not include it in the present analysis.', \"Two horizontal bar patterns that\\nare displaced by a horizontal distance that is small compared with the bar length, are\\nmore likely to be rendered indiscriminable by the noise process than are two vertical bar\\npatterns that are displaced by the same horizontal distance (which may be large\\ncompared with the bar's width).\", 'The Hamming distance, or number of binary activity\\nvalues that need to be altered to change one such pattern into the other, is greater in the\\nlatter case than in the former.', 'Therefore, the distance in L state space between the two\\n\\nUNORIENTED RECEPTIVE FIELDS\\n\\nFigure 1.', 'Orientation Selectivity in a Simple Model: As the input domain size\\n(see text) is reduced [from (a) upper left, to (b) upper right, to (c)\\nlower left figure], infomax favors the emergence of an\\norientation-selective L - M mapping.', \"(d) Lower right figure shows\\na solution obtained by applying Kohonen's relaxation algorithm with\\n50 M-points (shown as dots) to this mapping problem.\", '492\\n\\nstates should be greater in the latter case.', 'This leads to a \"warped\" rather than simple\\nrectangular state space.', 'We ignore this effect here, but it must be taken into account in\\na fuller treatment of the emergence of orientation selectivity.', \"Consider now an L - M transformation that consists of the three-step process\\n(discussed above) (i) noise-induced L - L' ; (ii) deterministic L' - M'; (iii)\\nlateral-interaction-induced M' - M. Step (ii) maps the two-dimensional L state space\\nof points (x, 0) onto a one-dimensional M state space.\", \"For the present discussion, we\\n.consider L' - M' maps satisfying the following Ansatz: Points corresponding to the\\nM states are spaced uniformly, and in topographic order, along a helical line in L state\\nspace (which we recall is represented by the surface of a horizontal cylinder).\", 'The pitch\\nof the helix (or the slope dO/dx) remains to be determined by the infomax principle.', 'Each M-neighborhood of M states (previous section) then corresponds to an interval\\non such a helix.', \"A state L' is mapped onto a state in a particular M-neighborhood if L'\\nis closer (in L space) to the corresponding interval of the helix than to any other portion\\nof the helix.\", 'We call this set of L states (for an M-neighborhood centered on M ) the\\n\"input domain\" of M. It has rectangular shape and lies on the cylindrical surface of the\\nL space.', 'We have seen (previous sections) that infomax tends to produce maps having (i)\\nequal M-region areas, (ii) topographic organization, and (iii) an input domain (for each\\nM-neighborhood) that has similar extent in all directions (in L space).', 'Our choice of\\nAnsatz enforces (i) and (ii) explicitly.', 'Criterion (iii) is satisfied by choosing dO / dx such\\nthat the input domain is square (for a given M-neighborhood size).', 'Figure 1a (having dO/dx = 0) shows a map in which the output M encodes only\\ninformation about bar center position x, and is independent of bar orientation o.', 'The\\nsize of the M -neighborhood is relatively large in this case.', \"The input domain of the state\\nM denoted by the 'x' is shown enclosed by dotted lines.\", '(The particular 0 value at which\\nwe chose to draw the M line in Fig.', '1a is irrelevant.)', 'For this M-neighborhood size, the\\nlength of the border of the input domain is as small as it can be.', 'As the M -neighborhood size is reduced, the dotted lines move closer together.', 'A\\nvertically oblong input domain (which would result if we kept dO/dx = 0 ) would not\\nsatisfy the infomax criterion.', 'The helix for which the input domain is square (for this\\nsmaller choice of M-neighborhood size) is shown in Fig.', 'lb.', 'The M states for this\\nsolution encode information about bar orientation as well as center position.', 'If each M\\nstate corresponds to a localized output activity pattern centered at some position in a\\none-dimensional array of M cells, then this solution corresponds to orientation-selective\\ncells organized in \"orientation columns\" (really \"orientation intervals\" in this\\none-dimensional model).', 'A \"labeling\" of the linear array of cells according to whether\\ntheir orientation preferences lie between 0 and 60, 60 and 120, or 120 and 180 degrees\\nis indicated by the bold, light, and dotted line segments beneath the rectangle in Fig.', '1b\\n(and 1c).', 'As the M-neighborhood size is decreased still further, the mapping shown in Fig.', 'Ie becomes favored over that of either Fig.', '1a or lb.', 'The \"orientation columns\" shown\\nin the lower portion of Fig.', '1c are narrower than in Fig.', '1b.', 'A more detailed analysis of the information rate function for various mappings\\nconfirms the main features we have here obtained by a simple geometric argument.', 'The same type of analysis can be applied to different types of input pattern\\nensembles.', 'To give just one other example, consider a network that receives an\\nensemble of simple patterns of acoustic input.', 'Each such pattern consists of a tone of\\n\\n\\x0c493\\n\\nsome frequency that is sensed by two \"ears\" with some interaural time delay.', 'Suppose\\nthat the initial network layers organize the information from each ear (separately) into\\ntonotopic maps, and that (by means of connections having a range of different time\\ndelays) the signals received by both ears over some time interval appear as patterns of\\ncell activity at some intermediate layer L. We can then apply the infomax principle to\\nthe signal transformation from layer L to the next layer M. The L state space can (as\\nbefore) be represented as a rectangle, whose axes are now frequency and interaural\\ndelay (rather than spatial position and bar orientation).', 'Apart from certain differences\\n(the density of L states may be nonuniform, and states at the top and bottom of the\\nrectangle are no longer identical), the infomax analysis can be carried out as it was for\\nthe simplified case of orientation selectivity.', 'Local Algorithms\\nThe information rate (Eqn.', 'I), which the infomax principle states is to be\\nmaximized subject to constraints (and possibly as part of an optimization function\\ncontaining other cost terms not discussed here), has a very complicated mathematical\\nform.', 'How might this optimization process, or an approximation to it, be implemented\\nby a network of cells and connections each of which has limited computational power?', 'The geometric form in which we have cast the infomax principle for some very simple\\nmodel cases, suggests how this might be accomplished.', 'An algorithm due to Kohonen 8 demonstrates how topographic maps can emerge\\nas a result of lateral interactions within the output layer.', 'I applied this algorithm to a\\none-dimensional M layer and a two-dimensional L layer, using a Euclidean metric and\\nimposing periodic boundary conditions on the short dimension of the L layer.', 'A\\nresulting map is shown in Fig.', 'Id.', 'This map is very similar to those of Figs.', '1band Ic,\\nexcept for one reversal of direction.', 'The reversal is not surprising, since the algorithm\\ninvolves only local moves (of the M-points) while the infomax principle calls for a\\nglobally optimal solution.', \"More generally, Kohonen's algorithm tends empirically8 to produce maps having\\nthe property that if one constructs the Voronoi diagram corresponding to the positions\\nof the M-points (that is, assigns each point L to an M region based on which M-point\\nL is closest to), one obtains a set of M regions that tend to have areas inversely\\nproportional to P(L) , and neighborhoods (corresponding to our input domains) that\\ntend to have similar extent in all directions rather than being elongated.\", 'The Kohonen algorithm makes no reference to noise, to information content, or\\neven to an optimization principle.', 'Nevertheless, it appears to implement, at least in a\\nqualitative way, the geometric conditions that infomax imposes in some simple cases.', 'This suggests that local algorithms along similar lines may be capable of implementing\\nthe infomax principle in more general situations.', 'Our geometric formulation of the infomax principle also suggests a connection\\nwith an algorithm proposed by von der Malsburg and Willshaw9 to generate topographic\\nmaps.', 'In their \"tea trade\" model, neighborhood relationships are postulated within the\\nsource and the target spaces, and the algorithm\\'s operation leads to the establishment\\nof a neighborhood-preserving mapping from source to target space.', \"Such neighborhood\\nrelationships arise naturally in our analysis when the infomax principle is applied to our\\nThe noise process induces a\\nthree-step L - L' - M' - M transformation.\", '494\\n\\nneighborhood relation on the L space, and lateral connections in the M cell layer can\\ninduce a neighborhood relation on the M space.', \"More recently, Durbin and Willshaw lO have devised an approach to solving certain\\ngeometric optimization problems (such as the traveling salesman problem) by a gradient\\ndescent method bearing some similarity to Kohonen's algorithm.\", 'There is a complementary relationship between the infomax principle and a local\\nalgorithm that may be found to implement it.', 'On the one hand, the principle may\\nexplain what the algorithm is \"for\" -- that is, how the algorithm may contribute to the\\ngeneration of a useful perceptual system.', 'This in turn can shed light on the system-level\\nrole of lateral connections and synaptic modification mechanisms in biological networks.', 'On the other hand, the existence of such a local algorithm is important for demonstrating\\nthat a network of relatively simple processors -- biological or synthetic -- can in fact find\\nglobal near-maxima of the Shannon information rate.', 'A Possible Connection Between Infomax and a Thermodynamic Principle\\nThe principle of \"maximum preservation of information\" can be viewed\\nequivalently as a principle of \"minimum dissipation of information.\"', 'When the principle\\nis satisfied, the loss of information from layer to layer is minimized, and the flow of\\ninformation is in this sense as \"nearly reversible\" as the constraints allow.', 'There is a\\nresemblance between this principle and the principle of \"minimum entropy production\"\\nII in nonequilibrium thermodynamics.', 'It has been suggested by Prigogine and others\\nthat the latter principle is important for understanding self-organization in complex\\nsystems.', 'There is also a resemblance, at the algorithmic level, between a Hebb-type\\nmodification rule and the autocatalytic processes l2 considered in certain models of\\nevolution and natural selection.', 'This raises the possibility that the connection I have\\ndrawn between synaptic modification rules and an information-theoretic optimization\\nprinciple may be an example of a more general relationship that is important for the\\nemergence of complex and apparently \"goal-oriented If structures and behaviors from\\nrelatively simple local interactions, in both neural and non-neural systems.', 'References\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nR. Linsker, Proc.', 'Natl.', 'Acad.', 'Sci.', 'USA 83,7508,8390,8779 (1986).', 'D. H .', 'Hubel and T. N. Wiesel, Proc.', 'Roy.', 'Soc.', 'London 8198,1 (1977).', 'D. O. Hebb, The Organization of Behavior (Wiley, N. Y., 1949).', 'R. Linsker, in: R. Cotterill (ed.', '), Computer Simulation in Brain Science\\n20-22 August 1986; Cambridge Univ.', 'Press, in press), p. 416.', 'R. Linsker, Computer (March 1988, in press).', 'E. Oja,J.', 'Math.', 'Bioi.', '15 , 267 (1982).', 'C .', 'E. Shannon, Bell Syst.', 'Tech.', 'J.', '27 .', '623 (1948).', 'T .', 'Kohonen, Self-Organization and Associative Memory (Springer-Verlag,\\nC. von der Malsburg and D. J. Willshaw, Proc.', 'Nat I.', 'A cad.', 'Sci.', 'USA 74 ,\\nR. Durbin and D. J. Willshaw, Nature 326,689 (1987).', 'P. Glansdorff and I. Prigogine, Thermodynamic Theory of Structure,\\nFluctuations (Wiley-Interscience, N. Y., 1971).', 'M. Eigen and P. Schuster, Die Naturwissenschaften 64 , 541 (1977).', '(Copenhagen.', 'N. Y .. 19S4).', '5176 (1977).', 'Stabili(v. and', '\\nConstrained Differential Optimization\\nJohn C. Platt\\nAlan H. Barr\\nCalifornia Institute of Technology, Pasadena, CA 91125\\n\\nAbstract\\nMany optimization models of neural networks need constraints to restrict the space of outputs to\\na subspace which satisfies external criteria.', 'Optimizations using energy methods yield \"forces\" which\\nact upon the state of the neural network.', 'The penalty method, in which quadratic energy constraints\\nare added to an existing optimization energy, has become popular recently, but is not guaranteed\\nto satisfy the constraint conditions when there are other forces on the neural model or when there\\nare multiple constraints.', 'In this paper, we present the basic differential multiplier method (BDMM),\\nwhich satisfies constraints exactly; we create forces which gradually apply the constraints over time,\\nusing \"neurons\" that estimate Lagrange multipliers.', 'The basic differential multiplier method is a differential version of the method of multipliers\\nfrom Numerical Analysis.', 'We prove that the differential equations locally converge to a constrained\\nminimum.', 'Examples of applications of the differential method of multipliers include enforcing permutation\\ncodewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem.', '1.', 'Introduction\\nOptimization is ubiquitous in the field of neural networks.', 'Many learning algorithms, such as\\nback-propagation,18 optimize by minimizing the difference between expected solutions and observed\\nsolutions.', 'Other neural algorithms use differential equations which minimize an energy to solve\\na specified computational problem, such as associative memory, D differential solution of the traveling salesman problem,s,lo analog decoding,lS and linear programming.', '1D Furthennore, Lyapunov\\nmethods show that various models of neural behavior find minima of particular functions.', '4,D\\nSolutions to a constrained optimization problem are restricted to a subset of the solutions of the\\ncorresponding unconstrained optimization problem.', 'For example, a mutual inhibition circuitS requires\\none neuron to be \"on\" and the rest to be \"off\".', 'Another example is the traveling salesman problem,ls\\nwhere a salesman tries to minimize his travel distance, subject to the constraint that he must visit\\nevery city exactly once.', 'A third example is the curve fitting problem, where elastic splines are as\\nsmooth as possible, while still going through data points.s Finally, when digital decisions are being\\nmade on analog data, the answer is constrained to be bits, either 0 or 1.', '14\\nA constrained optimization problem can be stated as\\nminimize / (~),\\nsubject to g(~) = 0,\\n\\n(1)\\n\\nwhere ~ is the state of the neural network, a position vector in a high-dimensional space; f(~) is a\\nscalar energy, which can be imagined as the height of a landscape as a function of position~; g(~) = 0\\nis a scalar equation describing a subspace of the state space.', 'During constrained optimization, the\\nstate should be attracted to the subspace g(~) = 0, then slide along the subspace until it reaches the\\nlocally smallest value of f(~) on g(~) = O.', 'In section 2 of the paper, we describe classical methods of constrained optimization, such as the\\npenalty method and Lagrange multipliers.', 'Section 3 introduces the basic differential multiplier method (BDMM) for constrained optimization, which calcuIates a good local minimum.', 'If the constrained optimization problem is convex, then\\nthe local minimum is the global minimum; in general, finding the global minimum of non-convex\\nproblems is fairly difficult.', 'In section 4, we show a Lyapunov function for the BDMM by drawing on an analogy from\\nphysics.', '?', 'American Institute of Physics 1988\\n\\n\\x0c613\\n\\nIn section 5, augmented Lagrangians, an idea from optimization theory, enhances the convergence\\nproperties of the BDMM.', 'In section 6, we apply the differential algorithm to two neural problems, and discuss the insensitivity of BDMM to choice of parameters.', 'Parameter sensitivity is a persistent problem in neural\\nnetworks.', '2.', 'Classical Methods of Constrained Optimization\\nThis section discusses two methods of constrained optimization, the penalty method and Lagrange\\nmultipliers.', 'The penalty method has been previously used in differential optimization.', 'The basic\\ndifferential multiplier method developed in this paper applies Lagrange multipliers to differential\\noptimization.', '2.l.', 'The Penalty Method\\nThe penalty method is analogous to adding a rubber band which attracts the neural state to\\nthe subspace g(~) = o.', 'The penalty method adds a quadratic energy term which penalizes violations of constraints.', '8 Thus, the constrained minimization problem (1) is converted to the following\\nunconstrained minimization problem:\\n\\n(2)\\n\\nFigure 1.', 'The penalty method makes a trough in state space\\nThe penalty method can be extended to fulfill multiple constraints by using more than one rubber\\nband.', \"Namely, the constrained optimization problem\\nminimize f (.~),\\n8ubject to go (~)\\n\\n= OJ\\n\\na\\n\\n= 1,2, ... , n;\\n\\n(3)\\n\\nis converted into unconstrained optimization problem\\nn\\n\\nminimize l'pena1ty(~) = f(~)\\n\\n+ L Co(go(~))2.\", '(4)\\n\\n0:::1\\n\\nThe penalty method has several convenient features.', 'First, it is easy to use.', 'Second, it is globally\\nconvergent to the correct answer as Co - 00.', '8 Third, it allows compromises between constraints.', 'For\\nexample, in the case of a spline curve fitting input data, there can be a compromise between fitting\\nthe data and making a smooth spline.', '614\\n\\nHowever, the penalty method has a number of disadvantages.', \"First, for finite constraint strengths\\nit doesn't fulfill the constraints exactly.\", 'Using multiple rubber band constraints is like building\\na machine out of rubber bands: the machine would not hold together perfectly.', 'Second, as more\\nconstraints are added, the constraint strengths get harder to set, especially when the size of the\\nnetwork (the dimensionality of\\ngets large.', 'In addition, there is a dilemma to the setting of the constraint strengths.', 'If the strengths are small,\\nthen the system finds a deep local minimum, but does not fulfill all the constraints.', 'If the strengths\\nare large, then the system quickly fulfills the constraints, but gets stuck in a poor local minimum.', \"COl'\\n\\n.u\\n\\n2.2.\", 'Lagrange Multipliers\\nLagrange multiplier methods also convert constrained optimization problems into unconstrained\\nextremization problems.', 'Namely, a solution to the equation (1) is also a critical point of the energy\\n\\n(5)\\n).', 'is called the Lagrange multiplier for the constraint g(~) = 0.8\\nA direct consequence of equation (5) is that the gradient of f is collinear to the gradient of 9 at\\nthe constrained extrema (see Figure 2).', \"The constant of proportionality between 'i1 f and 'i1 9 is -).\", \":\\n'i1 'Lagrange\\n\\n= 0 = 'i1 f + ).\", \"'i1 g.\\n\\n(6)\\n\\nWe use the collinearity of 'i1 f and 'i1 9 in the design of the BDMM.\", 'Figure 2.', \"At the constrained minimum, 'i1 f = -).\", \"'i1 9\\nA simple example shows that Lagrange multipliers provide the extra degrees of freedom necessary\\nto solve constrained optimization problems.\", 'Consider the problem of finding a point (x, y) on the\\nline x + y = 1 that is closest to the origin.', \"Using Lagrange multipliers,\\n'Lagrange\\n\\n= x 2 + y2 + ).\", \"(x + y -\\n\\n1)\\n\\n(7)\\n\\nNow, take the derivative with respect to all variables, x, y, and A.\\naeLagrange\\n\\n= 2x + A = 0\\n\\na'Lagrange\\n\\n= 2y + A = 0\\n\\nax\\nay\\n\\na'Lagrange =\\n\\na).\", 'x\\n\\n+y -\\n\\n1= 0\\n\\n(8)\\n\\n\\x0c615\\n\\nWith the extra variable A, there are now three equations in three unknowns.', 'In addition, the last\\nequation is precisely the constraint equation.', '3.', 'The Basic Differential Multiplier Method for Constrained Optimization\\nThis section presents a new \"neural\" algorithm for constrained optimization, consisting of differential equations which estimate Lagrange multipliers.', 'The neural algorithm is a variation of the\\nmethod of multipliers, first presented by Hestenes 9 and Powell 16 ?', '3.1.', 'Gradient Descent does not work with Lagrange Multipliers\\nThe simplest differential optimization algorithm is gradient descent, where the state variables of\\nthe network slide downhill, opposite the gradient.', 'Applying gradient descent to the energy in equation\\n(5) yields\\n\\nx.', '- _ a!Lagrange\\n,ax?,\\n\\\\.', 'a!Lagrange\\n= aA\\nJ\\\\\\n\\n= _\\n\\nal _ A ag\\nax?\"', \"ax' '\\n\\n= -g\\n\\n*.\", '(9)\\n\\n( )\\n\\nNote that there is a auxiliary differential equation for A, which is an additional \"neuron\" necessary\\nto apply the constraint g(~) = O.', 'Also, recall that when the system is at a constrained extremum,\\nVI = -AVg, hence, x.', '= O.\\nEnergies involving Lagrange multipliers, however, have critical points which tend to be saddle\\npoints.', 'Consider the energy in equation (5).', 'If ~ is frozen, the energy can be decreased by sending\\nA to +00 or -00.', 'Gradient descent does not work with Lagrange multipliers, because a critical point of the energy\\nin equation (5) need not be an attractor for (9).', 'A stationary point must be a local minimum in order\\nfor gradient descent to converge.', '3.2.', 'The New Algorithm: the Basic Differential Multiplier Method\\nWe present an alternative to differential gradient descent that estimates the Lagrange multipliers,\\nso that the constrained minima are attractors of the differential equations, instead of \"repulsors.\"', 'The\\ndifferential equations that solve (1) is\\n\\n.', 'al\\n,\\nax,\\ni = +g(*).', \"ag\\nax.'\", \"X' = - - - A -\\n\\n(10)\\n\\nEquation (10) is similar to equation (9).\", 'As in equation (9), constrained extrema of the energy\\n(5) are stationary points of equation (10).', 'Notice, however, the sign inversion in the equation for i,\\nas compared to equation (9).', 'The equation (10) is performing gradient ascent on A.', 'The sign flip\\nmakes the BDMM stable, as shown in section 4.', 'Equation (10) corresponds to a neural network with anti-symmetric connections between the A\\nneuron and all of the ~ neurons.', '3.3.', 'Extensions to the Algorithm\\nOne extension to equation (10) is an algorithm for constrained minimization with multiple constraints.', 'Adding an extra neuron for every equality constraint and summing all of the constraint forces\\ncreates the energy\\n(11)\\n!multiple = !', '(~) +\\nAo<ga(~),\\n\\nI:\\n0<\\n\\nwhich yields differential equations\\n\\nx\\' - _ al _ \"\" A agcr.', \",- ax'\\n\\n~\\n\\n'0<\\n\\n0<\\n\\nax' )\\n'\\n\\n(12)\\n\\n\\x0c616\\n\\nAnother extension is constrained minimization with inequality constraints.\", 'As in traditional\\noptimization theory.8 one uses extra slack variables to convert inequality constraints into equality\\nconstraints.', 'Namely.', 'a constraint of the form h(~) ~ 0 can be expressed as\\n\\n(13)\\nSince Z2 must always be positive, then h(~) is constrained to be positive.', 'The slack variable z is\\ntreated like a component of ~ in equation (10).', 'An inequality constraint requires two extra neurons,\\none for the slack variable % and one for the Lagrange multiplier ~.', 'Alternatively, the inequality constraint can be represented as an equality constraint For example,\\nif h(~) ~ 0, then the optimization can be constrained with g(~) = h(.~), when h(~) ~ 0; and\\ng(.~) = 0 otherwise.', '4.', 'Why the algorithm works\\nThe system of differential equations (10) (the BDMM) gradually fulfills the constraints.', 'Notice\\nthat the function g(~) can be replaced by kg(~), without changing the location of the constrained\\nminimum.', 'As k is increased, the state begins to undergo damped oscillation about the constraint\\nsubspace g(~) = o.', 'As k is increased further, the frequency of the oscillations increase, and the time\\nto convergence increases.', 'constraint subspace\\n\\n./\\n\\n/\\'\\n\\ninitial?state\\n\\n.,.-\\n\\npath of algorithm\\n\\n\"\\\\\\n\\n\\\\\\n\\nFigure 3.', 'The state is attracted to the constraint subspace\\n\\nThe damped oscillations of equation (10) can be explained by combining both of the differential\\nequations into one second-order differential equation.', '(14)\\nEquation (14) is the equation for a damped mass system, with an inertia term Xi.', 'a damping matrix\\n\\n(15)\\nand an internal force, gOg/O%i, which is the derivative of the internal energy\\n\\n(16)\\n\\n\\x0c617\\n\\nIf the system is damped and the state remains bounded, the state falls into a constrained minima.', 'As in physics, we can construct a total energy of the system, which is the sum of the kinetic and\\npotential energies.', 'E= T\\n\\n+U =\\n\\nL, i(xd\\n\\n2\\n\\n+ i(g(~))2.', '(17)\\n\\nIf the total energy is decreasing with time and the state remains bounded, then the system will\\ndissipate any extra energy, and will settle down into the state where\\n\\n(18)\\nwhich is a constrained extremum of the original problem in equation (1).', 'The time derivative of the total energy in equation (17) is\\n\\n= -\\n\\n(19)\\n\\nLx,A,jxj.', \"',i\\n\\nIf damping matrix Aii is positive definite, the system converges to fulfill the constraints.\", 'BDMM always converges for a special case of constrained optimization: quadratic programming.', 'A quadratic programming problem has a quadratic function f(~) and a piecewise linear continuous\\nfunction g(~) such that\\n\\n(20)\\nUnder these circumstances, the damping matrix Aii is positive definite for all\\nsystem converges to the constraints.', '~\\n\\nand A, so that the\\n\\n4.1.', 'Multiple constraints\\nFor the case of multiple constraints, the total energy for equation (12) is\\n\\nE = T\\n\\n+U =\\n\\nL\\ni\\n\\ni(Xi)2 +\\n\\nL igo(~)2.', '(21)\\n\\n0\\n\\nand the time derivative is\\n\\n(22)\\n\\nAgain, BDMM solves a quadratic programming problem, if a solution exists.', 'However, it is\\npossible to pose a problem that has contradictory constraints.', 'For example,\\n\\ngdx) = x =\\n\\n0,\\n\\ng2(X) = x - I = 0\\n\\n(23)\\n\\nIn the case of conflicting constraints, the BDMM compromises, trying to make each constraint go as\\nsmall as possible.', 'However, the Lagrange multipliers Ao goes to ?oo as the constraints oppose each\\nother.', 'It is possible, however, to arbitrarily limit the Ao at some large absolute value.', \"618\\n\\nLaSalle's invariance theorem 12 is used to prove that the BDMM eventually fulfills the constraints.\", 'Let G be an open subset of Rn.', 'Let F be a subset of G*, the closure of G, where the system of\\ndifferential equations (12) is at an equilibrium.', '(24)\\nIf the damping matrix\\n\\na2 f + \\'\" A a2 ga\\n-----:;_\\nax, ax;\\n\\n~\\n\\na\\n\\nax,ax;\\n\\n(25)\\n\\nis positive definite in G, if xa{ t) and Aa (t) are bounded, and remain in G for all time, and ~f F\\nis non-empty, then F is the largest invariant set in G*, hence, by LaSalle\\'s invariance theorem, the\\nsystem (t), Aa (t) approaches Fast -+ 00.\\n\\nx,\\n\\n5.', 'The Modified Differential Method of Multipliers\\nThis section presents the modified differemiaI multiplier method (MDMM), which is a modification of the BDMM with more robust convergence properties.', 'For a given constrained optimization\\nproblem, it is frequently necessary to alter the BDMM to have a region of positive damping surrounding the constrained minima.', 'The non-differential method of multipliers from Numerical Analysis also\\nhas this difficulty.', '2 Numerical Analysis combines the multiplier method with the penalty method to\\nyield a modified multiplier method that is locally convergent around constrained minima.', '2\\nThe BDMM is completely compatible with the penalty method.', 'If one adds a penalty force to\\nequation (10) corresponding to an quadratic energy\\nEpenalty\\n\\n= ~(g(~))2.', '(26)\\n\\nthen the set of differential equations for MDMM is\\n\\n.', 'af\\n\\nag\\n\\nx, = -ax,\\n-- A\\nax,- j = g(~).', 'ag\\nax,\\n\\ncg-,\\n\\n(27)\\n\\nThe extra force from the penalty does not change the position of the stationary points of the differential\\nequations, because the penalty force is 0 when g(~) = O.', 'The damping matrix is modified by the\\npenalty force to be\\n\\n(28)\\nThere is a theorem 1 that states that there exists a c* > 0 such that if c > c*, the damping matrix\\nin equation (28) is positive definite at constrained minima.', 'Using continuity, the damping matrix is\\npositive definite in a region R surrounding each constrained minimum.', 'If the system starts in the\\nregion R and remains bounded and in R, then the convergence theorem at the end of section 4 is\\napplicable, and MDMM will converge to a constrained minimum.', 'The minimum necessary penalty strength c for the MDMM is usually much less than the strength\\nneeded by the penalty method alone.', '2\\n\\n6.', 'Examples\\nThis section contains two examples which illustrate the use of the BDMM and the MDMM.', 'First,\\nthe BDMM is used to find a good solution to the planar traveling salesman problem.', 'Second, the\\nMDMM is used to enforcing mutual inhibition and digital results in the task of analog decoding.', '6.1.', 'Planar Traveling Salesman\\nThe traveling salesman problem (fSP) is, given a set of cities lying in the plane, find the shortest\\nclosed path that goes through every city exactly once.', 'Finding the shortest path is NP-complete.', '619\\n\\nFinding a nearly optimal path, however, is much easier than finding a globally optimal path.', 'There\\nexist many heuristic algorithms for approximately solving the traveling salesman problem.', '5,10,11,13\\nThe solution presented in this section is moderately effective and illustrates the independence of\\nBDMM to changes in parameters.', 'Following Durbin and Willshaw,5 we use an elastic snake to solve the TSP.', 'A snake is a discretized\\ncurve which lies on the plane.', 'The elements of the snake are points on the plane, (Xi, Yd.', 'A snake\\nis a locally connected neural network, whose neural outputs are positions on the plane.', 'The snake minimizes its length\\n\\n2:)Xi+1 - x,)2 - (Yi+l - Yi)2,\\n\\n(29)\\n\\ni\\n\\nsubject to the constraint that the snake must lie on the cities:\\n\\nk(x* - xc) = 0,\\nk(y* - Yc) = 0,\\n(30)\\nwhere (x*, y*) are city coordinates, (xc, Yc) is the closest snake point to the city, and k is the constraint\\nstrength.', 'The minimization in equation (29) is quadratic and the constraints in equation (30) are piecewise\\nlinear, corresponding to a CO continuous potential energy in equation (21).', 'Thus, the damping is\\npositive definite, and the system converges to a state where the constraints are fulfilled.', 'In practice, the snake starts out as a circle.', 'Groups of cities grab onto the snake, deforming\\nit As the snake gets close to groups of cities, it grabs onto a specific ordering of cities that locally\\nminimize its length (see Figure 4).', 'The system of differential equations that solve equations (29) and (30) are piecewise linear.', \"The\\ndifferential equations for Xi and Yi are solved with implicit Euler's method, using tridiagonal LV\\ndecomposition to solve the linear system.\", '17 The points of the snake are sorted into bins that divide\\nthe plane, so that the computation of finding the nearest point is simplified.', 'Figure 4.', 'The snake eventually attaches to the cities\\nThe constrained minimization in equations (29) and (30) is a reasonable method for approximately\\nsolving the TSP.', 'For 120 cities distributed in the unti square, and 600 snake points, a numerical step\\nsize of 100 time units, and a constraint strength of 5 x 10- 3 , the tour lengths are 6% ?', '2% longer\\nthan that yielded by simulated annealing 11 .', 'Empirically, for 30 to 240 cities, the time needed to\\ncompute the final city ordering scales as N1.6, as compared to the Kernighan-Lin method13 , which\\nscales roughly as N 2 .2 ?', 'The constraint strength is usable for both a 30 city problem and a 240 city problem.', 'Although\\nchanging the constraint strength affects the performance, the snake attaches to the cities for any nonzero constraint strength.', 'Parameter adjustment does not seem to be an issue as the number of cities\\nincreases, unlike the penalty method.', '620\\n\\n6.2.', 'Analog Decoding\\nAnalog decoding uses analog signals from a noisy channel to reconstruct codewords.', 'Analog\\ndecoding has been performed neurally,15 with a code space of permutation matrices, out of the\\npossible space of binary matrices.', 'To perform the decoding of permutation matrices, the nearest permutation matrix to the signal\\nmatrix must be found.', 'In other words, find the nearest matrix to the signal matrix, subject to the\\nconstraint that the matrix has on/off binary elements, and has exactly one \"on\" per row and one \"on\"\\nper column.', 'If the signal matrix is Ii; and the result is Vi;, then minimize\\n\\n- \"v..\\nL..J .,,1-.', '.,\\n\\n(31)\\n\\ni ,;\\n\\nsubject to constraints\\n\\nVi,,(l- Vi;) = OJ\\n\\nLVi\" -1 =\\n\\n(32)\\n\\nO.\\n\\n;\\n\\nIn this example, the first constraint in equation (32) forces crisp digital decisions.', 'The second\\nand third constraints are mutual inhibition along the rows and columns of the matrix.', 'The optimization in equation (31) is not quadratic, it is linear.', 'In addition, the first constraint in\\nequation (32) is non-linear.', 'Using the BDMM results in undamped oscillations.', 'In order to converge\\nonto a constrained minimum, the MDMM must be used.', 'For both a 5 x 5 and a 20 x 20 system, a\\nc = 0,2 is adequate for damping the oscillations.', 'The choice of c seems to be reasonably insensitive\\nto the size of the system, and a wide range of c, from 0.02 to 2.0, damps the oscillations.', '.?...?....', '..?????...???.', '?....?.?????', \"..'\\n????\", '???????', \"??..'\", '...????.', '.', '.?.???.', '??', '??', '.?.', '???', '???', '???', '??', '???', '???', '.... .', '...', '??', '??', '..', '???', '... ...', '.?....??', '..\\n\\n,\\n\\n?', '?', '?', '?', \".\\n\\n'\", '..?......', '?', '?', '... e?', '... .', '.. .', '?', '.. .', 'e?', '... ?', '.', '?', '???', '?', '?', '??', '.', '?', '?', '?', '?', \"??????\\n\\n'\", '????????', '?', '?', '?', '?', ',\\n\\n?', '.', '?', '.?', ': :e&:.', ':: ....?.', '???', '?.', '?', '????', '.', '?', '???', '..\\n\\n?', '???', '::r::::::::\\n.', '.', '?????', '....\\n\\n?', '?', ':~:.:.', ':\\n?', '?', '?', '?', '?', '.', '.', '???.', \"?\\n\\n'\", '?', '??', '???', '?', '?', '?', '?', '?', '?', '.', '?', '.....', 'Figure 5.', 'The decoder finds the nearest permutation matrix\\n\\nIn a test of the MDMM, a signal matrix which is a permutation matrix plus some noise, with\\na signal-to-noise ratio of 4 is supplied to the network.', 'In figure 5, the system has turned on the\\ncorrect neurons but also many incorrect neurons.', 'The constraints start to be applied, and eventually\\nthe system reaches a permutation matrix.', 'The differential equations do not need to be reset.', 'If a new\\nsignal matrix is applied to the network, the neural state will move towards the new solution.', '7.', 'ConClusions\\nIn the field of neural networks, there are differential optimization algorithms which find local\\nsolutions to non-convex problems.', 'The basic differential multiplier method is a modification of a\\nstandard constrained optimization algorithm, which improves the capability of neural networks to\\nperform constrained optimization.', 'The BDMM and the MDMM offer many advantages over the penalty method.', 'First, the differential equations (10) are much less stiff than those of the penalty method.', 'Very large quadratic terms\\nare not needed by the MDMM in order to strongly enforce the constraints.', 'The energy terrain for the\\n\\n\\x0c621\\n\\npenalty method looks like steep canyons, with gentle floors; finding minima of these types of energy\\nsurfaces is numerically difficult In addition, the steepness of the penalty tenns is usually sensitive\\nto the dimensionality of the space.', 'The differential multiplier methods are promising techniques for\\nalleviating stiffness.', 'The differential multiplier methods separate the speed of fulfilling the constraints from the accuracy of fulfilling the constraints.', 'In the penalty method, as the strengths of a constraint goes to\\n00, the constraint is fulfilled, but the energy has many undesirable local minima.', 'The differential\\nmultiplier methods allow one to choose how quickly to fulfill the constraints.', 'The BDMM fulfills constraints exactly and is compatible with the penalty method.', 'Addition of\\npenalty tenns in the MDMM does not change the stationary points of the algorithm, and sometimes\\nhelps to damp oscillations and improve convergence.', 'Since the BDMM and the MDMM are in the form of first-order differential equations, they can\\nbe directly implemented in hardware.', 'Performing constrained optimization at the raw speed of analog\\nVLSI seems like a promising technique for solving difficult perception problems.', '14\\nThere exist Lyapunov functions for the BDMM and the MDMM.', 'The BDMM converges globally for quadratic programming.', \"The MDMM is provably convergent in a local region around the\\nconstrained minima Other optimization algorithms, such as Newton's method,17 have similar local convergence properties.\", 'The global convergence properties of the BDMM and the MDMM are\\ncurrently under investigation.', 'In summary, the differential method of multipliers is a useful way of enforcing constraints on\\nneural networks for enforcing syntax of solutions, encouraging desirable properties of solutions, and\\nmaking crisp decisions.', 'Acknowledgments\\nThis paper was supported by an AT&T Bell Laboratories fellowship (JCP).', 'References\\n1.', 'K. J. Arrow, L. Hurwicz, H. Uzawa, Studies in Linear and Nonlinear Programming.', '(Stanford\\nUniversity Press, Stanford, CA, 1958).', '2.', 'D. P. Bertsekas, Automatica, 12, 133-145, (1976).', '3.', 'C. de Boor, A Practical Guide to Splines.', '(Springer-Verlag, NY, 1978).', '4.', 'M. A. Cohen, S. Grossberg, IEEE Trans.', 'Systems.', 'Man.', 'and Cybernetics, ,815-826, (1983).', '5.', 'R. Durbin, D. Willshaw, Nature, 326, 689-691, (1987).', '6.', 'J. C. Eccles, The Physiology of Nerve Cells, (Johns Hopkins Press, Baltimore, 1957).', '7.', 'M. R. Hestenes, J. Opt.', 'Theory Appl., 4, 303-320, (1969).', '8.', 'M. R. Hestenes, Optimization Theory, (Wiley & Sons, NY, 1975).', '9.', 'J. J. Hopfield, PNAS, 81, 3088, (1984).', '10.', 'J. J. Hopfield, D. W. Tank, Biological Cybernetics, 52, 141, (1985).', '11.', 'S. Kirkpatrick, C. D. Gelatt, C. M. Vecchi, Science, 220, 671-680, (1983).', '12.', 'J. LaSalle, The Stability of Dynamical Systems, (SIAM, Philadelphia, 1976).', '13.', 'S. Lin, B. W. Kernighan, Oper.', 'Res., 21,498-516 (1973).', '14.', 'C. A. Mead, Analog VLSI and Neural Systems, (Addison-Wesley, Reading.', 'MA, TBA).', '15.', 'J. C. Platt, J. J. Hopfield, in AlP Con/.', 'Proc.151: Neural Networksfor Computing (1.', 'Denker\\ned.)', '364-369, (American Institute of PhysiCS, NY, 1986).', '16.', 'M. 1.', 'Powell, in Optimization, (R. Fletcher, ed.', '), 283-298, (Academic Press, NY, 1969).', '17.', 'W. H. Press, B. P. Flannery, S. A. Teukolsky, W. T. Vetterling, Numerical Recipes, (Cambridge University Press, Cambridge, 1986).', '18.', 'D. Rumelhart, G. Hinton, R. Williams, in Parallel Distributed Processing, (D. Rumelhart,\\ned), 1, 318-362, (MIT Press, Cambridge, MA, 1986).', '19.', 'D. W. Tank, J. J. Hopfield, IEEE Trans.', 'Cir.', '& Sys., CAS-33, no.', '5,533-541 (1986).', '\\nA NEURAL-NETWORK SOLUTION TO THE CONCENTRATOR\\nASSIGNNlENT PROBLEM\\nGene A. Tagliarini\\nEdward W. Page\\nDepartment of Computer Science, Clemson University, Clemson, SC\\n\\n29634-1906\\nABSTRACT\\nNetworks of simple analog processors having neuron-like properties have\\nbeen employed to compute good solutions to a variety of optimization problems.', 'This paper presents a neural-net solution to a resource allocation problem that arises in providing local access to the backbone of a wide-area communication network.', 'The problem is described in terms of an energy function\\nthat can be mapped onto an analog computational network.', 'Simulation results\\ncharacterizing the performance of the neural computation are also presented.', 'INTRODUCTION\\nThis paper presents a neural-network solution to a resource allocation\\nproblem that arises in providing access to the backbone of a communication\\nnetwork.', '1 In the field of operations research, this problem was first known as\\nthe warehouse location problem and heuristics for finding feasible, suboptimal\\nsolutions have been developed previously.2.', '3 More recently it has been known\\nas the multifacility location problem4 and as the concentrator assignment problem.1\\nTHE HOPFIELD NEURAL NETWORK MODEL\\nThe general structure of the Hopfield neural network model5 ?', '6,7 is illustrated in Fig.', '1.', 'Neurons are modeled as amplifiers that have a sigmoid input!', 'output curve as shown in Fig.', '2.', 'Synapses are modeled by permitting the output of any neuron to be connected to the input of any other neuron.', 'The\\nstrength of the synapse is modeled by a resistive connection between the output\\nof a neuron and the input to another.', 'The amplifiers provide integrative analog\\nsummation of the currents that result from the connections to other neurons as\\nwell as connection to external inputs.', 'To model both excitatory and inhibitory\\nsynaptic links, each amplifier provides both a normal output V and an inverted\\noutput V. The normal outputs range between 0 and 1 while the inverting amplifier produces corresponding values between 0 and -1.', 'The synaptic link between the output of one amplifier and the input of another is defined by a\\nconductance Tij which connects one of the outputs of amplifier j to the input of\\namplifier i.', 'In the Hopfield model, the connection between neurons i and j is\\nmade with a resistor having a value Rij = 1rrij .', 'To provide an excitatory synaptic connection (positive Tij ), the resistor is connected to the normal output of\\nThis research was supported by the U.S. Army Strategic Defense Command.', '?', 'American Institute of Physics 1988\\n\\n\\x0c776\\n\\n13\\n\\n14\\n\\ninputs\\n\\n1\\n\\nV\\n\\no\\nVI\\n\\nV2\\nV3\\nV4\\noutputs\\nFig.', '1.', 'Schematic for a simplified\\nHopfield network with four neurons.', 'o\\n\\n-u\\n\\n+u\\n\\nFig.', '2.', 'Amplifier input/output\\nrelationship\\n\\namplifier j.', 'To provide an inhibitory connection (negative Tij), the resistor is\\nconnected to the inverted output of amplifier j.', 'The connections among the\\nneurons are defined by a matrix T consisting of the conductances T ij .', 'Hopfield has shown that a symmetric T matrix (Tij = Tji ) whose diagonal entries\\nare all zeros, causes convergence to a stable state in which the output of each\\namplifier is either 0 or 1.', 'Additionally, when the amplifiers are operated in the\\nhigh-gain mode, the stable states of a network of n neurons correspond to the\\nlocal minima of the quantity\\n\\nn\\n\\nE\\n\\n= (-112)\\n\\nn\\n\\nL L\\ni=l j=l\\n\\nn\\n\\nT?V.V?', 'IJ 1 J\\n\\nL\\n\\nV.I?', 'I 1\\n\\n(1)\\n\\nwhere Vi is the output of the ith neuron and Ii is the externally supplied input\\nto the ph neuron.', 'Hopfield refers to E as the computational energy of the system.', 'THE CONCENTRATOR ASSIGNMENT PROBLEM\\nConsider a collection of n sites that are to be connected to m concentrators as illustrated in Fig.', '3(a).', 'The sites are indicated by the shaded circles\\nand the concentrators are indicated by squares.', 'The problem is to find an\\nassignment of sites to concentrators that minimizes the total cost of the assignment and does not exceed the capacity of any concentrator.', 'The constraints\\nthat must be met can be summarized as follows:\\na) Each site i ( i\\nand\\n\\n= 1,\\n\\n2, ... , n ) is connected to exactly one concentrator;\\n\\n\\x0c777\\n\\nb) Each concentrator j (j = 1, 2, ... , m ) is connected to no more than kj\\nsites (where kj is the capacity of concentrator D.\\nFigure 3(b) illustrates a possible solution to the problem represented in Fig.', '3(a).', '?', '0\\n\\n0\\n\\n??', '??', '?', '0\\n\\n?', '?', '?', '??', '?', '0\\n\\no Concentrators\\n\\n?', 'Sites\\n\\n(a).', 'Site/concentrator map\\n\\n(b).', 'Possible assignment\\n\\nFig.', '3.', 'Example concentrator assignment problem\\nIf the cost of assigning site i to concentrator j is cij , then the total cost of\\n\\na particular assignment is\\ntotal cost\\n\\nn\\n\\nm\\n\\ni=l\\n\\nj=l\\n\\n= L L x ?', '?IJ c?', '?IJ\\n\\n(2)\\n\\nwhere Xij = 1 only if we actually decide to assign site i to concentrator j and is 0\\notherwise.', 'There are mn possible assignments of sites to concentrators that\\nsatisfy constraint a).', 'Exhaustive search techniques are therefore impractical\\nexcept for relatively small values of m and n.\\nTHE NEURAL NETWORK SOLUTION\\nThis problem is amenable to solution using the Hopfield neural network\\nmodel.', 'The Hopfield model is used to represent a matrix of possible assignments of sites to concentrators as illustrated in Fig.', '4.', 'Each square corresponds\\n\\n\\x0c778\\n\\nCONCENTRATORS\\n1\\n2\\nj\\nm\\n\\nr,;------;-,\\n/r\\n\\nSITES\\n\\n1\\n\\n,II 11- --III ---III,\\n\\n2\\n\\n,~ .---~ ---~I\\n????', ',..\\n?', '?', \"I The darkly shaded neu-\\n\\n~~\\n\\ni\\n\\n'~n\\n\\n~\\n\\nIII 11- --II --- II Iron corresponds to the\\n::\\n\\n:\\n\\n:\\n\\n'Ii ?\", '---Ii ---Ii \\'\\n~ -\\n\\n\" n+l\\n\\nSLACK .... < n+2\\n,~n+k j\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\nhypothesis that site i\\nshould be\\n\\nas~igned to\\n\\n:.J concentrator J.', '-\\n\\nII 111---11---11\\nIII II ---~ ---?', '?', '??', 'II 11- --III ---III\\nFig.', '4.', 'Concentrator assignment array\\n\\nto a neuron and a neuron in row i and column j of the upper n rows of the\\narray represents the hypothesis that site i should be connected to concentrator\\nj.', 'If the neuron in row i and column j is on, then site i should be assigned to\\nconcentrator j; if it is off, site i should not be assigned to concentrator j.', 'The neurons in the lower sub-array, indicated as \"SLACK\", are used to\\nimplement individual concentrator capacity constraints.', 'The number of slack\\nneurons in a column should equal the capacity (expressed as the number sites\\nwhich can be accommodated) of the corresponding concentrator.', 'While it is\\nnot necessary to assume that the concentrators have equal capacities, it was\\nassumed here that they did and that their cumulative capacity is greater than or\\nequal to the number of sites.', 'To ena~le the neurons in the network illustrated above to compute solutions to the concentrator problem, the network must realize an energy function\\nin which the lowest energy states correspond to the least cost assignments.', 'The\\nenergy function must therefore favor states which satisfy constraints a) and b)\\nabove as well as states that correspond to a minimum cost assignment.', 'The\\nenergy function is implemented in terms of connection strengths between neurons.', 'The following section details the construction of an appropriate energy\\nfunction.', '779\\n\\nTHE ENERGY FUNCTION\\nConsider the following energy equation:\\nn\\n\\nE\\n\\n=\\n\\nA\\n\\nm\\n\\nL ( .L1 y 1J..\\n\\n.', '1\\n1=\\n\\n2\\n\\n- 1 )\\n\\n+\\n\\nJ=\\n\\nB\\n\\nm\\n\\nn+k?', 'j=1\\n\\ni=1\\n\\nL (L\\n\\nJ y .. - k .', ')2\\n\\nIJ\\n\\nJ\\n\\n(3~\\n\\nm n+kj\\n\\n+ C\\n\\nL L\\n\\ny.. ( 1 - Yij )\\nj=1 i=1 1J\\n\\nwhere Y ij is the output of the amplifier in row i and column j of the neuron\\nmatrix, m and n are the number of concentrators and the number of sites\\nrespectively, and kj is the capacity of concentrator j.', 'The first term will be minimum when the sum of the outputs in each row\\nof neurons associated with a site equals one.', 'Notice that this term influences\\nonly those rows of neurons which correspond to sites; no term is used to coerce\\nthe rows of slack neurons into a particular state.', 'The second term of the equation will be minimum when the sum of the\\noutputs in each column equals the capacity kj of the corresponding concentrator.', 'The presence of the kj slack neurons in each column allows this term to\\nenforce the concentrator capacity restrictions.', 'The effect of this term upon the\\nupper sub-array of neurons (those which correspond to site assignments) is\\nthat no more than kj sites will be assigned to concentrator j.', 'The number of\\nneurons to be turned on in column j is kj ; consequently, the number of neurons turned on in column j of the assignment sub-array will be less than or\\nequal to kj .', 'The third term causes the energy function to favor the \"zero\" and \"one\"\\nstates of the individual neurons by being minimum when all neurons are in one\\nor the other of these states.', 'This term influences all neurons in the network.', 'In summary, the first term enforces constraint a) and the second term\\nenforces constraint b) above.', 'The third term guarantees that a choice is actually made; it assures that each neuron in the matrix will assume a final state\\nnear zero or one corresponding to the Xij term of the cost equation (Eq.', '2).', 'After some algebraic re-arrangement, Eq.', '3 can be written in the form of\\nEq.', '1 where\\n.,\\n{A * 8(i,k) * (1-8U,I) + B * 8U,1) * (1-8(i,k?, if i<n and k<n\\nT IJ kl =\\n(4)\\n,\\nC * 8U,I) * (1-8(i,k?, if i>n or k>n.', 'Here quadruple subscripts are used for the entries in the matrix T. Each entry\\nindicates the strength of the connection between the neuron in row i and column j and the neuron in row k and column I of the neuron matrix.', 'The function delta is given by\\n\\n\\x0c780\\n\\n1, if i = j\\n(5)\\n0, otherwise.', 'The A and B terms specify inhibitions within a row or a column of the upper\\nsub-array and the C term provides the column inhibitions required for the\\nneurons in the sub-array of slack neurons.', '8( i , j ) = {\\n\\nEquation 3 specifies the form of a solution but it does not include a term\\nthat will cause the network to favor minimum cost assignments.', 'To complete\\nthe formulation, the following term is added to each Tij,kl:\\n\\nD ?', '8( j , I ) ?', '( 1 - 8( i , k ) )\\n(cost [ i , j ] + cost [ k , I ])\\nwhere cost[ i , j ] is the cost of assigning site i to concentrator j.', 'The effect of\\nthis term is to reduce the inhibitions among the neurons that correspond to low\\ncost assignments.', 'The sum of the costs of assigning both site i to concentrator j\\nand site k to concentrator I was used in order to maintain the symmetry of T.\\nThe external input currents were derived from the energy equation (Eq.3)\\nand are given by\\nj , if i < n\\n(6)\\nIJ 2 ?', 'k j - 1, otherwise.', 'I.._ {2. k\\n\\nThis exemplifies a teChnique for combining external input currents which arise\\nfrom combinations of certain basic types of constraints.', 'AN EXAMPLE\\n\\nThe neural network solution for a concentrator assignment problem consisting of twelve sites and five concentrators was simulated.', 'All sites and concentrators were located within the unit square on a randomly generated map.', 'For this problem, it was assumed that no more than three sites could be\\nassigned to a concentrator.', 'The assignment cost matrix and a typical assignment resulting from the simulation are shown in Fig.', '5.', 'It is interesting to\\nnotice that the network proposed an assignment which made no use of concentrator 2.', 'Because the capacity of each concentrator kj was assumed to be three\\nsites, the external input current for each neuron in the upper sub-array was\\nI ij = 6\\nwhile in the sub-array of slack neurons it was\\nI ij = 5.', 'The other parameter values used in the simulation were\\nA = B = C =-2\\n\\nand\\n\\nD = 0.1 .', '781\\n\\nSITES\\n\\n1\\n\\nCONCENTRATORS\\n2\\n4\\n3\\n\\n5\\n\\n@\\n\\n.46\\n\\n.40\\n\\n.63\\n\\n.39\\n\\n.92\\n\\n.38\\n\\n.82\\n\\n.81\\n\\n.56\\n\\n@\\n\\n.51\\n\\n.76\\n\\n.46\\n\\n.17\\n\\n.39\\n\\n.77\\n\\n.41\\n\\nH\\n\\n@\\n\\n.81\\n\\n.54\\n\\n.52\\n\\nI\\n\\n.60\\n\\n.67\\n\\n.44\\n\\nJ\\n\\n@\\n\\n.84\\n\\n.76\\n\\nK\\n\\n.42\\n\\n.33\\n\\n.55\\n\\nL\\n\\n@\\n\\nA\\n\\n.47\\n\\n.28\\n\\nB\\n\\n.72\\n\\n.75\\n\\nC\\n\\n.95\\n\\n.71\\n\\nD\\n\\n.88\\n\\n.78\\n\\n@\\n@\\n@\\n\\nE\\n\\n.31\\n\\n.62\\n\\nF\\n\\n.25\\n\\nG\\n\\n.55\\n\\n.60 1.05\\n\\nG\\nB\\n.66\\n\\n.71\\n\\nG\\nG\\n.56\\n.51\\n.48\\n.38\\n.18\\n\\nFig.', '5.', 'The concentrator assignment cost matrix with choices circled.', 'Since this choice of parameters results in a T matrix that is symmetric\\nand whose diagonal entries are all zeros, the network will converge to the\\nminima of Eq.', '3.', 'Furthermore, inclusion of the term which is weighted by the\\nparameter D causes the network to favor minimum cost assignments.', 'To evaluate the performance of the simulated network, an exhaustive\\nsearch of all solutions to the problem was conducted using a backtracking algorithm.', 'A frequency distribution of the solution costs associated with the assignments generated by the exhaustive search is shown in Fig.', '6.', 'For comparison,\\na histogram of the results of one hundred consecutive runs of the neural-net\\nsimulation is shown in Fig.', '7.', 'Although the neural-net simulation did not find\\na global minimum, ninety-two of the one hundred assignments which it did\\nfind were among the best 0.01 % of all solutions and the remaining eight were\\namong the best 0.3%.', 'CONCLUSION\\nNeural networks can be used to find good, though not necessarily optimal, solutions to combinatorial optimization problems like the concentrator\\n\\n\\x0c782\\n\\nFrequency\\n\\nFrequency\\n\\n25\\n4000000\\n3500000\\n3000000\\n250000\\n\\n20\\n15\\n10\\n\\n1500000\\n100000\\n500000\\n\\n5\\n\\nOL----\\n\\n3.2\\n\\n4.2\\n\\n5.2\\n\\n6.2\\n\\n7.2\\n\\nCost\\n\\no\\n\\n8.2\\n\\nFig.', '6.', 'Distribution of assignment Fig.', '7.', 'Distribution of assignment\\ncosts resulting from an exhaustive costs resulting from 100 consecutive executions of the neural net\\nsearch of all possible solutions.', 'simulation.', 'assignment problem.', 'In order to use a neural network to solve such problems,\\nit is necessary to be able to represent a solution to the problem as a state of the\\nnetwork.', 'Here the concentrator assignment problem was successfully mapped\\nonto a Hopfield network by associating each neuron with the hypothesis that a\\ngiven site should be assigned to a particular concentrator.', 'An energy function\\nwas constructed to determine the connections that were needed and the resulting neural network was simulated.', 'While the neural network solution to the concentrator assignment problem did not find a globally minimum cost assignment, it very effectively rejected poor solutions.', 'The network was even able to suggest assignments which\\nwould allow concentrators to be removed from the communication network.', 'REFERENCES\\n1.', 'A. S. Tanenbaum, Computer Networks (Prentice-Hall: Englewood Cliffs,\\n\\nNew Jersey, 1981), p. 83.', '2.', 'E. Feldman, F. A. Lehner and T. L. Ray, Manag.', 'Sci.', 'V12, 670 (1966).', '3.', 'A. Kuehn and M. Hamburger, Manag.', 'Sci.', 'V9, 643 (1966).', '4.', 'T. Aykin and A.', '1.', 'G. Babu, 1. of the Oper.', 'Res.', 'Soc.', 'V38, N3, 241 (1987).', '5.', 'J.', '1.', 'Hopfield, Proc.', 'Natl.', 'Acad.', 'Sci.', 'U. S. A., V79, 2554 (1982).', '6.', 'J.', '1.', 'Hopfield and D. W. Tank, Bio.', 'Cyber.', 'V52, 141 (1985) .', '7.', 'D. W. Tank and 1.', '1.', 'Hopfield, IEEE Trans.', 'on Cir.', 'and Sys.', 'CAS-33, N5,\\n533 (1986).', '\\nEXPERIMENTAL DEMONSTRATIONS OF\\nOPTICAL NEURAL COMPUTERS\\nKen Hsu, David Brady, and Demetri Psaltis\\nDepartment of Electrical Engineering\\nCalifornia Institute of Technology\\nPasadena, CA 91125\\n\\nABSTRACT\\nWe describe two expriments in optical neural computing.', 'In the first\\na closed optical feedback loop is used to implement auto-associative image\\nrecall.', 'In the second a perceptron-Iike learning algorithm is implemented with\\nphotorefractive holography.', 'INTRODUCTION\\nThe hardware needs of many neural computing systems are well matched\\nwith the capabilities of optical systems l ,2,3.', 'The high interconnectivity\\nrequired by neural computers can be simply implemented in optics because\\nchannels for optical signals may be superimposed in three dimensions with\\nlittle or no cross coupling.', 'Since these channels may be formed holographically,\\noptical neural systems can be designed to create and maintain interconnections\\nvery simply.', 'Thus the optical system designer can to a large extent\\navoid the analytical and topological problems of determining individual\\ninterconnections for a given neural system and constructing physical paths\\nfor these interconnections.', 'An archetypical design for a single layer of an optical neural computer is\\nshown in Fig.', '1.', 'Nonlinear thresholding elements, neurons, are arranged on\\ntwo dimensional planes which are interconnected via the third dimension by\\nholographic elements.', 'The key concerns in implementing this design involve\\nthe need for suitable nonlinearities for the neural planes and high capacity,\\neasily modifiable holographic elements.', 'While it is possible to implement the\\nneural function using entirely optical nonlinearities, for example using etalon\\narrays\\\\ optoelectronic two dimensional spatial light modulators (2D SLMs)\\nsuitable for this purpose are more readily available.', 'and their properties,\\ni.e.', 'speed and resolution, are well matched with the requirements of neural\\ncomputation and the limitations imposed on the system by the holographic\\ninterconnections 5 ,6.', 'Just as the main advantage of optics in connectionist\\nmachines is the fact that an optical system is generally linear and thus\\nallows the superposition of connections, the main disadvantage of optics is\\nthat good optical nonlinearities are hard to obtain.', 'Thus most SLMs are\\noptoelectronic with a non-linearity mediated by electronic effects.', 'The need for\\noptical nonlinearities arises again when we consider the formation of modifiable\\noptical interconnections, which must be an all optical process.', 'In selecting\\n\\n@\\n\\nAmerican Institute of Physics 1988\\n\\n\\x0c378\\n\\na holographic material for a neural computing application we would like to\\nhave the capability of real-time recording and slow erasure.', 'Materials such\\nas photographic film can provide this only with an impractical fixing process.', 'Photorefractive crystals are nonlinear optical materials that promise to have\\na relatively fast recording response and long term memory4,5,6,7,B.', \"'.\", \"'.\", '.\\n\\n\"', \".....\\n\\n..\\n\\n'.\", '..', \".'\", '- ~ :-w:-=7 -~---\\n\\n\" .', \"'.\", '......\\n\\n.', \"'.\", 'Fourier\\nlens\\n\\nhologro.phlc I\"IealuI\"I\\n\\nFourier\\nlens\\n\\nFigure 1.', 'Optical neural computer architecture.', 'In this paper we describe two experimental implementations of optical\\nneural computers which demonstrate how currently available optical devices\\nmay be used in this application.', 'The first experiment we describe involves an\\noptical associative loop which uses feedback through a neural plane in the form\\nof a pinhole array and a separate thresholding plane to implement associate\\nregeneration of stored patterns from correlated inputs.', 'This experiment\\ndemonstrates the input-output dynamics of an optical neural computer similar\\nto that shown in Fig.', '1, implemented using the Hughes Liquid Crystal Light\\nValve.', 'The second experiment we describe is a single neuron optical perceptron\\nimplemented with a photorefractive crystal.', 'This experiment demonstrates\\nhow the learning dynamics of long term memory may be controlled optically.', 'By combining these two experiments we should eventually be able to construct\\nhigh capacity adaptive optical neural computers.', 'OPTICAL ASSOCIATIVE LOOP\\nA schematic diagram of the optical associative memory loop is shown in\\nFig.', '2.', 'It is comprised of two cascaded Vander Lugt correlators9.', 'The input\\nsection of the system from the threshold device P1 through the first hologram\\nP2 to the pinhole array P3 forms the first correlator.', 'The feedback section\\nfrom P3 through the second hologram P4 back to the threshold device P1\\nforms the second correlator.', 'An array of pinholes sits on the back focal plane\\nof L2, which coincides with the front focal plane of L3.', 'The purpose of the\\npinholes is to link the first and the second (reversed) correlator to form a closed\\noptical feedback loop 10.', 'There are two phases in operating this optical loop, the learning phase\\nand the recal phase.', 'In the learning phase, the images to be stored are\\nspatially multiplexed and entered simultaneously on the threshold device.', 'The\\n\\n\\x0c379\\n\\nthresholded images are Fourier transformed by the lens Ll.', 'The Fourier\\nspectrum and a plane wave reference beam interfere at the plane P2 and\\nrecord a Fourier transform hologram.', 'This hologram is moved to plane P4\\nas our stored memory.', 'We then reconstruct the images from the memory to\\nform a new input to make a second Fourier transform hologram that will stay\\nat plane P2.', 'This completes the\\nlearning phase.', 'In the recalling phase\\nan input is imaged on the threshold Input\\n~~~*+++~~~~\\ndevice.', 'This image is correlated with\\nthe reference images in the hologram\\nat P2.', 'If the correlation between the\\ninput and one of the stored images is\\nhigh a bright peak appears at one of\\nthe pinholes.', 'This peak is sampled by\\n~ -,.....,.- Second\\nPinhole\\nHologram\\nArray - -.... L z\\nthe pinhole to reconstruct the stored\\nI\\nI\\nimage from the hologram at P4.', 'The\\nreconstructed beam is then imaged\\nback to the threshold device to form a\\nclosed loop.', 'If the overall optical gain Figure.', '2.', 'All-optical associative\\nin the loop exceeds the loss the loop loop.', 'The threshold device is a LCLV,\\nsignal will grow until the threshold and the holograms are thermoplastic\\ndevice is saturated.', 'In this case, we plates.', 'can cutoff the external input image\\nand the optical loop will be latched at\\nthe stable memory.', 'The key elements in this optical loop are the holograms, the pinhole array,\\nand the threshold device.', 'If we put a mirror 10 or a phase conjugate mirror 7 ,11\\nat the pinhole plane P3 to reflect the correlation signal back through the\\nsystem then we only need one hologram to form a closed loop.', 'The use of two\\nholograms, however, improves system performance.', 'We make the hologram at\\nP2 with a high pass characteristic so that the input section of the loop has\\nhigh spectral discrimination.', 'On the other hand we want the images to be\\nreconstructed with high fidelity to the original images.', 'Thus the hologram at\\nplane P4 must have broadband characteristics.', 'We use a diffuser to achieve\\nthis when making this hologram.', 'Fig.', '3a shows the original images.', 'Fig.', '3b\\nand Fig.', '3c are the images reconstructed from first and second holograms,\\nrespectively.', 'As desired, Fig.', '3b is a high pass version of the stored image\\nwhile Fig.', '3c is broadband .', 'Each of the pinholes at the correlation plane P3 has a diameter of 60\\nj.lm.', 'The separations between the pinholes correspond to the separations of\\nthe input images at plane P 1.', 'If one of the stored images appears at P 1 there\\nwill be a bright spot at the corresponding pinhole on plane P3.', 'If the input\\nimage shifts to the position of another image the correlation peak will also\\n\\n\\x0c380\\n\\n,.', '~\\n?..', \"?\\n\\n'\", '.a:..J\\n\\n~\\n\\na.\\n\\n.', ', .\\'\"', '.', '~\\n\\n~.', \"(\\n\\ni\\n\\n\\\\~ .~\\n\\n-y::'\\n.\", '..\\n\\n?Il,...', \".'\", '.r\\n\\nI\\n\\nK~?', \"';t\\n\\nL\\n?\", '?', '?', '.#\\n\\nb.\\n\\nc.\\n\\nFigure 3.', '(a) The original images.', '(b)The reconstructed images from the highpass hologram P2.', '(c) The reconstructed images from the band-pass hologram\\n\\nP4.', 'shift to another pinhole.', 'But if the shift is not an exact image spacing the\\ncorrelation peak can not pass the pinhole and we lose the feedback signal.', 'Therefore this is a loop with \"discrete\" shift invariance.', \"Without the pinholes\\nthe cross-correlation noise and the auto-correlation peak will be fed back to\\nthe loop together and the reconstructed images won't be recognizable.\", 'There\\nis a compromise between the pinhole size and the loop performance.', 'Small\\npinholes allow good memory discrimination and sharp reconstructed images,\\nbut can cut the signal to below the level that can be detected by the threshold\\ndevice and reduce the tolerance of the system to shifts in the input.', 'The\\nfunction of the pinhole array in this system might also be met by a nonlinear\\nspatial light modulator, in which case we can achieve full shift invariance 12 ?', 'The threshold device at plane PI is a Hughes Liquid Crystal Light Valve.', 'The device has a resolution of 16 Ip/mm and uniform aperture of 1 inch\\ndiameter.', 'This gives us about 160,000 neurons at PI.', 'In order to compensate\\nfor the optical loss in the loop, which is on the order of 10- 5 , we need the\\nneurons to provide gain on the order of 105.', 'In our system this is achieved\\nby placing a Hamamatsu image intensifier at the write side of the LCLV.', 'Since the microchannel plate of the image intensifier can give gains of 104 , the\\ncombination of the LCLV and the image intensifier can give gains of 10 6 with\\nsensitivity down to n W /cm 2 .', 'The optical gain in the loop can be adjusted by\\nchanging the gain of the image intensifier.', 'Since the activity of neurons and the dynamics of the memory loop is\\na continuously evolving phenomenon, we need to have a real time device to\\nmonitor and record this behavior.', 'We do this by using a prism beam splitter\\nto take part of the read out beam from the LCLV and image it onto a CCD\\ncamera.', 'The output is displayed on a CRT monitor and also recorded on a\\nvideo tape recorder.', 'Unfortunately, in a paper we can only show static pictures\\ntaken from the screen.', 'We put a window at the CCD plane so that each time\\nwe can pick up one of the stored images.', 'Fig.', '4a shows the read out image\\n\\n\\x0c381\\n\\na.\\n\\nb.\\n\\nc.\\n\\nFigure 4.', '(a) The external input to the optical loop.', '(b) The feedback image\\nsuperimposed with the input image.', '(c) The latched loop image.', 'from the LCLV which comes from the external input shifted away from its\\nstored position.', 'This shift moves its correlation peak so that it does not match\\nthe position of the pinhole.', 'Thus there is no feedback signal going through\\nthe loop.', 'If we cut off the input image the read out image will die out with a\\ncharacteristic time on the order of 50 to 100 ms, corresponding to the response\\ntime of the LCLV.', 'Now we shift the input image around trying to search for\\nthe correct position.', 'Once the input image comes close enough to the correct\\nposition the correlation peak passes through the right pinhole, giving a strong\\nfeedback signal superimposed with the external input on the neurons.', 'The\\ntotal signal then goes through the feedback loop and is amplified continuously\\nuntil the neurons are saturated.', 'Depending on the optical gain of the neurons\\nthe time required for the loop to reach a stable state is between 100 ms and\\nseveral seconds.', 'Fig.', '4b shows the superimposed images of the external input\\nand the loop images.', 'While the feedback signal is shifted somewhat with\\nrespect to the input, there is sufficient correlation to induce recall.', 'If the\\nneurons have enough gain then we can cut off the input and the loop stays in\\nits stable state.', 'Otherwise we have to increase the neuron gain until the loop\\ncan sustain itself.', 'Fig.', '4c shows the image in the loop with the input removed\\nand the memory latched.', 'If we enter another image into the system, again\\nwe have to shift the input within the window to search the memory until we\\nare close enough to the correct position.', 'Then the loop will evolve to another\\nstable state and give a correct output.', 'The input images do not need to match exactly with the memory.', 'Since\\nthe neurons can sense and amplify the feedback signal produced by a partial\\nmatch between the input and a stored image, the stored memory can grow\\nin the loop.', 'Thus the loop has the capability to recall the complete memory\\nfrom a partial input.', 'Fig.', '5a shows the image of a half face input into the\\nsystem.', 'Fig.', '5b shows the overlap of the input with the complete face from\\nthe memory.', 'Fig.', '5c shows the stable state of the loop after we cut off the\\nexternal input.', 'In order to have this associative behavior the input must have\\nenough correlation with the stored memory to yield a strong feedback signal.', 'For instance, the loop does not respond to the the presentation of a picture of\\n\\n\\x0c382\\n\\na.\\n\\nc.\\n\\nFigure 5.', '(a) Partial face used as the external input.', '(b) The superimposed\\nimages of the partial input with the complete face recalled by the loop.', '(c)\\nThe complete face latched in the loop.', 'a.\\n\\nb.\\n\\nc.\\n\\nFigure 6.', '(a) Rotated image used as the external input.', '(b) The superimposed\\nimages of the input with the recalled image from the loop.', '(c) The image\\nlatched in the optical loop.', 'a person not stored in memory.', 'Another way to demonstrate the associative behavior of the loop is to use\\na rotated image as the input.', 'Experiments show that for a small rotation the\\nloop can recognize the image very quickly.', 'As the input is rotated more, it\\ntakes longer for the loop to reach a stable state.', \"If it is rotated too much,\\ndepending on the neuron gain, the input won't be recognizable.\", 'Fig.', '6a shows\\nthe rotated input.', 'Fig.', '6b shows the overlap of loop image with input after\\nwe turn on the loop for several seconds.', 'Fig.', '6c shows the correct memory\\nrecalled from the loop after we cut the input.', 'There is a trade-off between the\\ndegree of distortion at the input that the system can tolerate and its ability\\nto discriminate against patterns it has not seen before.', 'In this system the\\nfeedback gain (which can be adjusted through the image intensifier) controls\\nthis trade-off.', 'PHOTOREFRACTIVE PERCEPTRON\\nHolograms are recorded in photorefractive crystals via the electrooptic\\nmodulation of the index of refraction by space charge fields created by\\nthe migration of photogenerated charge 13 ,14.', 'Photorefractive crystals are\\nattractive for optical neural applications because they may be used to store\\n\\n\\x0c383\\n\\nlong term interactions between a very large number of neurons.', 'While\\nphotorefractive recording does not require a development step, the fact that\\nthe response is not instantaneous allows the crystal to store long term traces\\nof the learning process.', 'Since the photorefractive effect arises from the\\nreversible redistribution of a fixed pool of charge among a fixed set of optically\\naddressable trapping sites, the photorefractive response of a crystal does not\\ndeteriorate with exposure.', 'Finally, the fact that photorefractive holograms\\nmay extend over the entire volume of the crystal has previously been shown to\\nimply that as many as 10 10 interconnections may be stored in a single crystal\\nwith the independence of each interconnection guaranteed by an appropriate\\nspatial arrangement of the interconnected neurons 6 ,5.', 'In this section we consider a rudimentary optical neural system which uses\\nthe dynamics of photorefractive crystals to implement perceptron-like learning.', 'The architecture of this system is shown schematically in Fig.', '7.', 'The input\\nto the system, x, corresponds to a two dimensional pattern recorded from a\\nvideo monitor onto a liquid crystal light valve.', 'The light valve transfers this\\npattern on a laser beam.', 'This beam is split into two paths which cross in a\\nphotorefractive crystal.', 'The light propagating along each path is focused such\\nthat an image of the input pattern is formed on the crystal.', 'The images along\\nboth paths are of the same size and are superposed on the crystal, which is\\nassumed to be thinner than the depth of focus of the images.', 'The intensity\\ndiffracted from one of the two paths onto the other by a hologram stored in\\nthe crystal is isolated by a polarizer and spatially integrated by a single output\\ndetector.', 'The thresholded output of this detector corresponds to the output\\nof a neuron in a perceptron.', 'laser\\n\\n~---,t+\\n\\n-\\n\\nPB\\n\\nLCL V TV\\n\\n--f4HJ\\nucl\\n\\nBS$- -\\n\\nCOl\"lputer\\n\\nXtal\\n\\nPM\\n\\nFigure 7.', 'Photorefractive perceptron.', 'PB is a polarizing beam splitter.', 'Ll\\nand L2 are imaging lenses.', 'WP is a quarter waveplate.', 'PM is a piezoelectric\\nmirror.', 'P is a polarizer.', 'D is a detector.', 'Solid lines show electronic control.', 'Dashed lines show the optical path.', 'The ith component of the input to this system corresponds to the intensity\\nin the ith pixel of the input pattern.', 'The interconnection strength, Wi, between\\nthe ith input and the output neuron corresponds to the diffraction efficiency\\nof the hologram taking one path into the other at the ith pixel of the image\\nplane.', 'While the dynamics of Wi can be quite complex in some geometries\\n\\n\\x0c384\\n\\nand crystals, it is possible to show from the band transport model for the\\nphotorefractive effect that under certain circumstances the time development\\nof Wi may be modeled by\\n\\n(1)\\nwhere m(s) and 4>(s) are the modulation depth and phase, respectively, of the\\ninterference pattern formed in the crystal between the light in the two paths 15 ?', 'T is a characteristic time constant for crystal.', 'T is inversely proportional to\\nthe intensity incident on the ith pixel of the crystal.', 'Using Eqn.', '1 it is possible\\nto make Wi(t) take any value between 0 and W m l1Z by properly exposing the\\nith pixel of the crystal to an appropriate modulation depth and intensity.', 'The\\nmodulation depth between two optical beams can be adjusted by a variety of\\nsimple mechanisms.', 'In Fig.', '7 we choose to control met) using a mirror mounted\\non a piezoelectric crystal.', 'By varying the frequency and the amplitude of\\noscillations in the piezoelectric crystal we can electronically set both met) and\\n4>(t) over a continuous range without changing the intensity in the optical\\nbeams or interrupting readout of the system.', 'With this control over met) it\\nis possible via the dynamics described in Eqn.', '(1) to implement any learning\\nalgorithm for which Wi can be limited to the range (0, w maz ).', 'The architecture of Fig.', '7 classifies input patterns into two classes\\naccording to the thresholded output of the detector.', 'The goal of a learning\\nalgorithm for this system is to correctly classify a set of training patterns.', 'The\\nperceptron learning algorithm involves simply testing each training vector and\\nadding training vectors which yield too Iowan output to the weight vector\\nand subtracting training vectors which yield too high an output from the\\nweight vector until all training vectors are correctly classified 16.', 'This training\\nalgorithm is described by the equation L\\\\wi = aXj where alpha is positive\\n(negative) if the output for x is too low (high).', 'An optical analog of this\\nmethod is implemented by testing each training pattern and exposing the\\ncrystal with each incorrectly classified pattern.', 'Training vectors that yield\\na high output when a low output is desired are exposed at zero modulation\\ndepth .', 'Training vectors that yield a low output when high output is desired\\nare exposed at a modulation depth of one.', 'The weight vector for the k + 1th iteration when erasure occurs in the kth\\niteration is given by\\n\\n(2)\\nwhere we assume that the exposure time, L\\\\t, is much less than T. Note that\\nsince T is inversely proportional to the intensity in the ith pixel, the change in\\n\\n\\x0c385\\n\\nWi is proportional to the ith input.', 'The weight vector at the k + 1th iteration\\nwhen recording occurs in the kth iteration is given by\\n-2~t\\n\\n-~t\\n\\n_ /\\n\\n-~t\\n\\n-~t\\n\\nwi(k+ 1) = e-r-Wi(k) +2y Wi(k)Wmcue-r- (l-e-r- ) +wmaz(l-e-r-)\\nTo lowest order in\\n\\n6.t\\n.,.', '2\\n\\n(3)\\n\\nand ~,\\nEqn.', '(3) yields\\nw m ....\\n\\n_/\\n~t\\n~t 2\\nwi(k + 1) = wi(k) + 2y wi(k)Wmaz(-) + Wmaz(-)\\nT\\n\\nT\\n\\n(4)\\n\\nOnce again the change in Wi is proportional to the ith input.', 'We have implemented the architecture of Fig.', '7 using a SBN60:Ce crystal\\nprovided by the Rockwell International Science Center.', 'We used the 488 nm\\nline of an argon ion laser to record holograms in this crystal.', 'Most of the\\npatterns we considered were laid out on 10 x 10 grids of pixels, thus allowing\\n100 input channels.', 'Ultimately, the number of channels which may be achieved\\nusing this architecture is limited by the number of pixels which may be imaged\\nonto the crystal with a depth of focus sufficient to isolate each pixel along the\\nlength of the crystal.', '-\\n\\n??', '+.+\\nY\\n\\n....... ?', '?', '?', '1\\n\\n3\\n\\n2\\n\\n..... ?', '?', '4\\n\\nFigure 8.', 'Training patterns.', \"...\\n\\n1'1\\n\\nj\\n\\nIa.\", '8.\\n\\n!', 'l\\n\\nt\\n\\nI\\n,\\n\\n0\\n\\n0\\naCOftClS\\n\\n~\\n\\nW\\n\\nCIII)\\n\\nFigure 9.', 'Output in the second training cycle.', 'Using the variation on the perceptron learning algorithm described above\\nwith a fixed exposure times ~tr and ~te for recording and erasing, we have\\nbeen able to correctly classify various sets of input patterns.', 'One particular\\nset which we used is shown in Fig.', '8.', 'In one training sequence, we grouped\\npatterns 1 and 2 together with a high output and patterns 3 and 4 together\\nwith a low output.', 'After all four patterns had been presented four times,\\nthe system gave the correct output for all patterns.', 'The weights stored in\\nthe crystal were corrected seven times, four times by recording and three by\\nerasing.', 'Fig .', '9a shows the output of the detector as pattern 1 is recorded in\\nthe second learning cycle.', 'The dashed line in this figure corresponds to the\\nthreshold level.', 'Fig.', '9b shows the output of the detector as pattern 3 is erased\\nin the second learning cycle.', '386\\n\\nCONCLUSION\\nThe experiments described in this paper demonstrate how neural network\\narchitectures can be implemented using currently available optical devices.', 'By\\ncombining the recall dynamics of the first system with the learning capability\\nof the second, we can construct sophisticated optical neural computers.', 'ACKNOWLEDGEMENTS\\nThe authors thank Ratnakar Neurgaonkar and Rockwell International for\\nsupplying the SBN crystal used in our experiments and Hamamatsu Photonics\\nK.K.', 'for assistance with image intesifiers.', 'We also thank Eung Gi Paek and\\nKelvin Wagner for their contributions to this research.', 'This research is supported by the Defense Advanced Research Projects\\nAgency, the Army Research Office, and the Air Force Office of Scientific\\nResearch.', 'REFERENCES\\n1.', 'Y. S. Abu-Mostafa and D. Psaltis, Scientific American, pp.88-95, March,\\n\\n2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '1987.', 'D. Psaltis and N. H. Farhat, Opt.', 'Lett., 10,(2),98(1985).', 'A. D. Fisher, R. C. Fukuda, and J. N. Lee, Proc.', 'SPIE 625, 196(1986).', 'K. Wagner and D. Psaltis, Appl.', 'opt., 26(23), pp.5061-5076(1987).', 'D. Psaltis, D. Brady, and K. Wagner, Applied optics, March 1988.', 'D. Psaltis, J. Yu, X. G. Gu, and H. Lee, Second Topical Meeting on\\nOptical Computing, Incline Village, Nevada, March 16-18,1987.', 'A. Yariv, S.-K. Kwong, and K. Kyuma, SPIE proc.', '613-01,(1986).', 'D. Z. Anderson, Proceedings of the International Conference on Neural\\nNetworks, San Diego, June 1987.', 'A.', 'B. Vander Lugt, IEEE Trans.', 'Inform.', 'Theory, IT-I0(2), pp.139145(1964).', 'E. G. Paek and D. Psaltis, Opt.', 'Eng., 26(5), pp.428-433(1987).', 'Y. Owechko, G. J. Dunning, E. Marom, and B. H. Soffer, Appl.', 'Opt.', '26,(10) ,1900(1987).', 'D. Psaltis and J. Hong, Opt.', 'Eng.', '26,10(1987).', 'N. V. Kuktarev, V. B. Markov, S. G. Odulov, M. S. Soskin, and V. L.\\nVinetskii, Ferroelectrics, 22,949(1979).', 'J. Feinberg, D. Heiman, A. R. Tanguay, and R. W. Hellwarth, J. Appl.', 'Phys.', '51,1297(1980).', 'T. J.', 'Hall, R. Jaura, L. M. Connors, P. D. Foote, Prog.', 'Quan.', 'Electr.', '10,77(1985).', \"F. Rosenblatt, ' Principles of Neurodynamics: Perceptron and the Theory\\nof Brain Mechanisms, Spartan Books, Washington,(1961).\", \"\\nSupervised Learning of Probability Distributions\\nby Neural Networks\\nEric B. Baum\\nJet Propulsion Laboratory, Pasadena CA 91109\\nFrank Wilczek t\\nDepartment of Physics,Harvard University,Cambridge MA 02138\\n\\nAbstract:\\nWe propose that the back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual\\nfooting, and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying\\nthe synaptic weights in the gradient direction of the log likelihood,\\nrather than the 'error'.\", \"In the past thirty years many researchers have studied the\\nquestion of supervised learning in 'neural'-like networks.\", \"Recently\\na learning algorithm called 'back propagation H -\\n\\n4\\n\\nor the 'general-\\n\\nized delta-rule' has been applied to numerous problems including\\nthe mapping of text to phonemes 5 , the diagnosis of illnesses 6 and\\nthe classification of sonar targets 7 ?\", 'In these applications, it would\\noften be natural to consider imperfect, or probabilistic information.', 'We believe that by considering supervised learning from this\\nslightly larger perspective, one can not only place back propagat Permanent address: Institute for Theoretical Physics, Univer-\\n\\nsity of California, Santa Barbara CA 93106\\n?', 'American Institute of Physics 1988\\n\\n\\x0c53\\n\\ntion on a more rigorous and general basis, relating it to other well\\nstudied pattern recognition algorithms, but very likely improve its\\nperformance as well.', 'The problem of supervised learning is to model some mapping\\nbetween input vectors and output vectors presented to us by some\\nreal world phenomena.', 'To be specific, coqsider the question of\\nmedical diagnosis.', 'The input vector corresponds to the symptoms\\nof the patient; the i-th component is defined to be 1 if symptom i\\nis present and 0 if symptom i is absent.', 'The output vector corresponds to the illnesses, so that its j-th component is 1 if the j-th\\nillness is present and 0 otherwise.', 'Given a data base consisting\\nof a number of diagnosed cases, the goal is to construct (learn) a\\nmapping which accounts for these examples and can be applied to\\ndiagnose new patients in a reliable way.', 'One could hope, for instance, that such a learning algorithm might yield an expert system\\nto simulate the performance of doctors.', \"Little expert advice would\\nbe required for its design, which is advantageous both because experts' time is valuable and because experts often have extraodinary\\ndifficulty in describing how they make decisions.\", 'A feedforward neural network implements such a mapping between input vectors and output vectors.', 'Such a network has a set\\nof input nodes, one or several layers of intermediate nodes, and a\\nlayer of output nodes.', 'The nodes are connected in a forward directed manner, so that the output of a node may be connected to\\nthe inputs of nodes in subsequent layers, but closed loops do not\\noccur.', 'See figure 1.', 'The output of each node is assumed to be a\\nbounded semilinear function of its inputs.', 'That is, if\\nthe output of the j-th node and\\n\\nWij\\n\\nVj\\n\\ndenotes\\n\\ndenotes the weight associated\\n\\nwith the connection of the output of the j-th node to the input of\\n\\n\\x0c54\\n\\nthe i-th, then the i-th neuron takes value Vi = g(L,i Wi:jV:j), where\\ng is a bounded, differentiable function called the activation function.', 'g(x)\\n\\n= 1/(1 + e- X ),\\n\\ncalled the logistic function, is frequently\\n\\nused.', 'Given a fixed set of weights {Wi:j}, we set the input node\\nvalues to equal some input vector, compute the value of the nodes\\nlayer by layer until we compute the output nodes, and so generate\\nan output vector.', 'Figure 1: A 5 layer network.', 'Note bottleneck at layer 3.', '55\\n\\nSuch networks have been studied because of analogies to neurobiology, because it may be easy to fabricate them in hardware,\\nand because learning algorithms such as the Perceptron learning\\nalgorithm 8 , Widrow- Hoff9, and backpropagation have been able\\nto choose weights\\n\\nWi,.', 'that solve interesting problems.', 'Given a set of input vectors\\nvalues\\n\\ntj,\\n\\nsr, together with associated target\\n\\nback propagation attempts to adjust the weights so as\\n\\nto minimize the error E in achieving these target values, defined as\\n\\nE\\n\\n= E EJL = E(tj - oj)2\\nJL\\n\\nwhere\\n\\noj\\n\\ninput.', '(1)\\n\\nJL,i\\n\\nis the output of the j-th node when sJL is presented as\\n\\nBack propagation starts with randomly chosen\\n\\nWi,.', 'and\\n\\nthen varies in the gradient direction of E until a local minimum\\nis obtained.', 'Although only a locally optimal set of weights is obtained, in a number of experiments the neural net so generated\\nhas performed surprisingly well not only on the training set but on\\nsubsequent data.', '4 -\\n\\n6\\n\\nThis performance is probably the main reason\\n\\nfor widespread interest in backpropagation.', 'It seems to us natural, in the context of the medical diagnosis\\npro blem, the other real world problems to which backpropagation\\nhas been applied, and indeed in any mapping problem where one\\ndesires to generalize from a limited and noisy set of examples, to\\ninterpret the output vector in probabilistic terms.', 'Such an interpretation is standard in the literature on pattern classification.', '1o\\nIndeed, the examples might even be probabilistic themselves.', 'That\\nis to say it might not be certain whether symptom i was present\\nin case /L or not.', 'Let\\n\\nsr represent the probability symptom i is present in case\\n\\n/L, and let\\n\\ntj\\n\\nrepresent the probability disease j ocurred in case\\n\\n\\x0c56\\n\\nfL.', 'Consider for the moment the case where the\\n\\ntJ\\n\\nare 1 or 0,\\nA\\n\\nso that the cases are in fact fully diagnosed.', 'Let\\n\\nIi (s, 0)\\n\\nbe our\\n\\nprediction of the probability of disease i given input vector 5, where\\n{; is some set of parameters determined by our learning algorithm.', \"In the neural network case, the {; are the connection weights and\\n\\nIi (sl' , {Wi.i })\\n\\n=\\n\\noJ.\", 'Now lacking a priori knowledge of good\\n\\n0, the best one can do\\n\\nis to choose the parameters {; to maximize the likelihood that the\\ngiven set of examples should have occurred.', '10 The formula for this\\nlikelihood, p, is immediate:\\n\\nor\\n\\nThe extension of equation (2), and thus equation (3) to the\\ncase where the f are probabilities, taking values in [0,1]\\' is straight-\\n\\n\\x0c57\\n\\nforward * 1 and yields\\n\\nlog(p) =\\n\\n~ [tjlog(Jj (s\", 0)) + (1 -\\n\\ntj)log(1 - Ij (W, 0))]\\n\\n(4)\\n\\np. ,3\\n\\nExpressions of this sort often arise in physics and information theory and are generally interpreted as an entropy.', '11\\nWe may now vary the {O} in the gradient direction of the entropy.', \"The back propagation algorithm generalizes immediately\\nfrom minimizing 'Error' or 'Energy' to maximizing entropy or log\\nlikelihood, or indeed any other function of the outputs and the\\ninputs 12 .\", 'Of course it remains true that the gradient can be computed by back propagation with essentially the same number of\\ncomputations as are required to compute the output of the network.', 'A backpropagation algorithm based on log-likelihood is not\\nonly more intuitively appealing than one based on an ad-hoc definition of error, but will make quite different and more accurate\\npredictions as well.', 'Consider e.g.', 'training the net on an example which it already understands fairly well.', \"/j(80)\\n\\n=L\\n\\nNow, from eqn(l) BE/B/j\\n\\nSay\\n\\ntj\\n\\n= 2?, so using\\n\\n= 0, and\\n\\n'Error' as a\\n\\n* 1 We may see this by constructing an equivalent larger set of\\nexamples with the f taking only values 0 or 1 with the appropriate\\nfrequency.\", 'Thus assume the\\n\\ntj\\n\\nare rational numbers with denomi-\\n\\nnator dj and numerator nj and let p\\n\\n= IIp.,j dj.', 'What we mean by\\n\\nthe set of examples {tp.', ': J-t = 1, ... , M} can be represented by con-\\n\\nij = 0\\nfor p(J-t- 1) < v < pJ-t and 1 < vmod(dj) < (dj - nj), and ij = 1\\n\\nsidering a set of N\\n\\n= Mp examples {ij}\\n\\nwhere for each J-t,\\n\\notherwise.', 'N ow applying equation (3) gives equation (4), up to an\\noverall normalization.', '58\\n\\ncriterion the net learns very little from this example, whereas, using eqn(3), Blog(p)/B!', ';j\\n\\n= 1/(1 -\\n\\nf), so the net continues to learn\\n\\nand can in fact converge to predict probabilities near 1.', \"Indeed\\nbecause back propagation using the standard 'Error' measure can\\nnot converge to generate outputs of 1 or 0, it has been customary in the literature 4 to round the target values so that a target\\nof 1 would be presented in the learning algorithm as some ad hoc\\nnumber such as .8, whereas a target of 0 would be presented as .2.\", 'In the context of our general discussion it is natural to ask\\nwhether using a feedforward network and varying the weights is in\\nfact the most effective alternative.', 'Anderson and Abrahams 13 have\\ndiscussed this issue from a Bayesian viewpoint.', 'From this point of\\nview, fitting output to input using normal distributions and varying\\nthe means and covariance matrix may seem to be more logical.', 'Feedforward networks do however have several advantages for\\ncomplex problems.', 'Experience with neural networks has shown the\\nimportance of including hidden units wherein the network can form\\nan internal representation of the world.', 'If one simply uses normal\\ndistributions, any hidden variables included will simply integrate\\nout in calculating an output.', 'It will thus be necessary to include at\\nleast third order correlations to implement useful hidden variables.', 'Unfortunately, the number of possible third order correlations is\\nvery large, so that there may be practical obstacles to such an\\napproach.', 'Indeed it is well known folklore in curve fitting and\\npattern classification that the number of parameters must be small\\ncompared to the size of the data set if any generalization to future\\ncases is expected.', '10\\nIn feedforward nets the question takes a different form.', 'There\\ncan be bottlenecks to information flow.', 'Specifically, if the net is\\n\\n\\x0c59\\n\\nconstructed with an intermediate layer which is not bypassed by\\nany connections (i.e.', 'there are no connections from layers preceding\\nto layers subsequent), and if furthermore the activation functions\\nare chosen so that the values of each of the intermediate nodes\\ntend towards either 1 or 0*2, then this layer serves as a bottleneck\\nto information flow.', 'No matter how many input nodes, output\\nnodes, or free parameters there are in the net, the output will be\\nconstrained to take on no more than 21 different patterns, where\\nI is the number of nodes in the bottleneck layer.', \"Thus if I is\\n\\nsmall, some sort of 'generalization' must occur even if the number\\nof weights is large.\", 'One plausible reason for the success of back\\npropagation in adequately solving tasks, in spite of the fact that\\nit finds only local minima, is its ability to vary a large number of\\nparameters.', 'This freedom may allow back propagation to escape\\nfrom many putative traps and to find an acceptable solution.', 'A good expert system, say for medical diagnosis, should not\\nonly give a diagnosis based on the available information, but should\\nbe able to suggest, in questionable cases, which lab tests might be\\nperformed to clarify matters.', 'Actually back propagation inherently has such a capability.', 'Back propagation involves calculation\\nof 81og(p)/8wij.', 'This information allows one to compute immediately 81og(p)/8s j .', 'Those input nodes for which this partial derivative is large correspond to important experiments.', 'In conclusion, we propose that back propagation can be generalized, put on a satisfactory conceptual footing, and very likely\\nmade more efficient, by defining the values of the output and in*2\\n\\nAlternatively when necessary this can be enforced by adding\\n\\nan energy term to the log-likelihood to constrain the parameter\\nvariation so that the neuronal values are near either 1 or O.', \"60\\n\\nput neurons as probabilities, and replacing the 'Error' by the loglikelihood.\", 'Acknowledgement: E. B. Baum was supported in part by DARPA\\n\\nthrough arrangement with NASA and by NSF grant DMB-840649,\\n802.', 'F. Wilczek was supported in part by NSF grant PHY82-17853\\nReferences\\n\\n(1)Werbos,P, \"Beyond Regression: New Tools for Prediction and\\nAnalysis in the Behavioral Sciences\" , Harvard University Dissertation (1974)\\n(2)Parker D. B., \"Learning Logic\" ,MIT Tech Report TR-47, Center\\nfor Computationl Research in Economics and Management Science,\\nMIT, 1985\\n(3)Le Cun, Y., Proceedings of Cognitiva 85,p599-604, Paris (1985)\\n(4)Rumelhart, D. E., Hinton, G. E., Williams, G. E., \"Learning\\nInternal Representations by Error Propagation\", in \"Parallel Distributed Processing\" , vol 1, eds.', 'Rumelhart, D. E., McClelland, J.\\nL., MIT Press, Cambridge MA,( 1986)\\n(5)Sejnowski, T. J., Rosenberg, C. R., Complex Systems, v 1, pp\\n145-168 (1987)\\n(6)LeCun, Y., Address at 1987 Snowbird Conference on Neural\\nNetworks\\n(7)Gorman, P., Sejnowski, T. J., \"Learned Classification of Sonar\\nTargets Using a Massively Parallel Network\", in \"Workshop on\\nNeural Network Devices and Applications\", JPLD-4406, (1987)\\npp224-237\\n(8)Rosenblatt, F., \"Principles of Neurodynamics: Perceptrons and\\n\\n\\x0c61\\n\\nthe theory of brain mechanisms\", Spartan Books, Washington DC\\n(1962)\\n(9)Widrow, B., Hoff, M. E., 1960 IRE WESCON Cony.', 'Record,\\nPart 4, 96-104 (1960)\\n(10)Duda, R. 0., Hart, P. E., \"Pattern Classification and Scene\\nAnalysis\", John Wiley and Sons, N.Y., (1973)\\n(11)Guiasu, S., \"Information Theory with Applications\", McGraw\\nHill, NY, (1977)\\n(12)Baum,E.B., \"Generalizing Back Propagation to Computation\" ,\\nin \"Neural Networks for Computing\", AlP Conf.', 'Proc.', '151, Snowbird UT (1986)pp47-53\\n(13)Anderson, C.H., Abrahams, E., \"The Bayes Connection\" , Proceedings of the IEEE International Conference on Neural N etwor ks,\\nSan Diego,(1987)', '\\n\\nTHE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY IS EXPONENTIAL\\nP. A. Choul\\nStanford University.', 'Stanford.', 'CA 94305\\nABSTRACT\\nThe capacity of an associative memory is defined as the maximum\\nnumber of vords that can be stored and retrieved reliably by an address\\nvithin a given sphere of attraction.', 'It is shown by sphere packing\\narguments that as the address length increases.', 'the capacity of any\\nassociati ve memory is limited to an exponential grovth rate of 1 - h2 ( 0).', 'vhere h2(0) is the binary entropy function in bits.', 'and 0 is the radius\\nof the sphere of attraction.', 'This exponential grovth in capacity can\\nactually be achieved by the Kanerva associative memory.', 'if its\\nparameters are optimally set .', 'Formulas for these op.timal values are\\nprovided.', 'The exponential grovth in capacity for the Kanerva\\nassociative memory contrasts sharply vith the sub-linear grovth in\\ncapacity for the Hopfield associative memory.', 'ASSOCIATIVE MEMORY AND ITS CAPACITY\\nOur model of an associative memory is the folloving.', 'Let ()(,Y) be\\nan (address.', 'datum) pair.', 'vhere )( is a vector of n ?ls and Y is a\\nvector of m ?ls.', 'and let ()(l),y(I)), ... ,()(M) , y(M)).', 'be M (address,\\ndatum) pairs stored in an associative memory.', 'If the associative memory\\nis presented at the input vith an address )( that is close to some\\nstored address )(W. then it should produce at the output a vord Y that\\nis close to the corresponding contents y(j).', 'To be specific, let us say\\nthat an associative memory can correct fraction 0 errors if an )( vi thin\\nHamming distance no of )((j) retrieves Y equal to y(j).', 'The Hamming\\nsphere around each )(W vill be called the sphere of attraction, and 0\\nviII be called the radius of attraction.', 'One notion of the capacity of this associative memory is the\\nmaximum number of vords that it can store vhile correcting fraction 0\\nerrors .', 'Unfortunately.', 'this notion of capacity is ill-defined.', 'because\\nit depends on exactly vhich (address.', 'datum) pairs have been stored.', 'Clearly.', 'no associative memory can correct fraction 0 errors for every\\nsequence of stored (address, datum) pairs.', 'Consider.', 'for example, a\\nsequence in vhich several different vords are vritten to the same\\naddress .', 'No memory can reliably retrieve the contents of the\\novervritten vords.', 'At the other extreme.', \"any associative memory ' can\\nstore an unlimited number of vords and retrieve them all reliably.\", 'if\\ntheir contents are identical.', 'A useful definition of capacity must lie somevhere betveen these\\ntvo extremes.', 'In this paper.', 've are interested in the largest M such\\nthat for most sequences of addresses XU), .. .', ', X(M) and most sequences of\\ndata y(l), ... , y(M).', 'the memory can correct fraction 0 errors.', 'We define\\nIThis vork vas supported by the National Science Foundation under NSF\\ngrant IST-8509860 and by an IBM Doctoral Fellovship.', '?', \"American Institute of Physics 1988\\n\\n\\x0c185\\n\\nmost sequences' in a probabilistic sense, as some set of sequences yi th\\ntotal probability greater than say, .99.\", 'When all sequences are\\nequiprobab1e, this reduces to the deterministic version: 991. of all\\nsequences.', 'In practice it is too difficult to compute the capacity of a given\\nassociative memory yith inputs of length n and outputs of length Tn.', 'Fortunately, though, it is easier to compute the asymptotic rate at\\nwhich A1 increases, as n and Tn increase, for a given family of\\nassociative memories.', 'This is the approach taken by McEliece et al.', '[1]\\ntoyards the capacity of the Hopfield associative memory.', 'We take the\\nsame approach tovards the capacity of the Kanerva associative memory,\\nand tovards the capacities of associative memories in general .', 'In the\\nnext section ve provide an upper bound on the rate of grovth of the\\ncapacity of any associative memory fitting our general model.', 'It is\\nshown by sphere packing arguments that capacity is limited to an\\nexponential rate of grovth of 1- h2(t5), vhere h2(t5) is the binary entropy\\nfunction in bits, and 8 is the radius of attraction.', 'In a later section\\nit vill turn out that this exponential grovth in capacity can actually\\nbe achieved by the Kanerva associative memory, if its parameters are\\noptimally set.', 'This exponential grovth in capacity for the Kanerva\\nassociative memory contrasts sharply yith the sub-linear grovth in\\ncapacity for the Hopfield associative memory [1].', 'I\\n\\nA UNIVERSAL UPPER BOUND ON CAPACITY\\nRecall that our definition of the capacity of an associative memory\\nis the largest A1 such that for most sequences of addresses\\nX(1), ... ,X(M) and most sequences of data y(l), ... , y(M), the memory can\\ncorrect fraction 8 errors.', 'Clearly, an upper bound to this capacity is\\nthe largest Af for vhich there exists some sequence of addresses\\nX(1), .', '.', '.', ', X(M) such that for most sequences of data y(l), ... , y(M), the\\nmemory can correct fraction 8 errors.', 'We nov derive an expression for\\nthis upper bound.', 'Let 8 be the radius of attraction and let DH(X(i) , d) be the sphere\\nof attraction, i.e., the set of all Xs at most Hamming distance d= Ln8J\\nfrom .y(j).', 'Since by assumption the memory corrects fraction 8 errors,\\nevery address X E DH(XU),d) retrieves the vord yW.', 'The size of\\nDH(XU),d) is easily shown to be independent of xU) and equal to\\nvn.d = 2:%=0\\nvhere\\nis the binomial coefficient n!jk!', '(n - k)!.', 'Thus\\nn\\nout of a total of 2 n-bit addresses, at least vn.d addresses retrieve\\ny(l), at least Vn.d addresses retrieve y(2), at least Vn.d addresses\\nretrieve y(~, and so forth.', \"It fol10vs that the total number of\\ndistinct yU)s can be at most 2 n jv n .d ' Nov, from Stirling's formula it\\ncan be shovn that if d:S; nj2, then vn.d = 2nh2 (d/n)+O(logn), vhere\\nh 2 ( 8) = -81og 2 8 - (1 - 8) log2( 1 - 8) is the binary entropy function in bits,\\nand O(logn) is some function yhose magnitude grovs more slovly than a\\nconstant times log n. Thus the total number of distinct y(j)s can be at\\nmost 2 n (1-h2(S?+O(logn)\\nSince any set containing I most sequences' of Af\\nTn-bit vords vill contain a large number of distinct vords (if Tn is\\n\\n(1:),\\n\\n(I:)\\n\\n\\x0c186\\n\\nFigure 1: Neural net representation of the Kanerva associative memory.\", 'Signals propagate from the bottom (input) to the top (output).', 'Each arc multiplies the signal by its\\nweight; each node adds the incoming signals and then thresholds.', 'sufficiently large --- see [2] for details), it follovs that\\nM :5 2 n (l-h 2 (o?+O(logn).', '(1)\\n\\nIn general a function fen) is said to be O(g(n)) if f(n)fg(n) is\\nbounded, i.e.', ', if there exists a constant a such that If(n)1 :5 a\\\\g(n)1 for\\nall n. Thus (1) says that there exists a constant a such that\\nM :5 2 n(l-h 2 (S?+alogn.', 'It should be emphasized that since a is unknow,\\nthis bound has no meaning for fixed n. Hovever, it indicates that\\nasymptotically in n, the maximum exponential rate of grovth of M is\\n1 - h2 ( 6).', 'Intui ti vely, only a sequence of addresses X(l), ... , X(M) that\\noptimally pack the address space {-l,+l}n can hope to achieve this\\nupper bound.', 'Remarkably, most such sequences are optimal in this sense,\\nvhen n is large.', 'The Kanerva associative memory can take advantage of\\nthis fact.', 'THE KANERVA ASSOCIATIVE MEMORY\\nThe Kanerva associative memory [3,4] can be regarded as a tvo-layer\\nneural netvork, as shovn in Figure 1, vhere the first layer is a\\npreprocessor and the second layer is the usual Hopfield style array.', 'The preprocessor essentially encodes each n-bit input address into a\\nvery large k-bit internal representation, k ~ n, vhose size will be\\npermitted to grov exponentially in n. It does not seem surprising,\\nthen, that the capacity of the Kanerva associative memory can grov\\nexponentially in n, for it is knovn that the capacity of the Hopfield\\narray grovs almost linearly in k, assuming the coordinates of the\\nk-vector are dravn at random by independent flips of a fair coin [1].', '187\\n\\nFigure 2: Matrix representation of the Kanerva associative memory.', 'Signals propagate\\nfrom the right (input) to the left (output).', 'Dimensions are shown in the box corners.', 'Circles stand for functional composition; dots stand for matrix multiplication.', 'In this situation, hovever, such an assumption is ridiculous: Since the\\nk-bit internal representation is a function of the n-bit input address,\\nit can contain at most n bits of information, whereas independent flips\\nof a fair coin contain k bits of information.', \"Kanerva's primary\\ncontribution is therefore the specification of the preprocessor, that\\nis, the specification of how to map each n-bit input address into a very\\nlarge k-bit internal representation.\", 'The operation of the preprocessor is easily described.', 'Consider\\nthe matrix representation shovn in Figure 2.', 'The matrix Z is randomly\\npopulated vith ?ls.', 'This randomness assumption is required to ease the\\nanalysis.', 'The function fr is 1 in the ith coordinate if the ith row of\\nZ is within Hamming distance r of X, and is Oothervise.', 'This is\\naccomplished by thresholding the ith input against n-2r.', 'The\\nparameters rand k are two essential parameters in the Kanerva\\nassociative memory.', 'If rand k are set correctly, then the number of 1s\\nin the representation fr(ZX) vill be very small in comparison to the\\nnumber of Os.', 'Hence fr(Z~Y) can be considered to be a sparse internal\\nrepresentation of X.', 'The second stage of the memory operates in the usual way, except on\\nthe internal representation of X.', 'That is, Y = g(W fr(ZX)), vhere\\nM\\n\\nl-V = LyU)[Jr(ZXU))]t,\\n\\n(2)\\n\\ni=l\\n\\nand 9 is the threshold function whose ith coordinate is +1 if the ith\\ninput is greater than 0 and -1 is the ith input is less than O.', 'The ith\\ncolumn of l-V can be regarded as a memory location vhose address is the\\nith row of Z.', 'Every X vi thin Hamming distance r of the ith rov of Z\\naccesses this location.', 'Hence r is known as the access radius, and k is\\nthe number of memory locations.', 'The approach taken in this paper is to fix the linear rate p at\\nwhich r grovs vith n, and to fix the exponential rate ~ at which k grovs\\nwith n. It turns out that the capacity then grovs at a fixed\\nexponential rate Cp,~(t5), depending on p, ~, and 15.', 'These exponential\\nrates are sufficient to overcome the standard loose but simple\\npolynomial bounds on the errors due to combinatorial approximations.', '188\\n\\nTHE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY\\nFix 0 $ K $1.', '0 $ p$ 1/2.', 'and 0 $ 0 $ min{2p,1/2}.', 'Let n be the\\ninput address length, and let Tn be the output word length.', 'It is\\nassumed that Tn is at most polynomial in n, i.e., Tn = exp{O(logn)}.', 'Let\\nr = IJmJ be the access radius, let k = 2 L\"nJ be the number of memory\\nlocations, and let d= LonJ be the radius of attraction.', 'Let Afn be the\\nnumber of stored words.', 'The components of the n-vectors X(l), .. .', ', X(Mn) ,\\nthe m-vectors y(l), ... , y(,Yn), and the k X n matrix Z are assumed to be\\nlID equiprobable ?1 random variables.', 'Finally, given an n-vector X,\\nlet Y = g(W fr(ZX)) where W = Ef;nl yU)[Jr(ZXW)jf.', \"Define the quantity\\n\\nCp ,,(0) = { 26 + 2(1- 0)h(P;~~2)\\n'Cp,ICo(p)(o)\\nwhere\\nKO(p)\\n\\n2h(p)\\n\\n2; - 2(1- ;)h(P~242)\\n\\n= 2h(p) -\\n\\nand\\n\\n~-\\n\\n; =\\n\\nTheorem:\\n\\n+ K, -\\n\\nIf\\nAf\\n\\nJ\\n\\n196 -\\n\\nif K, $ K,o(p)\\nif K> K,O(p) ,\\n\\n+ 1- he;)\\n\\n(3)\\n\\n(4)\\n\\n2p(1 - p).\", '< 2nCp... (5)+O(logn)\\n\\nn_\\n\\nthen for all f>O, all sufficiently large n, all jE{l, ... ,Afn }.', 'and all\\nX E DH(X(j) , d),\\n\\nP{y\\n\\n-::J y(j)}\\n\\n< f.\\n\\nSee [2].', 'Interpretation: If the exponential growth rate of the number of\\nstored words Afn is asymptotically less than C p ,,, ( 0), then for every\\nsufficiently large address length n. there is some realization of the\\nnx 2n \" preprocessor matrix Z such that the associative memory can\\ncorrect fraction 0 errors for most sequences of Afn (address, datum)\\npairs.', 'Thus Cp,IC( 0) is a lover bound on the exponential growth rate of\\nthe capacity of the Kanerva associative memory with access radius np and\\nnumber of memory locations 2nIC ?', 'Figure 3 shows Cp,IC(O) as a function of the radius of attraction 0,\\nfor K,= K,o(p) and p=O.l, 0.2, 0.3, 0.4 and 0.45.', 'For?', 'any fixed access\\nradius p, Cp,ICO(p) (0) decreases as 0 increases.', 'This reflects the fact\\nthat fewer (address, datum) pairs can be stored if a greater fraction of\\nerrors must be corrected.', 'As p increases, Cp,,,o(p)(o) begins at a lower\\npoint but falls off less steeply.', 'In a moment we shall see that p can\\nbe adjusted to provide the optimal performance for a given O.', 'Not ShOVIl in Figure 3 is the behavior of Cp ,,, ( 0) as a function of K,.', 'However, the behavior is simple.', 'For K, > K,o(p), Cp,,,(o) remains\\nunchanged, while for K$ K,o(p), Cp,,,(o) is simply shifted doVIl by the\\ndifference KO(p)-K,.', 'This establishes the conditions under which the\\nKanerva associative memory is robust against random component failures.', 'Although increasing the number of memory locations beyond 2rl11:o(p) does\\nnot increase the capacity, it does increase robustness.', 'Random\\nProof:\\n\\n\\x0c189\\n\\n0.8\\n\\n0.6\\n\\n\\'!I.2 ...... - - -\\n\\n\"\\n\\n\" ?1\\n\\n1Il.2\\n\\nIIl.S\\n\\n1Il.3\\n\\nFigure 3: Graphs of Cp,lCo(p)(o) as defined by (3).', 'The upper envelope is 1- h2(0).', 'component failures will not affect the capacity until so many components\\nhave failed that the number of surviving memory locations is less than\\n2nlCo (p) .', 'Perhaps the most important curve exhibited in Figure 3 is the\\nsphere packing upper bound 1 - h2 ( 0).', 'which is achieved for a particular\\n\\nJ\\n\\np by b = ~ - 196 - 2p(1 - p).', 'Equivalently.', 'the upper bound is achieved\\nfor a particular 0 by P equal to\\n\\npoCo) =\\n\\nt - Jt - iO(l -\\n\\n~o).', '(5)\\n\\nThus (4) and (5) specify the optimal values of the parameters K and P.\\nrespectively.', 'These functions are shown in Figure 4.', 'With these\\noptimal values.', '(3) simplifies to\\n\\nthe sphere packing bound.', 'It can also be seen that for 0 = 0 in (3).', 'the exponential growth\\nrate of the capacity is asymptotically equal to K. which is the\\nexponential growth rate of the number of memory locations.', 'k n ?', 'That is.', 'Mn = 2n1C +O(logn) = k n .', '20 (logn).', 'Kanerva [3] and Keeler [5] have argued\\nthat the capacity at 8 =0 is proportional to the number of memory\\nlocations, i.e .?', 'Mn = k n .', '(3. for some constant (3.', 'Thus our results are\\nconsistent with those of Kanerva and Keeler.', \"provided the 'polynomial'\\n20 (logn) can be proved to be a constant.\", 'However.', 'the usual statement of\\ntheir result.', 'M = k?(3.', 'that the capacity is simply proportional to the\\nnumber of memory locations.', 'is false.', 'since in light of the universal\\n\\n\\x0c190\\n\\nliLS\\n\\no\\nriJ.S\\n\\nFigure 4: Graphs of KO(p) and co(p), the inverse of Po(<5), as defined by (4) and (5).', 'upper bound, it is impossible for the capacity to grow without bound,\\nwith no dependence on the dimension n. In our formulation, this\\ndifficulty does not arise because we have explicitly related the number\\nof memory locations to the input dimension: kn =2n~.', 'In fact, our\\nformulation provides explicit, coherent relationships between all of the\\nfollowing variables: the capacity .~, the number of memory locations k,\\nthe input and output dimensions n and Tn, the radius of attraction C,\\nand the access radius p. We are therefore able to generalize the\\nresults of [3,5] to the case C>0, and provide explicit expressions for\\nthe asymptotically optimal values of p and K as well.', 'CONCLUSION\\nWe described a fairly general model of associative memory and\\nselected a useful definition of its capacity.', 'A universal upper bound\\non the growth of the capacity of such an associative memory was shown by\\na sphere packing argument to be exponential with rate 1 - h 2 ( c), where\\nh2(C) is the binary entropy function and 8 is the radius of attraction.', 'We reviewed the operation of the Kanerva associative memory, and stated\\na lower bound on the exponential growth rate of its capacity.', 'This\\nlower bound meets the universal upper bound for optimal values of the\\nmemory parameters p and K. We provided explicit formulas for these\\noptimal values.', 'Previous results for <5 =0 stating that the capacity of\\nthe Kanerva associative memory is proportional to the number of memory\\nlocations cannot be strictly true.', 'Our formulation corrects the problem\\nand generalizes those results to the case C > o.', '191\\n\\nREFERENCES\\n1.', 'R.J. McEliece, E.C.', 'Posner, E.R.', 'Rodemich, and S.S. Venkatesh,\\n\"The capacity of the Hopfield associative memory,\" IEEE\\nTransactions on Information Theory, submi tt ed .', '2.', 'P.A.', 'Chou, \"The capacity of the Kanerva associative memory,\"\\nIEEE Transactions on Information Theory, submitted.', '3.', 'P. Kanerva, \"Self-propagating search: a unified theory of\\nmemory,\" Tech.', 'Rep. CSLI-84-7, Stanford Center for the Study of\\nLanguage and Information.', 'Stanford.', 'CA, March 1984.', '4.', 'P. Kanerva, \"Parallel structures in human and computer memory,\"\\nin Neural Networks for Computing, (J .S.', 'Denker.', 'ed.', '), Nev York:\\nAmerican Institute of Physics.', '1986.', '5 .', 'J.D.', 'Keeler.', '\"Comparison betveen sparsely distributed memory and\\nHopfield-type neural netvork models,\" Tech .', 'Rep. RIACS TR 86 .', '31,\\nNASA Research Institute for Advanced Computer Science, Mountain\\nViev.', 'CA, Dec. 1986.', '\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto\\nOsaka University, Toyonaka, Osaka 560, Japan\\nABSTRACT\\nAn efficient method of self-organizing associative databases is proposed together with\\napplications to robot eyesight systems.', 'The proposed databases can associate any input\\nwith some output.', 'In the first half part of discussion, an algorithm of self-organization is\\nproposed.', 'From an aspect of hardware, it produces a new style of neural network.', 'In the\\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\\nmobile robot system are demonstrated.', 'INTRODUCTION\\nLet a mapping f : X -+ Y be given.', 'Here, X is a finite or infinite set, and Y is another\\nfinite or infinite set.', 'A learning machine observes any set of pairs (x, y) sampled randomly\\nfrom X x Y.', '(X x Y means the Cartesian product of X and Y.)', 'And, it computes some\\nestimate j : X -+ Y of f to make small, the estimation error in some measure.', 'Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine.', 'However, such expression on performance\\nis incomplete.', 'Since, it lacks consideration on the candidates of J of j assumed preliminarily.', 'Then, how should we find out good learning machines?', 'To clarify this conception,\\nlet us discuss for a while on some types of learning machines.', 'And, let us advance the\\nunderstanding of the self-organization of associative database .', '.', \"Parameter Type\\nAn ordinary type of learning machine assumes an equation relating x's and y's with\\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\\nset F of candidates of\\n(F is some subset of mappings from X to Y.)\", 'And, it computes\\nvalues of the parameters based on the observed samples.', 'We call such type a parameter\\ntype.', 'For a learning machine defined well, if F 3 f, j approaches f as the number of samples\\nincreases.', 'In the alternative case, however, some estimation error remains eternally.', 'Thus,\\na problem of designing a learning machine returns to find out a proper structure of f in this\\nsense.', 'On the other hand, the assumed structure of f is demanded to be as compact as possible\\nto achieve a fast learning.', 'In other words, the number of parameters should be small.', 'Since,\\nif the parameters are few, some j can be uniquely determined even though the observed\\nsamples are few.', 'However, this demand of being proper contradicts to that of being compact.', 'Consequently, in the parameter type, the better the compactness of the assumed structure\\nthat is proper, the better the learning machine.', 'This is the most elementary conception\\nwhen we design learning machines .', '1.\\n\\n.', 'Universality and Ordinary Neural Networks\\nNow suppose that a sufficient knowledge on f is given though J itself is unknown.', 'In\\nthis case, it is comparatively easy to find out proper and compact structures of J.', 'In the\\nalternative case, however, it is sometimes difficult.', \"A possible solution is to give up the\\ncompactness and assume an almighty structure that can cover various 1's.\", 'A combination\\nof some orthogonal bases of the infinite dimension is such a structure.', 'Neural networks 1 ,2\\nare its approximations obtained by truncating finitely the dimension for implementation.', '?', 'American Institute of Physics 1988\\n\\n\\x0c768\\nA main topic in designing neural networks is to establish such desirable structures of 1.', 'This work includes developing practical procedures that compute values of coefficients from\\nthe observed samples.', 'Such discussions are :flourishing since 1980 while many efficient methods have been proposed.', 'Recently, even hardware units computing coefficients in parallel\\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.', 'Nevertheless, in neural networks, there always exists a danger of some error remaining\\neternally in estimating /.', 'Precisely speaking, suppose that a combination of the bases of a\\nfinite number can define a structure of 1 essentially.', 'In other words, suppose that F 3 /, or\\n1 is located near F. In such case, the estimation error is none or negligible.', 'However, if 1\\nis distant from F, the estimation error never becomes negligible.', 'Indeed, many researches\\nreport that the following situation appears when 1 is too complex.', 'Once the estimation\\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\\neven though the dimension is heighten.', 'This property sometimes is a considerable defect of\\nneural networks .', '.', 'Recursive Type\\nThe recursive type is founded on another methodology of learning that should be as\\nfollows.', 'At the initial stage of no sample, the set Fa (instead of notation F) of candidates\\nof I equals to the set of all mappings from X to Y.', \"After observing the first sample\\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\\nthe second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\\nof samples proceeds.\", 'The after observing i-samples, which we write\\nis one of the most\\nlikelihood estimation of 1 selected in fi;.', 'Hence, contrarily to the parameter type, the\\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.', 'The recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for\\nsome x\\'s correlated to the sample.', 'Hence, this type has an architecture composed of a rule\\nfor rewriting and a free memory space.', 'Such architecture forms naturally a kind of database\\nthat builds up management systems of data in a self-organizing way.', 'However, this database\\ndiffers from ordinary ones in the following sense.', 'It does not only record the samples already\\nobserved, but computes some estimation of l(x) for any x E X.', 'We call such database an\\nassociative database.', 'The first subject in constructing associative databases is how we establish the rule for\\nrewriting.', 'For this purpose, we adapt a measure called the dissimilarity.', 'Here, a dissimilarity\\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\\nwhenever l(x) t /(x).', 'However, it is not necessarily defined with a single formula.', 'It is\\ndefinable with, for example, a collection of rules written in forms of \"if?', '..', 'then??', '.. \"\\nThe dissimilarity d defines a structure of 1 locally in X x Y.', 'Hence, even though\\nthe knowledge on f is imperfect, we can reflect it on d in some heuristic way.', 'Hence,\\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\\nd well.', \"Especially, we can easily find out simple d's for those l's which process analogically\\ninformation like a human.\", '(See the applications in this paper.)', \"And, for such /'s, the\\nrecursive type shows strongly its effectiveness.\", \"We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???.\", 'One of the simplest\\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.', 'i\\n\\ni\"\\n\\nI,\\n\\nAlgorithm 1.', 'At the initial stage, let So be the empty set.', 'For every i =\\n1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\\n\\nd(x, x*) =\\n\\nmin\\n(%,y)ES.-t\\n\\nd(x, x) .', 'Furthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"\\n\\n(1)\\n\\ny,n.', '769\\n\\nAnother version improved to economize the memory is as follows.', 'Algorithm 2, At the initial stage, let So be composed of an arbitrary element\\nin X x Y.', 'For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such\\nthat (x?, y.)', 'E Si-l and\\nd(x, x?)', '=\\n\\nmin\\n\\nd(x, x) .', \"(i,i)ES.-l\\n\\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}'\\nIn either construction, ii approaches to f as i increases.\", 'However, the computation time\\ngrows proportionally to the size of Si.', 'The second subject in constructing associative\\ndatabases is what addressing rule we should employ to economize the computation time.', 'In\\nthe subsequent chapters, a construction of associative database for this purpose is proposed.', 'It manages data in a form of binary tree.', 'SELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nGiven a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative\\ndatabase is as follows.', \"Algorithm 3,'\\n\\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd.\", 'Here, x[.]', 'and y[.]', 'are\\nvariables assigned for respective nodes to memorize data..', 'Furthermore, let t = 1.', 'Step 2: Increase t by 1, and put x, in.', 'After reset a pointer n to the root, repeat\\nthe following until n arrives at some terminal node, i.e., leaf.', 'Notations nand\\nd(xt, x[n)), let n\\n\\nn mean the descendant nodes of n.\\n=n.', 'Otherwise, let n =n.', 'If d(x\" r[n)) ~\\n\\nStep 3: Display yIn] as the related information.', 'Next, put y, in.', 'If yIn] = y\" back\\nto step 2.', 'Otherwise, first establish new descendant nodes n and n. Secondly,\\nlet\\n\\n(x[n], yIn))\\n(x[n], yIn))\\n\\n(x[n], yIn)),\\n(Xt, y,).', '(2)\\n(3)\\n\\nFinally, back to step 2.', 'Here, the loop of step 2-3 can be stopped at any time\\nand also can be continued.', 'Now, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared.', 'Then, we obtain a new style of neural network with gate elements\\nbeing randomly connected by this algorithm.', 'LETTER RECOGNITION\\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\\nelastic matching method for recognizing hand written discrete English letters4 , the global\\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\\nstyleS, etc.', 'are published.', 'The self-organization of associative database realizes the recognition of handwritten continuous English letters.', '770\\n\\n9 /wn\"\\n\\nNOV\\n\\n~ ~ ~ -xk :La.t\\n\\n~~ ~ ~~~\\n\\ndw1lo\\'\\n\\n~~~~~of~~\\n\\n~~~ 4,-?~~4Fig.', '1.', 'Source document.', \"2~~---------------'\\n\\nlOO~---------------'\\n\\nH\\n\\no\\n\\no\\nFig.\", '2.', 'Windowing.', '1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNumber of samples\\n\\no\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNUAlber of sampl es\\n\\nFig.', '3.', 'An experiment result.', 'An image scanner takes a document image (Fig.', '1).', 'The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig.', '2), and processes the\\nsequence of letters while shifting the window.', 'That is, the recognizer scans a word in a\\nslant direction.', 'And, it places the window so that its left vicinity may be on the first black\\npoint detected.', 'Then, the window catches a letter and some part of the succeeding letter.', 'If recognition of the head letter is performed, its end position, namely, the boundary line\\nbetween two letters becomes known.', 'Hence, by starting the scanning from this boundary\\nand repeating the above operations, the recognizer accomplishes recursively the task.', 'Thus\\nthe major problem comes to identifying the head letter in the window.', 'Considering it, we define the following.', '?', \"Regard window images as x's, and define X accordingly.\", '?', 'For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\\nwindow image X.', 'Project each B onto window image x.', 'Then, measure the Euclidean\\ndistance 6 between fj and a black point B on x being the closest to B.', \"Let d(x, x) be\\nthe summation of 6's for all black points B's on x divided by the number of B's.\", '?', 'Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y\\naccordingly.', 'An operator teaches the recognizer in interaction the relation between window image and\\nreading& boundary with algorithm 3.', 'Precisely, if the recalled reading is incorrect, the\\noperator teaches a correct reading via the console.', 'Moreover, if the boundary position is\\nincorrect, he teaches a correct position via the mouse.', 'Fig.', '1 shows partially a document image used in this experiment.', 'Fig.', '3 shows the\\nchange of the number of nodes and that of the recognition rate defined as the relative\\nfrequency of correct answers in the past 1000 trials.', 'Speciiications of the window are height\\n= 20dot, width = 10dot, and slant angular = 68deg.', 'In this example, the levels of tree\\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.', 'Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\\na rare case.', 'However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable\\nbecause of excessive lluctuation in writing.', 'If the consistency of the x, y-relation is not\\nassured like this, the number of nodes increases endlessly (d. Fig.', '3).', 'Hence, it is clever to\\nstop the learning when the recognition rate attains some upper limit.', 'To improve further\\nthe recognition rate, we must consider the spelling of words.', 'It is one of future subjects.', '771\\n\\nOBSTACLE AVOIDING MOVEMENT\\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.', 'The system made up by the authors (Fig.', '4) also belongs to this category.', 'Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\\na cost minimization problem under some cost criterion established artificially.', 'Contrarily,\\nthe self-organization of associative database reproduces faithfully the cost criterion of an\\noperator.', 'Therefore, motion of the robot after learning becomes very natural.', 'Now, the length, width and height of the robot are all about O.7m, and the weight is\\nabout 30kg.', 'The visual angle of camera is about 55deg.', 'The robot has the following three\\nfactors of motion.', 'It turns less than ?30deg, advances less than 1m, and controls speed less\\nthan 3km/h.', \"The experiment was done on the passageway of wid th 2.5m inside a building\\nwhich the authors' laboratories exist in (Fig.\", '5).', 'Because of an experimental intention, we\\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc.', 'on the passage way at\\nrandom.', 'We let the robot take an image through the camera, recall a similar image, and\\ntrace the route preliminarily recorded on it.', 'For this purpose, we define the following.', '?', 'Let the camera face 28deg downward to take an image, and process it through a low\\npass filter.', 'Scanning vertically the filtered image from the bottom to the top, search\\nthe first point C where the luminance changes excessively.', 'Then, su bstitu te all points\\nfrom the bottom to C for white, and all points from C to the top for black (Fig.', '6).', \"(If no obstacle exists just in front of the robot, the white area shows the ''free'' area\\nwhere the robot can move around.)\", \"Regard binary 32 x 32dot images processed thus\\nas x's, and define X accordingly.\", '?', 'For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\\nimage between x and X.\\n?', \"Regard as y's the images obtained by drawing routes on images x's, and define Y\\naccordingly.\", 'The robot superimposes, on the current camera image x, the route recalled for x, and\\ninquires the operator instructions.', 'The operator judges subjectively whether the suggested\\nroute is appropriate or not.', 'In the negative answer, he draws a desirable route on x with the\\nmouse to teach a new y to the robot.', 'This opera.tion defines implicitly a sample sequence\\nof (x, y) reflecting the cost criterion of the operator.', '.', '::l\" !', '-\\n\\nIibUBe\\n\\n_.', '-\\n\\n22\\n\\n11\\n\\nRoan\\n\\n12\\n\\n{-\\n\\n13\\n\\nStationary uni t\\n\\nFig.', '4.', 'Configuration of\\nautonomous mobile robot system.', '~\\n\\nI\\n\\n,\\n\\n23\\n\\n24\\n\\nNorth\\n14\\n\\nrmbi Ie unit (robot)\\n\\n-\\n\\nRoan\\n\\ny\\n\\nt\\n\\nFig.', '5.', 'Experimental\\nenvironment.', '772\\n\\nWall\\n\\nCamera image\\n\\nPreprocessing\\n\\nA\\n\\n::: !fa\\n\\n?', 'Preprocessing\\n\\n0\\n\\nO\\n\\nCourse\\nsuggest ion\\n\\n??', '..\\n\\nSearch\\n\\nA\\n\\nFig.', '6.', 'Processing for\\nobstacle avoiding movement.', 'x\\n\\nFig.', '1.', 'Processing for\\nposition identification.', 'We define the satisfaction rate by the relative frequency of acceptable suggestions of\\nroute in the past 100 trials.', 'In a typical experiment, the change of satisfaction rate showed\\na similar tendency to Fig.', '3, and it attains about 95% around time 800.', 'Here, notice that\\nthe rest 5% does not mean directly the percentage of collision.', '(In practice, we prevent the\\ncollision by adopting some supplementary measure.)', 'At time 800, the number of nodes was\\n145, and the levels of tree were distributed in 6-17.', 'The proposed method reflects delicately various characters of operator.', \"For example, a\\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\\ntrained by another operator 0' brushes quickly against obstacles.\", 'This fact gives us a hint\\non a method of printing \"characters\" into machines.', 'POSITION IDENTIFICATION\\nThe robot can identify its position by recalling a similar landscape with the position data\\nto a camera image.', \"For this purpose, in principle, it suffices to regard camera images and\\nposition data as x's and y's, respectively.\", 'However, the memory capacity is finite in actual\\ncompu ters.', 'Hence, we cannot but compress the camera images at a slight loss of information.', 'Such compression is admittable as long as the precision of position identification is in an\\nacceptable area.', 'Thus, the major problem comes to find out some suitable compression\\nmethod.', 'In the experimental environment (Fig.', '5), juts are on the passageway at intervals of\\n3.6m, and each section between adjacent juts has at most one door.', 'The robot identifies\\nroughly from a surrounding landscape which section itself places in.', 'And, it uses temporarily\\na triangular surveying technique if an exact measure is necessary.', 'To realize the former task,\\nwe define the following .', '?', 'Turn the camera to take a panorama image of 360deg.', 'Scanning horizontally the\\ncenter line, substitute the points where the luminance excessively changes for black\\nand the other points for white (Fig.', '1).', \"Regard binary 360dot line images processed\\nthus as x's, and define X accordingly.\", '?', 'For every (x, x) E X x X, project each black point A on x onto x.', 'And, measure the\\nEuclidean distance 6 between A and a black point A on x being the closest to A.', \"Let\\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\\nDenoting the numbers of A's and A's respectively by nand n, define\\n\\n\\x0c773\\n\\nd(x, x) =\\n\\n~(~\\n+ ~).\", '2 n\\nn\\n\\n(4)\\n\\n?', \"Regard positive integers labeled on sections as y's (cf.\", 'Fig.', '5), and define Y accordingly.', 'In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator.', 'The robot runs arbitrarily on the passageways within 18m area\\nand learns the relation between landscapes and position data.', '(Position identification beyond 18m area is achieved by crossing plural databases one another.)', 'This task is automatic\\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.', 'We define the identification rate by the relative frequency of correct recalls of position\\ndata in the past 100 trials.', 'In a typical example, it converged to about 83% around time\\n400.', 'At time 400, the number of levels was 202, and the levels oftree were distributed in 522.', 'Since the identification failures of 17% can be rejected by considering the trajectory, no\\npro blem arises in practical use.', 'In order to improve the identification rate, the compression\\nratio of camera images must be loosened.', 'Such possibility depends on improvement of the\\nhardware in the future.', 'Fig.', '8 shows an example of actual motion of the robot based on the database for obstacle\\navoiding movement and that for position identification.', 'This example corresponds to a case\\nof moving from 14 to 23 in Fig.', '5.', 'Here, the time interval per frame is about 40sec.', ',~.', '.~ (\\n;~\"i..\\n~\\n\\n\"\\n\\n\"\\n\\n.', '..I\\n\\nI\\n\\n?', '?\\n\\n\"', \"I'\\n.\", \"'.1\\nt\\n\\n;\\n\\ni\\n\\n-:\\n, .\", '.', \", 'II\\n\\nFig.\", '8.', 'Actual motion of the robot.', '774\\n\\nCONCLUSION\\nA method of self-organizing associative databases was proposed with the application to\\nrobot eyesight systems.', 'The machine decomposes a global structure unknown into a set of\\nlocal structures known and learns universally any input-output response.', 'This framework\\nof problem implies a wide application area other than the examples shown in this paper.', 'A defect of the algorithm 3 of self-organization is that the tree is balanced well only\\nfor a subclass of structures of f. A subject imposed us is to widen the class.', 'A probable\\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\\nestablish another rule depending on the distribution function of values of d. It is now under\\ninvestigation.', 'REFERENCES\\n1.', 'Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'\\n\\nScience 233 (1986), pp.', '625-633.', '2.', 'Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp.', '533-536.', '3.', 'Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word\\nRecognition,\" IEEE Expert, Fall (1986), pp.', '63-70.', '4.', 'Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res.', 'Develop.', '31-1 (1987), pp.', '91-95.', '5.', 'Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and\\nGlobal Training,\" IEEE Trans.', 'Pattern.', 'Anal.', '& Mach.', 'Intell.', 'PAMI 9-1\\n(1987) pp.', '91-102.', '6.', 'Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile\\nRobots,\" 4th Int.', 'Symp.', 'of Robotics Research (1987), pp.', '267-276.', '7.', 'Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987\\nIEEE Int.', 'Conf.', 'on Robotics & Automation (1987), pp.', '99-105.', '8.', 'Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"\\nIEEE Jour.', 'of Robotics & Automation RA 2-3 (1986), pp.', '117-125.', '9.', 'Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int.', 'Conf.', 'on Robotics & Automation (1987), pp.', '1725-1730.', '10.', 'Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"\\n1987 IEEE Int.', 'Conf.', 'on Robotics & Automation (1987), pp.', '273-279.', '\\n\\nAN ARTIFICIAL NEURAL NETWORK FOR SPATIOTEMPORAL BIPOLAR PATTERNS: APPLICATION TO\\nPHONEME CLASSIFICATION\\nToshiteru Homma\\nLes E. Atlas\\nRobert J.', 'Marks II\\n\\nInteractive Systems Design Laboratory\\nDepartment of Electrical Engineering, Ff-l0\\nUniversity of Washington\\nSeattle, Washington 98195\\n\\nABSTRACT\\nAn artificial neural network is developed to recognize spatio-temporal\\nbipolar patterns associatively.', 'The function of a formal neuron is generalized by\\nreplacing multiplication with convolution, weights with transfer functions, and\\nthresholding with nonlinear transform following adaptation.', 'The Hebbian learning rule and the delta learning rule are generalized accordingly, resulting in the\\nlearning of weights and delays.', 'The neural network which was first developed\\nfor spatial patterns was thus generalized for spatio-temporal patterns.', 'It was\\ntested using a set of bipolar input patterns derived from speech signals, showing\\nrobust classification of 30 model phonemes.', '1.', 'INTRODUCTION\\nLearning spatio-temporal (or dynamic) patterns is of prominent importance in biological\\nsystems and in artificial neural network systems as well.', 'In biological systems, it relates to such\\nissues as classical and operant conditioning, temporal coordination of sensorimotor systems and\\ntemporal reasoning.', 'In artificial systems, it addresses such real-world tasks as robot control,\\nspeech recognition, dynamic image processing, moving target detection by sonars or radars, EEG\\ndiagnosis, and seismic signal processing.', 'Most of the processing elements used in neural network models for practical applications\\nhave been the formal neuron l or\" its variations.', 'These elements lack a memory flexible to temporal patterns, thus limiting most of the neural network models previously proposed to problems\\nof spatial (or static) patterns.', 'Some past solutions have been to convert the dynamic problems to\\nstatic ones using buffer (or storage) neurons, or using a layered network with/without feedback.', 'We propose in this paper to use a \"dynamic formal neuron\" as a processing element for\\nlearning dynamic patterns.', 'The operation of the dynamic neuron is a temporal generalization of\\nthe formal neuron.', 'As shown in the paper, the generalization is straightforward when the activation part of neuron operation is expressed in the frequency domain.', 'Many of the existing learning rules for static patterns can be easily generalized for dynamic patterns accordingly.', 'We show\\nsome examples of applying these neural networks to classifying 30 model phonemes.', '?', 'American Institute of Physics 1988\\n\\n\\x0c32\\n\\n2.', 'FORMAL NEURON AND DYNAMIC FORMAL NEURON\\nThe formal neuron is schematically drawn in Fig.', \"l(a), where\\n\\nr = [Xl Xz ... xd 1\\nYi' i = 1,2?...\", '?N\\nZi, i = 1,2. .', '.', '.', \"?N\\n\\nInput\\nActivation\\nOutput\\nTransmittance\\nNode operator\\nNeuron operation\\n\\nW\\n\\n= [Wil\\n\\nWiZ ... wiLf\\n\\n11 where 11(') is a nonlinear memory less transform\\nZi\\n\\n= 11(wTr>\\n\\n(2.1)\\n\\nNote that a threshold can be implicitly included as a transmittance from a constant input.\", \"In its original form of formal neuron, Xi E {O,I} and 110 is a unit step function u (').\", 'A\\nvariation of it is a bipolar formal neuron where Xi E {-I, I} and 110 is the sign function sgn O.', 'When the inputs and output are converted to frequency of spikes, it may be expressed as\\nXi E Rand 110 is a rectifying function rO.', 'Other node operators such as a sigmoidal function\\nmay be used.', 'We generalize the notion of formal neuron so that the input and output are functions of\\ntime.', 'In doing so, weights are replaced with transfer functions, multiplication with convolution,\\nand the node operator with a nonlinear transform following adaptation as often observed in biological systems.', 'Fig.', '1(b) shows a schematic diagram of a dynamic formal neuron where\\nr(l) = [Xl(t) xz(t) ... xdt)f\\nYi(t), i == 1,2?...', '.', 'N\\nZi(t), i = 1,2?...', '?N\\nw(t) = [Wjl(t) wiZ(t) ... WiL(t)]T\\nai (t)\\n\\nInput\\nActivation\\nOutput\\nTransfer function\\nAdaptation\\nNode operator\\nNeuron operation\\n\\n1l where 110 is a nonlinear memoryless transform\\nZj(t)\\n\\n=ll(ai (-t).', 'W;(t)T .x(t?', '(2.2)\\n\\nFor convenience, we denote ?', 'as correlation instead of convolution.', 'Note that convolving a(t)\\nwith b(t) is equivalent to correlating a( -t) with b(t).', 'If the Fourier transforms r(f)=F{r(t)}, w;(f)=F{W;(t)}, Yj(f)=F{Yi(t)}, and\\naj(f) = F {ai(t)} exist, then\\nYi (f)\\n\\n= ai (f)\\n\\n[Wi (f\\n\\nfT\\n\\nr(f)]\\n\\n(2.3)\\n\\nwhere Wi (f fT is the conjugate transpose of Wi (t).', 'x,(1)\\n\\nI----zt\\n\\n1----zt(I)\\n\\n?', '(b)\\n\\nFig.', '1.', 'Formal Neuron and Dynamic Formal Neuron.', '33\\n\\n3.', 'LEARNING FOR FORMAL NEURON AND DYNAMIC FORMAL NEURON\\nA number of learning rules for formal neurons has been proposed in the past.', 'In the following paragraphs, we formulate a learning problem and describe two of the existing learning\\nrules, namely, Hebbian learning and delta learning, as examples.', 'Present to the neural network M pairs of input and desired output samples\\nk ::;: 1,2, ... ,M , in order.', 'Let W(k)::;: [w/k) w!k) \\'\" wJk~T where wr) is the\\ntransmittance vector at the k-th step of learning.', 'Likewise, let\\n{X<k), (lk)},\\n\\nK(k) = [X<I) x\\'-2)\\n\\n... X<k)], r(k)\\n\\n~(k)\\n\\n= [z<I) z<2)\\n\\n... ~k)],\\n\\n\\'Ik)\\n\\n= W(k)x\\'-k),\\n\\nz<k)\\n\\nand\\n\\n= rfl) t 2) ...\\nD(k) = [(ll) (l2)\\n\\nt k)],\\n\\'\"\\n\\n(lk)] ,\\n\\nwhere\\n\\n= n<tk?,\\n\\nand\\n\\nn<Y> = [T1(Y I) T1(Y2) .. .', 'T1(yN)]T.\\n\\nThe Hebbian learning rule 2 is described as follows *:\\nW(k) ::;: W(k-I) + a;JC.k)X<k)T\\n\\n(3.1)\\n\\nThe delta learning (or LMS learning) rule 3, 4 is described as follows:\\nW(k)\\n\\n= W(k-I) _\\n\\no.', '{W(k-l)t:k ) _ (lk)}X<k)T\\n\\n(3.2)\\n\\nThe learning rules described in the previous section are generalized for the dynamic formal\\nneuron by replacing multiplication with correlation.', 'First, the problem is reformulated and then\\nthe generalized rules are described as follows.', 'Present to the neural network M pairs of time-varing input\\n= 1,2, .. .', ',M , in order.', 'Let W(k)(t) = [WI(t)(k)(t)\\nwhere w/k)(t) is the vector whose elements W;)t)(t) are transfer functions\\nto the neuron i at the k-th step of learning.', 'The Hebbian learning rule for\\nthen\\n{X<k)(t), (lk)(t)), k\\n\\nW(kl(t)\\n\\n= W(k-I)(t) + 0.(-1}.', '(lk)(t).', 'X<k)(t)T\\n\\nand output samples\\nw~k)(t)??', '.', 'wJk)(t)f\\nconnecting the input j\\nthe dynamic neuron is\\n\\n.', '(3.3)\\n\\nThe delta learning rule for dynamic neuron is then\\nW(kl(t) ::;: W(k-I)(t) - o.(-t).', '{W(k-Il(t).', 'X<k)(t) - (It)(t)} .X<k)(t)T .', '(3.4)\\n\\nThis generalization procedure can be applied to other learning rules in some linear discriminant systems 5 , the self-organizing mapping system by Kohonen6 , the perceptron 7 , the backpropagation model 3 , etc.', 'When a system includes a nonlinear operation, more careful analysis\\nis necesssay as pointed out in the Discussion section.', '4.', 'DELTA LEARNING,PSEUDO INVERSE AND REGULARIZATION\\nThis section reviews the relation of the delta learning rule to the pseudo-inverse and the\\ntechnique known as regularization.', \"4, 6, 8, 9,10\\nConsider a minimization problem as described below: Find W which minimizes\\nR\\n\\n= LII'Ik) -\\n\\n(lk)U\\n\\ni = <f-k) -\\n\\n(lky <tk) - (lk?\", '(4.1)\\n\\nsubject to t k ) = WX<k) ?', 'A solution by the delta rule is, using a gradient descent method,\\nW(k)\\n\\n-\\n\\n= W(k-I) _ o.-1... R (k)\\naw\\n\\n?', 'This interpretation assumes a strong supervising signal at the output while learning.', '(4.2)\\n\\n\\x0c34\\n\\nwhere R (k) = II y<k) ... ~A:)1I1.', 'The minimum norm solution to the problem, W*, is unique and\\ncan tie expressed as\\n\\nW* == D xt\\n\\n(4.3)\\n\\nwhere !.', 't is the Moore-Penrose pseudo-inverse of!.', \", i.e.,\\n\\nX t = lim(XTX + dl/)-lX T = limXT (X XT\\n-\\n\\na-.o -\\n\\n-\\n\\n-\\n\\nOn the condition that 0 <\\n\\n-\\n\\na-+O-\\n\\n- -\\n\\n+ dl/)-l.\\n-\\n\\n(4.4)\\n\\na < ~ where An- is the max.imum eigenvalue of !.T!., J'.k) and\\n\\n(jC.k) are independent, and WCl) is uncorrelated with ~l),\\n\\nE {W*}\\n\\n=E (~c..)}\\n\\n(4.5)\\n\\nwhere E {x} denotes the expected value of x.\", 'One way to make use of this relation is to calculate W* for known standard data and refine it by (4.2), thereby saving time in the early stage of\\nlearning.', 'However, this solution results in an ill-conditioned W often in practice.', 'When the problem is ill-posed as such, the technique known as regularization can alleviate the ill-conditioning\\nof W .', 'The problem is reformulated by finding W which minimizes\\n\\nR(a)\\n\\n= Dly<t) -\\n\\n(jC.k)IIl\\n\\n+ dlLII wkll 1\\n\\n1\\n\\n(4.6)\\n\\nk\\n\\nt =\\n\\nsubject to k ) ~k) where W = [Wlw2 ... WN]T .', 'This reformulation regularizes (4.3) to\\nW (a) =\\n\\nD!.T (!.', '!.T + a2n-1\\n\\n(4.7)\\n\\nwhich is statistically equivalent to Wc..) when the input has an additive noise of variance dl\\nutlcorrelated with ~l).', 'Interestingly, the leaky LMS algorithm ll leads to a statistically\\nequivalent solution\\nW(l)\\n\\n= ~WCk-l) _ tx~(k-l)~l) -\\n\\nwhete 0 < ~ < 1 and 0 <\\n\\nE {W(a)}\\nif dl =\\n\\n(4.8)\\n\\n{jC.l)};f<l)T\\n\\n2\\n\\na < Amax ?', 'These solutions are related as\\n\\n=E {Wc..)}\\n\\n(4.9)\\n\\n..!::J!', 'when WCl) is uncorrelated with ;f<l) .11\\na\\n-\\n\\nEquation (4.8) can be generalized for a network using dynamic formal neurons, resulting in\\na equation similar to (3.4).', 'Making use of (4.9), (4.7) can be generalized for a dynamic neuron\\nnetwork as\\nW (t ; a) = F- 1{Q if )!.', 'if fT (!.', 'if )!.', 'if)CT\\n\\nn-\\n\\n+ a2\\n\\n1}\\n\\n(4.10)\\n\\nwhere F-1 denotes the inverse Fourier transform.', 's. SYNTHESIS OF BIPOLAR PHONEME PATTERNS\\nThis section illustrates the scheme used to synthesize bipolar phoneme patterns and to\\nform prototype and test patterns.', 'The fundamental and first three formant frequencies, along with their bandwidths, of\\nphonemes provided by Klatt l2 were taken as parameters to synthesize 30 prototype phoneme patterns.', 'The phonemes were labeled as shown in Table 1.', 'An array of L (=100) input neurons\\nOOVered the range of 100 to 4000 Hz.', 'Each neuron had a bipolar state which was +1 only when\\none of the frequency bands in the phoneme presented to the network was within the critical band\\n\\n\\x0c35\\nof the neuron and -1 otherwise.', 'The center frequencies if e) of critical bands were obtained by\\ndividing the 100 to 4000 Hz range into a log scale by L. The critical bandwidth was a constant\\n100 Hz up to the center frequency Ie = 500 Hz and 0.2/e Hz when Ie >500 Hz.13\\nThe parameters shown in Table 1 were used to construct Table 1.', 'Labels of Phonemes\\n30 prototype phoneme patterns.', 'For 9. it was constructed as a\\nLabel\\nPhoneme\\ncombination of t and 9.', 'Fl.', 'F 2 .F 3 were the first.', 'second.', 'and\\n1\\n[i Y]\\nthird formants.', \"and B I' B 2. and B 3. were corresponding\\n[Ia]\\n2\\nbandwidths.\", 'The fundamental frequency F 0 = 130 Hz with B 0 =\\n3\\nleY]\\n10 Hz was added when the phoneme was voiced.', 'For plosives.', '4\\n[Ea ]\\nthere was a stop before formant traces start.', \"The resulting bipo[3e']\\n5\\nlar patterns are shown in Fig.2.\", 'Each pattern had length of 5\\n6\\n[el]\\ntime units, composed by linearly interpolating the frequencies\\n7\\n[~]\\nwhen the formant frequency was gliding.', '8\\n[It ]\\nA sequence of phonemes converted from a continuous\\n[ow]\\n9\\npronunciation of digits, {o, zero, one, two, three.', 'four, five, six.', '10\\n[\\\\I~]\\nseven, eight, nine }, was translated into a bipolar pattern, adding\\n11\\n[u w]\\n12\\ntwo time units of transition between two consequtive phonemes\\n[a;J\\n13\\n[a ]\\nby interpolating the frequency and bandwidth parameters\\n[aWl\\n14\\nlinearly.', 'A flip noise was added to the test pattern and created a\\n15\\nloY]\\nnoisy test pattern.', 'The sign at every point in the original clean\\n16\\n[w]\\ntest pattern was flipped with the probability 0.2.', 'These test pat17\\n[y]\\nterns are shown in Fig.', '3.', \"18\\n[r]\\n19\\n[I]\\n20\\n[f]\\n21\\n[v]\\nI'IlDM_ Label I 1 5 7 , II Il 15 .7 ., JI 21 Z5 17 It\\n2 4 , I II 11 14 16 II II U 14 I' II JO\\n22\\n[9]\\nII.\", '23\\n[\\\\]\\n24\\n[s]\\n25\\n[z]\\n26\\n[p]\\n27\\n[t]\\n28\\n[d]\\n29\\n[k]\\n30\\n[n]\\n\\nFig.', '2.', 'Prototype Phoneme Patterns.', '(Thirty phoneme patterns are shown\\nin sequence with intervals of two time units.)', '6.', 'SIMULATION OF SPATIO-TEMPORAL FILTERS FOR PHONEME CLASSIFICATION\\nThe network system described below was simulated and used to classify the prototype\\nphoneme patterns in the test patterns shown in the previoius section.', 'It is an example of generalizing a scheme developed for static patterns 13 to that for dynamic patterns.', 'Its operation is\\nin two stages.', 'The first stage operation is a spatio-temporal filter bank:\\n\\n\\x0c36\\n\\n?', '!!', 'z .4\\n\\n:!', 'e=\\n\\n~\\n\\n!', '?', '?', \"I\\n?\\n\\n'\", ',\\n\\nI\\n\\nif\\n\\n\"I \\'\\n~..', \"I '\\n\\n,\\n\\nlU\\n\\n'I\\n\\nU'\\n\\n(b)\\n\\n(a)\\n\\nFig.\", '3.', 'Test Patterns.', '(a) Clean Test Pattern.', '(b) Noisy Test Pattern.', '(6.1)\\n\\n1(t) = W(t).r(t) , and r(t) = !:(a(-t)y(t?', '.', 'The second stage operation is the \"winner-take-all\" lateral inhibition:\\n(/(t) = zt(t) , and (/(t+A) = !:(~(-t).', '(/(t) -\\n\\nIi),\\n\\n(6.2)\\n\\nand\\nA(t) = (1\\n\\n-\\n\\n114\\n+ -)/O(t) - -S\"fiI\\' 2,O(t-nA).', 'SN -\\n\\nN\\n\\n(6.3)\\n\\n11=0\\n\\nwhere Ii is a constant threshold vector with elements hi = h and 0(.)', 'is the Kronecker delta\\nfunction.', 'This operation is repeated a sufficient number of times, No .13,14 The output is\\n(/(t + No ?A).', 'Two models based on different leaming rules were simulated with parameters shown\\nbelow.', 'Model 1 (Spatio-temporal Matched Filter Bank)\\nLet a(t) = O(t) , (/tk) = et in (3.3) where ek is a unit vector with its elements eki = O(k-i) .', '(6.4)\\n\\nW(t)=!(t)T.', '4 1\\nh=200, and a(t) = 2,-O(t-nA).', '11=0 S\\n\\nModel 2 (Spatio-temporal Pseudo-inverse Filter)\\nLet D\\n\\n=L in (4.10).', 'Using the alternative expression in (4.4),\\nW (t) = F- 1{(!', '(j fT!', '(j) + cr2n-lXCT}.', 'h = O.OS ,cr2 = 1000.0, and a(t)\\n\\n(6.5)\\n\\n= O(t).', 'This minimizes\\nR (cr,!)', '= DI1k )(j) \"\\n\\n(/t\")(j )lIi + cr 22,11\\nk\\n\\nw\" if )lIi\\n\\nfor all !', '.', '(6.6)\\n\\n\\x0c37\\n\\nBecause the time and frequency were finite and discrete in simulation, the result of the\\ninverse discrete Fourier transform in (6.5) may be aliased.', 'To alleviate the aliasing, the transfer\\nfunctions in the prototype matrix:!', '(t) were padded with zeros, thereby doubling the lengths.', 'Further zero-padding the transfer functions did not seem to change teh result significantly.', 'The results are shown in Fig.', '4(a)-(d).', 'The arrows indicate the ideal response positions at\\nthe end of a phoneme.', 'When the program was run with different thresholds and adaptation function a (t), the result was not very sensitive to the threshold value, but was, nevertheless affected\\nby the choice of the adaptation function.', 'The maximum number of iterations for the lateral inhibition network to converge was observed: for the experiments shown in Fig.', '4(a) - (d), the\\nnumbers were 44, 69, 29, and 47, respectively.', 'Model 1 missed one phoneme and falsely\\nresponded once in the clean test pattern.', 'It missed three and had one false response in the noisy\\ntest pattern.', 'Model 2 correctly recognized all phonemes in the clean test pattern, and falsealarmed once in the noisy test pattern.', '7.', 'DISCUSSION\\nThe notion of convolution or correlation used in the models presented is popular in\\nengineering disciplines and has been applied extensively to designing filters, control systems, etc.', 'Such operations also occur in biological systems and have been applied to modeling neural networks.', 'IS ,16 Thus the concept of dynamic formal neuron may be helpful for the improvement of\\nartificial neural network models as well as the understanding of biological systems.', 'A portion of\\nthe system described by Tank and Hopfield 11 is similar to the matched filter bank model simulated in this paper.', 'The matched filter bank model (Modell) performs well when all phonemes (as above) are\\nof the same duration.', \"Otherwise, it would perform poorly unless the lengths were forced to a\\nmaximum length by padding the input and transfer functions with -1' s during calculation.\", 'The\\npseudo-inverse filter model, on the other hand, should not suffer from this problem.', 'However,\\nthis aspect of the 11KXlel (Model 2) has not yet been explicitly simulated.', 'Given a spatio-temporal pattern of size L x K, i.e., L spatial elements and K temporal elements, the number of calculations required to process the first stage of filtering by both models is\\nthe same as that by a static formal neuron network in which each neuron is connected to the L x\\nK input elements.', 'In both cases, L x K multiplications and additions are necessary to calculate\\none output value.', 'In the case of bipolar patterns, the rnutiplication used for calculation of activation can be replaced by sign-bit check and addition.', 'A future investigation is to use recursive\\nfilters or analog filters as transfer functions for faster and more efficient calculation.', 'There are\\nvarious schemes to obtain optimal recursive or analog filters.t 8,19 Besides the lateral inhibition\\nscheme used in the models, there are a number of alternative procedures to realize a \"winnertake-all\" network in analog or digital fashion.', 'IS, 20, 21\\nAs pointed out in the previous section, the Fourier transform in (6.5) requires a precaution\\nconcerning the resulting length of transfer functions.', 'Calculating the recursive correlation equation (3.4) also needs such preprocessing as windowing or truncation.', '22\\nThe generalization of static neural networks to dynamic ones along with their learning\\nrules is strainghtforward as shown if the neuron operation and the learning rule are linear.', 'Generalizing a system whose neuron operation and/or learning rule are nonlinear requires more careful analysis and remains for future work.', 'The system described by Watrous and Shastri l6 is an\\nexample of generalizing a backpropagation model.', 'Their result showed a good potential of the\\nmodel and a need for more rigorous analysis of the model.', 'Generalizing a system with recurrent\\nconnections is another task to be pursued.', 'In a system with a certain analytical nonlinearity, the\\nsignals are expressed by Volterra functionals, for example.', 'A practical learning system can then\\nbe constructed if higher kernels are neglected.', 'For example, a cubic function can be used instead\\nof a sigmoidal function.', \"38\\n\\n1'1\\n\\n3.\", '0-{\\'-r.\\n\\n1\\\\\\n~\\n\\nj\"--\\n\\n~\\n\\n;~.', '1\\\\\\n\\nU\\n\\n--{.', '!', \"e\\n\\n(a)\\n\\n~\\n\\nz\\n\\n~\\n0\\n\\n'\\\\\\n.t\\n\\n?f-t\\n\\n7\\\\\\n\\n-\\n\\n-.\", '?', '?', 'I\\n\\nI\\n\\n, I\\n\\nI\\n\\nI\\n\\nI\\n\\nI\\nIS.', 't ..\\n\\n51\\n\\nI\\n\\nI\\n\\nen\\n\\nTIme\\n\\n\"t\\n\\n~\\n\\nl~\\n\\n~\\n~\\n~7\\n\\n!.', '!', '1\\n\\n1\\n\\n~\\n\\ne\\nIi\\n\\n(b)\\n\\nz\\n\\n\";\\n\\n.', ':-\\n\\n~\\n\\n~\\n\\n?', '1.\\nl\\n\\n?', '?', 'j\\n\\nr--\\n\\nI\\n\\nu\\n\\nI\\n\\nt ..', 'I\\nlSI\\n\\ntu\\n\\nTIme\\n\\nFig.', '4.', 'Performance of Models.', '(a) Modell with Clean Test Pattern.', '(b)\\nModel 2 with Clean Test Pattern.', '(c) Modell with Noisy Test Pattern.', '(d) Model 2 with Noisy Test Pattern.', 'Arrows indicate the ideal response\\npositions at the end of phoneme.', '8.', 'CONCLUSION\\nThe formal neuron was generalized to the dynamic formal neuron to recognize spatiotemporal patterns.', 'It is shown that existing learning rules can be generalized for dynamic formal\\nneurons.', 'An artificial neural network using dynamic formal neurons was applied to classifying 30\\nmodel phonemes with bipolar patterns created by using parameters of formant frequencies and\\ntheir bandwidths.', 'The model operates in two stages: in the first stage, it calculates the correlation between the input and prototype patterns stored in the transfer function matrix, and, in the\\nsecond stage, a lateral inhibition network selects the output of the phoneme pattern close to the\\ninput pattern.', \"39\\n\\n---{'.-\\\\\\n\\n3.\", '1\"\\'?', \"j\\n\\n,--; '\\n\\nat\\n\\ni!!\", 'e\\n\\n(C)\\n\\nzii\\n\\n~\\n\\nC\\n\\nIt\\n!\"', 'P,\\nX\\n\\n?', '?', 'I\\n\\nI\\n\\nI\\n\\nI\\n\\nI\\n\\nt ..\\n\\n51\\n\\nu.\\n\\nt51\\n\\nnme\\n.~\\n\\n3.\\n\\n\"', \"I\\n\\n~0\\n.'\", '--~\\n\\nu\\n\\n\\'1\\n\\n\"?', 'i!!', 'e\\nii\\n\\n(d)\\n\\nz\\n\\n,..\\n\\n~\\n\\nC\\n\\n?', 'It\\n\\n?', 'I\\n\\nI\\n\\n,\\n\\n?', 'Fig.', '4 (continued.)', 'Two models with different transfer functions were tested.', 'Model 1 was a matched filter\\nbank model and Model 2 was a pseudo-inverse filter model.', 'A sequence of phoneme patterns\\ncorresponding to continuous pronunciation of digits was used as a test pattern.', 'For the test pattern, Modell missed to recognize one phoneme and responded falsely once while Model 2\\ncorrectly recognized all the 32 phonemes in the test pattern.', 'When the flip noise which flips the\\nsign of the pattern with the probability 0.2, Model 1 missed three phonemes and falsely\\nresponded once while Model 2 recognized all the phonemes and false-alarmed once.', 'Both\\nmodels detected the phonerns at the correct position within the continuous stream.', 'References\\n1.', 'W. S. McCulloch and W. Pitts, \"A logical calculus of the ideas imminent in nervous\\nactivity,\" Bulletin of Mathematical Biophysics, vol.', '5, pp.', '115-133, 1943.', '2.', 'D. O. Hebb, The Organization of Behavior, Wiley, New York, 1949.', '40\\n\\n3.', 'D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning internal representations by\\nerror propagation,\" in Parallel Distributed Processing.', 'Vol.', '1, MIT, Cambridge, 1986.', '4.', 'B. Widrow and M. E. Hoff, \"Adaptive switching circuits,\" Institute of Radio Engineers.', 'Western Electronics Show and Convention, vol.', 'Convention Record Part 4, pp.', '96-104,\\n1960.', '5.', 'R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis.', 'Chapter 5, Wiley,\\nNew York, 1973.', '6.', 'T. Kohonen, Self-organization and Associative Memory, Springer-Verlag, Berlin, 1984.', '7.', 'F. Rosenblatt, Principles of Neurodynamics, Spartan Books, Washington, 1962.', '8.', '1.', 'M. Varah, \"A practical examination of some numerical methods for linear discrete illposed problems,\" SIAM Review, vol.', '21, no.', '1, pp.', '100-111, 1979.', '9.', 'C. Koch, J. Marroquin, and A. Y uiIle, \"Analog neural networks in early vision,\" Proceedings of the National Academy of Sciences.', 'USA, vol.', '83, pp.', '4263-4267, 1986.', '10.', 'G. O.', 'Stone, \"An analysis of the delta rule and the learning of statistical associations,\" in\\nParallel Distributed Processing .?', 'Vol.', '1, MIT, Cambridge, 1986.', '11.', 'B. Widrow and S. D. Stearns, Adaptive Signal Processing, Prentice-Hall, Englewood\\nCliffs, 1985.', '12.', 'D. H. Klatt, \"Software for a cascade/parallel formant synthesizer,\" Journal of Acoustical\\nSociety of America, vol.', '67, no.', '3, pp.', '971-995, 1980.', '13.', 'L. E. Atlas, T. Homma, and R. J.', 'Marks II, \"A neural network for vowel classification,\"\\n\\nProceedings International Conference on Acoustics.', 'Speech.', 'and Signal Processing, 1987.', '14.', 'R. P. Lippman, \"An introduction to computing with neural nets,\" IEEE ASSP Magazine,\\nApril, 1987.', '15.', 'S. Amari and M. A. Arbib, \"Competition and cooperation in neural nets,\" in Systems Neuroscience, ed.', 'J. Metzler, pp.', '119-165, Academic Press, New York, 1977.', '16.', 'R. L. Watrous and L. Shastri, \"Learning acoustic features from speech data using connectionist networks,\" Proceedings of The Ninth Annual Conference of The Cognitive Science\\nSociety, pp.', '518-530, 1987.', '17.', 'D. Tank and J. J. Hopfield, \"Concentrating information in time: analog neural networks\\nwith applications to speech recognition problems,\" Proceedings of International Conference on Neural Netoworks, San Diego, 1987.', '18.', 'J. R. Treichler, C. R.', 'Johnson,Jr., and M. G. Larimore, Theory and Design of Adaptive\\nFilters.', 'Chapter 5, Wiley, New York, 1987.', '19.', 'M Schetzen, The Volterra and Wiener Theories of Nonlinear Systems.', 'Chapter 16, Wiley,\\nNew York, 1980.', '20.', 'S. Grossberg, \"Associative and competitive principles of learning,\" in Competition and\\nCooperation in Neural Nets, ed.', 'M. A. Arbib, pp.', '295-341, Springer-Verlag, New York,\\n1982.', '21.', 'R. J.', 'Marks II, L. E. Atlas, J. J. Choi, S. Oh, K. F. Cheung, and D. C. Park, \"A performance analysis of associative memories with nonlinearities in the correlation domain,\"\\n(submitted to Applied Optics), 1987.', '22.', 'D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing, pp.', '230-234, Prentice-Hall, Englewood Cliffs, 1984.', '\\nOPTIMIZAnON WITH ARTIFICIAL NEURAL NETWORK SYSTEMS:\\nA MAPPING PRINCIPLE\\nAND\\nA COMPARISON TO GRADIENT BASED METHODS t\\nHarrison MonFook Leong\\nResearch Institute for Advanced Computer Science\\nNASA Ames Research Center 230-5\\nMoffett Field, CA, 94035\\n\\nABSTRACT\\nGeneral formulae for mapping optimization problems into systems of ordinary differential\\nequations associated with artificial neural networks are presented.', 'A comparison is made to optimization using gradient-search methods.', 'The perfonnance measure is the settling time from an initial\\nstate to a target state.', 'A simple analytical example illustrates a situation where dynamical systems\\nrepresenting artificial neural network methods would settle faster than those representing gradientsearch.', 'Settling time was investigated for a more complicated optimization problem using computer simulations.', 'The problem was a simplified version of a problem in medical imaging: determining loci of cerebral activity from electromagnetic measurements at the scalp.', 'The simulations\\nshowed that gradient based systems typically settled 50 to 100 times faster than systems based on\\ncurrent neural network optimization methods.', 'INTRODUCTION\\nSolving optimization problems with systems of equations based on neurobiological principles\\nhas recently received a great deal of attention.', 'Much of this interest began when an artificial\\nneural network was devised to find near-optimal solutions to an np-complete problem 13.', 'Since\\nthen, a number of problems have been mapped into the same artificial neural network and variations of it 10.13,14,17.18,19.21,23.24.', 'In this paper, a unifying principle underlying these mappings is\\nderived for systems of first to nth -order ordinary differential equations.', 'This mapping principle\\nbears similarity to the mathematical tools used to generate optimization methods based on the gradient.', 'In view of this, it seemed important to compare the optimization efficiency of dynamical\\nsystems constructed by the neural network mapping principle with dynamical systems constructed\\nfrom the gradient.', '.', 'THE PRINCIPLE\\nThis paper concerns itself with networks of computational units having a state variable V, a\\nfunction!', 'that describes how a unit is driven by inputs, a linear ordinary differential operator with\\nconstant coefficients D (v) that describes the dynamical response of each unit, and a function g that\\ndescribes how the output of a computational unit is detennined from its state v. In particular, the\\npaper explores how outputs of the computational units evolve with time in tenns of a scalar function E, a single state variable for the whole network.', 'Fig.', 'I summarizes the relationships between\\nvariables, functions, and operators associated with each computational unit.', 'Eq.', '(1) summarizes the\\nequations of motion for a network composed of such units:\\nD\"-+(M)(v)\\n\\n=1(g 1(v I)\\' .', '.', '.', '?', 'gN (VN ) )\\n\\n(I)\\n\\nwhere the i th element of jJ(M) is D(M)(Vj), superscript (M) denotes that operator D is Mth order,\\nthe i th element of\\nis !i(gl(VI) ?', '...?', 'gN(VN?, and the network is comprised of N computational units.', 'The network of Hopfield 12 has M=I, functions\\nare weighted linear sums, and functions 1 (where the ith element of 1 is gj(Vj) ) are all the same sigmoid function.', 'We will examine two ways of defining functions\\ngiven a function F. Along with these definitions will be\\n\\n1\\n\\n1\\n\\n1\\n\\nt Work supported by NASA Cooperative Agreement No.', 'NCC 2-408\\n\\n?', 'American Institute of Physics 1988\\n\\n\\x0c475\\n\\ndefined corresponding functions E that will be used to describe the dynamics of Eq.', '(1).', 'The first method corresponds to optimization methods introduced by artificial neural network\\nresearch.', 'It will be referred to as method V y (\"dell gil):\\n\\n!', \"== VyF\\n\\n(2a)\\n\\nwith associated E function\\n\\ntN[\\ndv '(S)jdg .\", '(S)\\nE\"j = F(\"g)-JL D(M)(v ?', \"(S?- - ''\\nds.\", 'i\\n\\ndt\\n\\n\\'\\n\\ndt\\n\\n(2b)\\n\\nHere, V xR denotes the gradient of H, where partials are taken with respect to variables of X, and\\nE7 denotes the E function associated with gradient operator V7\\' With appropriate operator D and\\nand g,\\nis simply the \"energy function\" of Hopfield 12.', 'Note that Eq.', '(2a) makes\\nfunctions\\nthat can be derived from scalar potential functions.', 'explicit that we will only be concerned with\\nFor example, this restriction excludes artificial neural networks that have connections between excitatory and inhibitory units such as that of Freeman 8.', 'The second method corresponds to optimization methods based on the gradient.', 'It will be referred to as method V if (\"dell v\"):\\n\\n1\\n\\nEr\\n\\n1\\n\\n1 == VyoF\\n\\n(3a)\\n\\nwith associated E function\\nEv>\\n\\nN [\\ndv ?', '(s) 1\\ndv ?', '(s )\\n= FCg) -JL\\nD(M)(v .', \"(s?--' '\\ni\\ndt\\ndt\\nt\\n\\nds\\n\\n(3b)\\n\\nI\\n\\nwhere notation is analogous to that for Eqs.\", '(2).', 'computational unit i :\\n~_\\n??', \"The critical result\\nthat allows us to map\\n\\\\\\\\\\noptimization problems into\\ntransform that detennines unit i's\\nnetworks described by Eq.\", 'output from state variable Vi\\n(1) is that conditions on the\\nconstituents of the equation\\ndifferential operator specifying the\\ncan be chosen so that along\\ndynamical characteristics of unit i\\nany solution trajectory, the\\nE function corresponding\\nfunction governing how inputs to\\nto the system will be a\\nunit i are combined to drive it\\nmonotonic function of time.', 'For method V\"j\\' here are\\n/\\nthe conditions: all functions g are 1) differentiable\\n/gl(V 1) \\'Tg2 (v:z)\\nI\\'\\nand 2) monotonic in the\\nsame sense.', 'Only the first\\nFigure 1: Schematic of a computational unit i from which netcondition is needed to\\nworks considered in this paper are constructed.', 'Triangles suggest\\nmake a similar assertion for\\nconnections between computational units.', 'method Vv- When these conditions are met and when solutions of Eq.', '(1) exist, the dynamical systems can be used for optimization.', 'The appendix contains proofs for the monotonicity of function\\nE along solution trajectories and references necessary existence theorems.', 'In conclusion, mapping\\noptimization problems onto dynamical systems summarized by Eq.', '(l) can be reduced to a matter\\nof differentiation if a scalar function representation of the problem can be found and the integrals\\nof Eqs.', '(2b) and (3b) are ignorable.', \"This last assumption is certainly upheld for the case where\\noperator D has no derivatives less than M'h order.\", \"In simulations below, it will be observed to\\nhold for the case M =1 with a nonzero O'h order derivative in D .\", '(Also see Lapedes and Falber 19.)', 'PERSPECTIVES OF RECENT WORK\\n\\n\\x0c476\\n\\nThe fonnulations above can be used to classify the neural network optimization techniques\\nused in several recent studies.', 'In these studies, the functions 1 were all identical.', \"For the most\\npart, following Hopfield's fonnulation, researchers 10.13.14.17.23.24 have used method Vy to derive\\nwith Ey quadratic in functions 1 and\\nfonns of Eq.\", '(1) that exhibit the ability to find extrema of\\nall functions 1 describable by sigmoid functions such as tanh (x ).', 'However, several researchers\\nhave written about artificial neural networks associated with non-quadratic E functions.', 'Method\\nVy has been used to derive systems capable of finding extrema of non-quadrntic Ey 19.', 'Method\\nVv has been used to derive systems capable of optimizing Ev where Ev were not necessarily quadratic in variables V 21.', 'A sort of hybrid of the two methods was used by Jeffery and Rosner 18 to\\nfind extrema of functions that were not quadratic.', 'The important distinction is that their functions j\\nwere derived from a given function Fusing Eq.', '(3a) where, in addition, a sign definite diagonal\\nmatrix was introduced; the left side of Eq.', '(3a) was left multiplied by this matrix.', 'A perspective\\non the relationship between all three methods to construct dynamical systems for optimization is\\nsummarized by Eq.', '(4) which describes the relationship between methods Vyand Vyo:\\n\\nE-t\\n\\nV?', \"= <liag [a~~;ll-l V,J'\\n\\n(4)\\n\\nwhere diag [ Xi] is a diagonal matrix with Xi as the diagonal element of row i.\", '(A similar equation\\nhas been derived for quadratic F s.) The relationship between the method of Jeffery and Rosner\\nand Vv is simply Eq.', '(4) with the time dependent diagonal matrix replaced by a constant diagonal\\nmatrix of free parameters.', 'It is noted that Jeffery and Rosner presented timing results that compared\\nsimulated annealing.', 'conjugate-gradient, and artificial neural network methods for optimization.', 'Their results are not comparable to the results reported below since they used computation time as\\na perfonnance measure, not settling times of analog systems.', 'The perspective provided by Eq.', '(4)\\nwill be useful for anticipating the relative performance of methods V~ and Vv in the analytical\\nexample below and will aid in understanding the results of computer simulations.', 'COMPARISON OF METHODS Vt AND Vv\\nWhen M =1 and operator D has no Ofh order derivatives, method Vv is the basis of gradientsearch methods of optimization.', 'Given the long history of of such methods.', 'it is important to know\\nwhat possible benefits could be achieved by the relatively ne,w optimization scheme.', 'method Vy .', 'In the following.', 'the optimization efficiency of methods Vt and Vv is compared by comparing settling times.', 'the time required for dynamical systems described by Eq.', '(1) to traverse a continuous\\npath to local optima.', 'To qualify this perfonnance measure.', 'this study anticipates application to the\\ncreation of analog devices that would instantiate Eq.', '(1); hence, we are not interested in estimating\\nthe number of discrete steps that would be required to find local optima, an appropriate performance measure if the point was to develop new numerical methods.', \"An analytical example will\\nserve to illustrate the possibility of improvements in settling time by using method Vt instead of\\nmethod VV' Computer simulations will be reported for more complicated problems following this\\nexample.\", 'For the analytical example, we will examine the case where all functions 1 are identical and\\ng(v)\\n\\n= tanhG(v -Th)\\n\\n(5)\\n\\nwhere G > 0 is the gain and Th is the threshold.', 'Transforms similar to this are widely used in\\nartificial neural network research.', 'Suppose we wish to use such computational units to search a\\nmulti-dimensional binary solution space.', 'We note that\\n\\n!li..\\n= G sech 2G(v -Th)\\ndv\\n\\n(6)\\n\\nis near 0 at valid solution states (comers of a hypercube for the case of binary solution spaces).', 'We\\nsee from Eq.', '(4) that near a valid solution state.', 'a network based on method Vy will allow computational units to recede from incorrect states and approach correct states comparatively faster.', 'Does\\n\\n\\x0c477\\n\\nthis imply faster settling time for method V\"t?', \"To obtain an analytical comparison of settling times, consider the case where M =1 and\\noperator D has no Om order derivatives and\\n\\nF\\n\\n1\\n= -2~'J\\n~('.?(tanhGv?\", ')(tanhGv ? )', '?', \"J\\n\\n(7)\\n\\n'oJ\\n\\nwhere matrix S is symmetric.\", 'Method Vy gives network equations\\n\\ndV =StanhGv\\n\\n(8)\\n\\n~ =diag [G sech 2Gvj 1S tanhGV\\n\\n(9)\\n\\ndt\\n\\nand method Vv gives network equations\\n\\nwhere tanhGY denotes a vector with i\\'\" component tanhGv;.', 'For method Vr there is one stable\\npoint, i.e.', \"where '::\\n\\n= 0, at V = O .\", 'For method Vv the stable points are\\n\\nV = 0 and V ?', 'V where\\n\\nV is the set of vectors with component values that are either +- or - .', 'Further trivialization\\nallows for comparing estimates of settling times: Suppose S is diagonal.', 'For this case, if Vj = 0 is\\non the trajectory of any computational unit i for one method, Vj\\n0 is on the trajectory of that unit\\nfor the other method; hence, a comparison of settling times can be obtained by comparing time\\nestimates for a computational unit to evolve from near 0 to near an extremum or, equivalently, the\\nconverse.', 'Specifically, let the interval be [Bo, I-a] where 0< Bo<l-a and o<a<1.', 'For method V..,\\nintegrating velocity over time gives the estimate\\n\\n=\\n\\n1[1\\'2 [1\\n1 1+ [1-a\\n5(2-5) - l-aJ\\n\"5(2-a) ~\\n00 lJ\\n\\nT Vi = G\\n\\nIn\\n\\n(10)\\n\\nand for method V y the estimate is\\n\\nT,,;=\\n\\n~ln [~~~) ~l\\n\\n(11)\\n\\nFrom these estimates, method Vv will always take longer to satisfy the criterion for convergence:\\nNote that only with the largest value for Bo, Bo =1-5, is the first term of Eq.', '(10) zero; for any\\nsmaller Bo, this term is positive.', 'Unfortunately, this simple analysis cannot be generalized to nondiagonal S. With diagonal S, all computational units operate independently.', \"Hence, the derivation\\nof ':: is irrelevant with respect to convergence rates; convergence rate depends only on the diagonal element of S having the smallest magnitude.\", 'In this sense, the problem is one dimensional.', \"But for non-diagonal S, the problem would be, in general, multi-dimensional and, hence, the direction of ':: becomes relevant To compare settling times for non-diagonal S, computer simulations\\nwere done.\", \"'These are described below.\", 'COMPUTER SIMULAnONS\\nMethods\\nThe problem chosen for study was a much simplified version of a problem in medical imaging: Given electromagnetic field measurements taken from the human scalp, identify the location\\nand magnitude of cerebral activity giving rise to the fields.', 'This problem has received much attention in the last 20 years 3,6.7.', 'The problem, sufficient for our purposes here, was reduced to the\\nfollowing problem: given a few samples of the electric potential field at the surface of a spherical\\nconductor within which reside several static electric dipoles, identify the dipole locations and\\nmoments.', \"For this situation, there is a closed form solution for electric potential fields at the\\n\\n\\x0c478\\n\\nspherical surface:\\n(12)\\nwhere ~ is the electric potential at the spherical conductor surface, 'Xsamp/~ is the location of the\\nsample point ( x denotes a vector, i the corresponding unit vector, and x the corresponding vector\\nmagnitude), j1; is the dipole moment of dipole i, and d; is the vector from dipole i to X:ampl~ (This\\nequation can be derived from one derived by Brody, Terry, and Ideker 4 ).\", 'Fig.', '2 facilitates picturing these relationships.', 'With this analytical solution, the problem was formulated as a least squares minimization problem where\\nthe variables were dipole moments.', 'In short, the following process was used: A dipole model was chosen.', 'This model was used with Eq.', '(12) to calculate potentials at points on a sphere which covered about 60% of\\nthe surface.', 'A cluster of internal locations that encompassed the locations of the model was specified.', 'The\\ntwo optimization techniques were then required to determine dipole moment values at cluster locations such\\nthat the collection of dipoles at cluster locations accuFigure 2: Vectors of Eq.', '(12).', 'rately reflected the dipole distribution specified by the\\nmodel.', 'This was to be done given only the potential values at the sample points and an initial guess of\\ndipole moments at cluster locations.', 'The optimization systems were to accomplish the task by\\nminimizing the sum of squared differences between potentials calculated using the dipole model\\nand potentials calculated using a guess of dipole moments at cluster locations where the sum is\\ntaken over all sample points.', 'Further simplifications of the problem included\\n1)\\nchoosing the dipole model locations to correspond exactly to various locations of the cluster,\\n2)\\nrequiring dipole model moments to.be I, 0, or -I, and\\n3)\\nrepresenting dipole moments at cluster locations with two bit binary numbers.', \"To describe the dynamical systems used, it suffices to specify operator D and functions '( of\\nEq.\", '(1) and function F used in Eqs.', '(2a) and (3a).', 'Operator D was\\nD\\n\\n=\\n\\nd\\n\\ndt + 1.', '(13)\\n\\nEq.', \"(5) with a multiplicative factor of 112 was used for all functions '(.\", 'Hence, regarding\\nsimplification 3) above, each cluster location was associated with two computational units.', 'Considering simplification 2) above, dipole moment magnitude 1 would be represented by both computational units being in the high state, for -I, both in the low state, and for 0, one in the high state and\\none in the low state.', 'Regarding function F ,\\nF\\n\\n= ~\\n\\nall samp/~\\npoims s\\n\\n[~lMaSlll\\'~d(X:) -\\n\\n<Ilcillomr (\\'Xs)\\n\\nr-\\n\\nc\\n\\n~\\n\\ng (v)2\\n\\n(14)\\n\\nall compu,ariOflal\\nu\"irs j\\n\\nwhere ~_as\"\"~d is calculated from the dipole model and Eq.', '(12) (The subscript measured is used\\nbecause the role of the dipole model is to simulate electric potentials that would be measured in a\\nreal world situation.', 'In real world situations, we do not know the source distribution underlying\\n~_asar~d .', \"), C is an experimentally detennined constant (.002 was used), and ~clJIS'~r is Eq.\", '(12)\\nwhere the sum of Eq.', '(12) is taken over all cluster locations and the k,h coordinate of the i,h cluster location dipole moment is\\n?', \"Pi#:\\n\\n=\\n\\n~\\nall bits b\\n\\ng (Vil:b)'\\n\\n(15)\\n\\n\\x0c479\\n\\nIndex j of Eq.\", '(14) corresponds to one combination of indices ikb.', 'Sample points, 100 of them, were scattered semi-uniformly over the spherical surface\\nemphasized by horizontal shading in Fig.', '3.', 'Ouster locations, 11, and model dipoles, 5, were scattered within the subset of the sphere emphasized by vertical shading.', 'For the dipole model used,\\n10 dipole moment components were non-zero; hence, optimization techniques needed to hold 56\\ndipole moment components at zero and set 10 components to correct non-zero values in order to\\ncorrectly identify the dipole model underlying ~_Qs\"\\'~d\\'\\nThe dynamical systems corresponding to\\n0.8\\nmethods V,.', \"and Vv' were integrated using the\\nrelative radii\\nforward Euler method (e.g.\", 'Press, Flannery,\\nI I\\nTeukolsky, and Vetterling 22).', \"Numerical\\n,'\\nI I\\nmethods were observed to be convergent experI\\nimentally: settling time and path length were\\nI ,\\nobserved to asymtotically approach stable\\nI I\\nvalues as step size of the numerical integrator\\nI I\\nwas decreased over two orders of magnitude.\", 'Settling times, path lengths, and relative\\ndirections of travel were calculated for the two\\noptimization methods using several different\\ninitial bit patterns at the cluster locations.', 'In\\nFigure 3: illustration of the distribution of\\nother\\nwords.', 'the search was started at different\\nsample points on the surface of the sphericorners\\nof the hypercube comprising the space\\ncll conductor (horizontal shading) and the\\nof acceptable solutions.', 'One corner of the\\ndistribution of model dipole locations and\\nhypercube was chosen to be the target solution.', 'cluster locations within the conductor\\n(Note\\nthat a zero dipole moment has a degen(verticll shading).', 'erate two bit representation in the dynamical\\nsystems explored; the target corner was arbitrarily chosen to be one of the degenerate solutions.)', 'Note from Eq.', '(5) that for the network to reach a hypercube corner, all elements of would have\\nto be singular.', 'For this reason, settling time and other measures were studied as a function of the\\nproximity of the computational units to their extremum states.', 'Computations were done on a Sequent Balance.', 'I\\n\\n,\\n\\nI\\n\\nI\\n\\n,\\n\\nI\\n\\nv\\n\\n5\\nResults\\nGraph 1 shows results for exploring settling\\ntime as a function of extremum depth, the minimum of\\nthe deviations of variables\\nfrom the threshold of\\nfunctions g. Extremum depth is reported in multiples\\nof the width of functions g. The term transition, used\\nin the caption of Graph 1 and below, refers to the\\nmovement of a computational unit from one extremum\\nstate to the other.', 'The calculations were done for two\\ninitial states, one where the output of 1 computational\\nunit was set to zero and one where outputs of 13 computational units were set to zero; bence, 1 and 13,\\nrespectively, half transitions were required to reach\\nthe target hypercube comer.', \"It can be observed that\\nsettling time increases faster for method V v' than that\\nfor method Vy just as we would expect from considering Eqs.\", '(4) and (5).', 'However, it can be observed\\nthat method Vv is still an order of magnitude faster\\neven wben extremum depth is 3 widths of functions\\ng. For the purpose of unambiguously identifying\\nwhat hypercube corner the dynamical system settles\\n\\nv\\n\\n+,1\\n\\n4\\n3\\n\\nI\\n\\n-\\n\\n~\\n\\nI\\n\\nI\\n\\n~\\n\\n~\\n\\n#\\n\\n...\\n\\nt---.', 'o\\no\\n\\n\"\\n-\\n\\n2\\nextremum depth\\n\\n1\\n\\n\\'\"-\\n\\n=-\\n\\n-\\n\\n4\\n\\n3\\n\\nGraph 1: settling time as a function of\\nextremum depth.', '#: method Vr- 1 half\\ntransition required.', '.', ': method V 13\\nhalf transitions required.', '+: method\\nV.... 1 half transition required.', '-: V....\\n13 half transitions required.', 'r\\n\\n\\x0c480\\n\\nto, this extremum depth is more than adequate.', 'Table 1 displays results for various initial conditions.', 'Angles are reported in degrees.', 'These\\nmeasures refer to the angle between directions of travel in v-space as specified by the two optimization methods.', 'The average angle reported is taken over all trajectory points visited by the numerical integrator.', 'Initial angle is the angle at the beginning of the path.', 'Parasite cost percentage is a\\nmeasure that compares parasite cost, the integral in Eqs.', '(2b) and (3b), to the range of function F\\nover the path:\\n\\n.', 'parasite cost %\\n\\nparasite cost\\n\\n= 100x I F\\nF\\nI\\nfi\",,\" ;,udal\\n\\ntransitions\\nreauired\\n\\ntime\\n\\n1\\n\\n0.16\\n0.0016\\n\\n100\\n\\n6.1\\n1.9\\n\\n2\\n\\n0.14\\n0.0018\\n\\n78\\n\\n4.7\\n1.9\\n\\n75\\n\\n3\\n\\n0.15\\n0.0021\\n\\n71\\n\\n4.7\\n2.1\\n\\n74\\n\\n7\\n\\n0.19\\n0.0032\\n\\n59\\n\\n4.6\\n2.4\\n\\n63\\n\\n10\\n\\n0.17\\n0.0035\\n\\n49\\n\\n3.8\\n2.5\\n\\n60\\n\\n13\\n\\n0.80\\n0.0074\\n\\n110\\n\\n9.2\\n3.2\\n\\n39\\n\\nrelative path initial Mean angle extremum\\ntime\\nlen2th anlZle (std dev)\\ndeoth\\n68\\n\\n(16)\\n\\nparasite\\ncost %\\n\\n76 (3.8)\\n76 (3.5)\\n\\n2.3\\n2.3\\n\\n0.22\\n0.039\\n\\n72 (4.3)\\n73 (4.1)\\n\\n2.5\\n2.5\\n\\n0.055\\n0.016\\n\\n71 (3.7)\\n72 (3.0)\\n\\n2.3\\n2.5\\n\\n0.051\\n0.0093\\n\\n69 (4.1)\\n71 (7.0)\\n\\n2.4\\n2.7\\n\\n0.058\\n0.0033\\n\\n63 (2.8)\\n\\n64 (4.7)\\n\\n2.5\\n2.8\\n\\nO.OOO6{)\\n\\n77 (11)\\n71 (8.9)\\n\\n2.3\\n2.7\\n\\n0.076\\n0.0028\\n\\n0.030\\n\\nTable 1: Settling time and other measurements for various required transitions.', 'For\\neach transition case, the upper row is for V y and the lower row is for V v- Std deY\\ndenotes standard deviation.', 'See text for definition of measurement terms and units.', 'Noting the differences in path length and angles reported, it is clear that the path taken to the target\\nhypercube comer was quite different for the two methods.', 'Method V v settles from 1 to 2 orders of\\nmagnitude faster than method V -r and usually takes a path less than half as long.', 'These relationships did not change significantly for different values for c of Eq.', '(14) and coefficients of Eq.', '(13)\\n(both unity in Eq.', '(13?.', 'Values used favored method Vr Parasite cost is consistently less\\nsignificant for method V v and is quite small for both methods.', 'To further compare the ability of the optimization methods to solve the brain imaging problem, a large variety of initial hypercube comers were tested.', 'Table 2 displays results that suggest\\nthe ability of each method to locate the target comer or to converge to a solution that was consistent with the dipole model.', 'Initial comers were chosen by randomly selecting a number of computational units and setting them to eXtI\"emwn states opposite to that required by the target solution.', 'Five cases were run for each case of required transitions.', 'It can be observed that the system based\\non method Vv is better at finding the target comer and is much better at finding a solution that is\\nconsistent with the dipole model.', 'DISCUSSION\\nThe simulation results seem to contradict settling time predictions of the second analytical\\nexample.', 'It is intuitively clear that there is no contradiction when considering the analytical example as a one dimensional search and the simulations as multi-dimensional searches.', 'Consider Fig.', '4\\nwhich illustrates one dimensional search starting at point I.', 'Since both optimization methods must\\ndecrease function E monotonically, both must head along the same path to the minimum point A.', 'Now consider Fig.', \"5 which illustrates a two dimensional search starting at point I: Here, the two\\nmethods needn't follow the same paths.\", 'The two dashed paths suggest that method V.\" can still be\\n\\n\\x0c481\\n\\nV..\\n\\ntransitions I\\nrequired\\n3\\n4\\n\\n5\\n\\nI\\n\\n6\\n\\nVv\\n\\n~erent dipole different target different dipole different target\\ncomer comer\\nsolution\\ncomer comer,\\nsolution\\n1\\n4\\n0\\n5\\n0\\n0\\n4\\n1\\n1\\n1\\n0\\n3\\n4\\n4\\n1\\n1\\n0\\n0\\n4\\n1\\n1\\n2\\n0\\n2\\n1\\n4\\n4\\n1\\n0\\n0\\n3\\n1\\n1\\n5\\n0\\n0\\n5\\n5\\n0\\n0\\n0\\nI 0\\n2\\n3\\n0\\n5\\n0\\n0\\n\\nI\\n\\nI\\n\\nI\\n\\n7\\n13\\n20\\n26\\n\\n33\\n\\n5\\n\\n0\\n\\n40\\n\\n5\\n5\\n5\\n\\n0\\n\\n46\\n\\nI\\n\\n53\\n\\nI\\n\\n0\\n0\\n\\n0\\n0\\n0\\n0\\n\\n3\\n3\\n2\\n4\\n\\n2\\n\\nI\\n\\n2\\n\\n3\\n1\\n\\n0\\n\\nI\\n\\n0\\n0\\n\\n!', '0\\n\\nTable 2: Solutions found starting from various initial conditions, five cases for each\\ntransition case.', 'Different dipole solution indicates that the system assigned non-zero\\ndipole moments at cluster locations that did not correspond to locations of the dipole\\nmodel sources.', 'Different corner indicates the solution was consistent with the dipole\\nmodel but was not the target hypercube comer.', 'Target corner indicates that the solution was the target solution.', 'monotonically decreasing E while traversing a more circuitous route to minimum B or traversing a path to minimum\\nA.', 'The longer path lengths reported in Table 1 for method\\nV~ suggest the occurrence of the fonner.', 'The data of Table\\n2 verifies the occurrence of the latter: Note that for many\\nv\\ncases where the system based on method Vv settled to the .', 'Figure 4: One dimensional search\\ntarget comer, the system based on method V~ settled to some\\nother minimum.', 'for minima.', 'Would we observe similar differences in optimization\\nI\\nefficiency for other optimization problems that also have\\nbinary solution spaces?', 'A view that supports the plausibility\\nof the affirmative is the following: Consider Eq.', '(4) and Eq.', 'E\\n(5).', 'We have already made the observation that method Vv\\nwould slow convergence into extrema of functions g. We\\nhave observed this experimentally via Graph 1.', \"These observations suggest that computational units of Vv systems\\ntend to stay closer to the transition regions of functions g\\ncompared to computational units of V'I systems.\", 'It seems\\nplausible that this property may allow Vv systems to avoid\\nadvancing too deeply toward ineffective solutions and, hence,\\nallow the systems to approach effective solutions more\\nFigure 5: Two dimensional search\\nefficiently.', '1bis behavior might also be the explanation for\\nfor minima.', 'the comparative success of method Vv revealed in Table 2.', 'Regarding the construction of electronic circuitry to instantiate Eq.', '(l), systems based on\\nmethod Vv would require the introduction of a component implementing multiplication by the\\nderivative of functions g. This additional complexity may binder the use of method Vv for the\\n\\n\\x0c482\\n\\nconstruction of analog circuits for optimization.', 'To\\nillustrate the extent of this additional complexity, Fig.', 'Input\\n6a shows a schematized circuit for a computational\\nunit of method V-r and Fig.', '6b shows a schematized\\ncircuit for a computational unit of method VT The\\nsimulations reported above suggest that there may be\\nproblems for which improvements in settling time\\nOutput\\nmay offset complications that might come with added\\ncircuit complexity.', 'On the problem of imaging cerebral activity, the\\nresults above suggest the possibility of constructing\\nanalog devices to do the job.', 'Consider the problem of\\nanalyzing electric potentials from the scalp of one perOutput\\nson: It is noted that the measured electric potentials,\\nFigure 6: Schematized circuits for a com- ~_as\"rcd\\' appear as linear coefficients in F of Eq.', 'putational unit Notation is consistem (14); hence, they would appear as constant terms in\\nwith Horowitz and Hill IS.', 'Shading of of Eq.', '(1).', 'Thus.', 'cf)_asllrcd would be implemented as\\namplifiers is to e3IIllark components amplifier biases in the circuits of Figs.', '6.', 'This is a\\nreferred to in the text.', 'a) Computational significant benefit.', 'To understand this.', 'note that funcunit for method Vr b) Computational tion Ij of Fig.', '1 corresponding to the optimization of\\n.', 'ti\\nthod V\\nfunction F of Eq.', '(14) would involve a weighted\\numt or me\\n...\\nlinear sum of inputs g 1(v 1), ???', ', gN (VN).', 'The weights\\nwould be the nonlinear coefficients of Eq.', '(14) and correspond to the strengths of the connections\\nshown in Fig.', '1.', 'These connection strengths need only be calculated once for the person ar!d Car!', 'then be set in hardware using, for example, a resistor network.', 'Electric potential measurements\\ncould then be ar!alyzed by simply using the measurements to bias the input to shaded amplifiers of\\nFigs.', '6.', 'For initialization, the system can be initialized with all dipole moments at zero (the 10\\ntransition case in Table 1).', 'This is a reasonable first guess if it is assumed that cluster locations are\\nfar denser than the loci of cerebral activity to be observed.', 'For subsequent measurements, the solution for immediately preceding measurements would be a reasonable initial state if it is assumed\\nthat cerebral activity of interest waxes and wanes continuously.', 'Might non-invasive real time imaging of cerebral activity be possible using such optimization\\ndevices?', 'Results of this study are far from adequate for answering this question.', 'Many complexities that have been avoided may nUllify the practicality of the idea.', 'Among these problems are:\\n1)\\nThe experiment avoided the possibility of dipole sources actually occurring at locations other\\nthan cluster locations.', 'The minimization of function F of Eq.', '(14) may circumvent this\\nproblem by employing the superposition of dipole moments at neighboring cluster locations\\nto give a sufficient model in the mear!.', '2)\\nThe experiment asswned a very restricted range of dipole strengths.', 'This might be dealt\\nwith by increasing the number of bits used to represent dipole moments.', '3)\\nThe conductor model, a homogeneously conducting sphere, may not be sufficient to model\\nthe hwnan head 16.', 'Non-sphericity ar!d major inhomogeneities in conductivity Car!', 'be dealt\\nwith, to a certain extent, by replacing Eq.', '(12) with a generalized equation based on a\\nnumerical approximation of a boundary integral equation 20\\n4)\\nThe cerebral activity of interest may not be observable at the scalp.', '5)\\nNot all forms of cerebral activity give rise to dipolar sources.', '(For example, this is well\\nknown in olfactory cortex 8.)', '6)\\nActivity of interest may be overwhelmed by irrelevant activity.', 'Many methods have been\\ndevised to contend with this problem (For example, Gevins and Morgan 9.)', 'Clearly, much theoretical work is left to be done.', '(a)\\n\\n(b)\\n\\n1\\n\\nCONCLUDING REMARKS\\n\\n\\x0c483\\n\\nIn this study.', 'the mapping principle underlying the application of artificial neural networks to\\nthe optimization of multi-dimensional scalar functions has been stated explicitly.', 'Hopfield 12 has\\nshown that for some scalar functions.', 'i.e.', 'functions F quadratic in functions 1. this mapping can\\nlead to dynamical systems that can be easily implemented in hardware.', 'notably.', 'hardware that\\nrequires electronic components common to semiconductor technology.', 'Here.', 'mapping principles\\nthat have been known for a considerably longer period of time.', 'those underlying gradient based\\noptimization, have been shown capable of leading to dynamical systems that can also be implemented using semiconductor hardware.', 'A problem in medical imaging which requires the search of\\na multi-dimensional surface full of local extrema has suggested the superiority of the latter mapping\\nprinciple with respect to settling time of the corresponding dynamical system.', '1bis advantage may\\nbe quite significant when searching for global extrema using techniques such as iterated descent 2\\nor iterated genetic hill climbing 1 where many searches for local extrema are required.', 'This advantage is further emphasized by the brain imaging problem: volumes of measurements can be\\nanalyzed without reconfiguring the interconnections between computational units; hence, the cost of\\ndeveloping problem specific hardware for finding local extrema may be justifiable.', 'Finally.', 'simulations have contributed plausibility to a possible scheme for non-invasively imaging cerebral\\nactivity.', 'APPENDIX\\n\\nTo show that for a dynamical system based on method Vr E,.', 'is a monotonic function of\\ntime given that all functions g are differentiable and monotonic in the same sense, we need to\\nshow that the derivative of ET with respect to time is semi-definite:\\n\\ndET\\ndt\\n\\n-\\n\\nN dFT dg j N [M\\ndVj ] dg,\\n= L - - - L D( )(Vj)-- - .', 'j\\n\\ndgj\\n\\ndt\\n\\ni\\n\\ndt\\n\\n(Ala)\\n\\ndt\\n\\nSubstituting Eq.', '(2a),\\n-dET ==\\ndt\\n\\nN [\\nI,\\nf?', \"dV'] dg?\", \"'dt\\ndt\\n\\n(Alb)\\n\\n-D(M)(v ? )\", \"+ - ' - ' .\", \"j '\\n\\nUsing Eq.\", '(1),\\n\\nd~ = N [dV i\\ndt\\n~, dt\\n\\n]2 dgi ~O\\n\\n(Alc)\\n\\nav?, s\\n\\nas needed.', 'The appropriate inequality depends on the sense in which functions 1 are monotonic.', 'In a similar manner, the result can be obtained for method Vv>- With the condition that functions 1\\nare differentiable, we can show that the derivative of 4 is semi-definite:\\n\\ndE.\".', 'dt\\n\\n_v\\n\\ndv?', \"N [\\ndV'] dv?\", \"v= IN ,dF' - I,\\nD(M)(Vj)_-' - ' .\", 'j\\n\\ndVj\\n\\ndt\\n\\nj\\n\\ndt\\n\\n(A2a)\\n\\ndt\\n\\nUsing Eqs.', '(3a) and (1),\\n\\ndEv\\nN [dVj\\ndt - ~\\n, dt\\n\\n--~-\\n\\n]2~0\\n\\n(A2b)\\n\\nS\\n\\nas needed.', 'In order to use the results derived above to conclude that Eq.', '(1) can be used for optimization of functions 4 and Et in the vicinity of some point\\nwe need to show that there exists a\\nneighborhood of Vo in which there exist solution trajectories to Eq.', '(1).', 'The necessary existence\\ntheorems and transformations of Eq.', '(1) needed in order to apply the theorems can be found in\\nmany texts on ordinary differential equations; e.g.', 'Guckenheimer and Holmes 11.', 'Here, it is mainly\\nimportant to state that the theorems require that functions ,?c(1), functions g are differentiable,\\nand initial conditions are specified for all derivatives of lower order than M.\\n\\nvo.', '484\\n\\nACKNOWLEDGEMENTS\\nI would like to thank Dr. Michael Raugh and Dr. Pentti Kanerva for constructive criticism\\nand support.', 'I would like to thank Bill Baird and Dr. James Keeler for reviewing this work.', 'I\\nwould like to thank Dr. Derek Fender, Dr. John Hopfield, and Dr. Stanley Klein for giving me\\nopportunities that fostered this conglomeration of ideas.', '[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n[13]\\n[14]\\n[15]\\n[16]\\n\\n[171\\n[18]\\n[19]\\n[20]\\n[21]\\n[22]\\n[23]\\n[24]\\n\\nREFERENCES\\nAckley D.H., \"Stochastic iterated genetic bill climbing\", PhD.', 'dissertation, Carnegie Mellon\\nU.,1987.', 'Bawn E., Neural Networks for Computing, ed.', 'Denker 1.S.', '(AlP Confrnc.', 'Proc.', '151, ed.', 'Lerner R.G.', '), p53-58, 1986.', 'Brody D.A., IEEE Trans.', 'vBME-32, n2, pl06-110, 1968.', 'Brody D.A., Terry F.H., !deker RE., IEEE Trans.', 'vBME-20, p141-143, 1973.', 'Cohen M.A., Grossberg S., IEEE Trans.', 'vSMC-13, p815-826, 1983.', 'Cuffin B.N., IEEE Trans.', 'vBME-33, n9, p854-861.', '1986.', 'Darcey T.M., AIr J.P., Fender D.H., Prog.', 'Brain Res., v54, pI28-134, 1980.', 'Freeman W J., \"Mass Action in the Nervous System\", Academic Press, Inc., 1975.', 'Gevins A.S., Morgan N.H., IEEE Trans., vBME-33, n12, pl054-1068, 1986.', 'Goles E., Vichniac G.Y., Neural Networks for Computing, ed.', 'Denker J.S.', '(AlP Confrnc.', 'Proc.', '151, ed.', 'Lerner R.G.', '), p165-181, 1986.', 'Guckenheimer J., Holmes P., \"Nonlinear Oscillations, Dynamical Systems, and Bifurcations\\nof Vector Fields\", Springer Verlag, 1983.', 'Hopfield J.I., Proc.', 'Nat!.', 'Acad.', 'Sci., v81, p3088-3092, 1984.', 'Hopfield 1.1., Tank D.W., Bio.', 'Cybrn., v52, p141-152, 1985.', 'Hopfield 1.J., Tank D.W., Science, v233, n4764, p625-633, 1986.', 'Horowitz P., Hill W., \"The art of electronics\", Cambridge U.', 'Press, 1983.', 'Hosek RS., Sances A., Jodat RW., Larson S.I., IEEE Trans., vBME-25, nS, p405-413, 1978.', 'Hutchinson J.M., Koch C., Neural Networks for Computing, ed.', 'Denker J.S.', '(AlP Confrnc.', 'Proc.', '151, ed.', 'Lerner RG.', '), p235-240, 1986.', 'Jeffery W., Rosner R, Astrophys.', 'I., v310, p473-481, 1986.', 'Lapedes A., Farber R., Neural Networks for Computing, ed.', 'Denker 1.S.', '(AlP Confrnc.', 'Proc.', 'lSI, ed.', 'Lerner RG.', '), p283-298, 1986.', 'Leong H.M.F., \\'\\'Frequency dependence of electromagnetic fields: models appropriate for the\\nbrain\", PhD.', 'dissertation, California Institute of Technology, 1986.', 'Platt I.C., Hopfield J.J., Neural Networks for Computing, ed.', 'Denker I.S.', '(AlP Confrnc.', 'Proc.', '151, ed.', 'Lerner RG.', '), p364-369, 1986.', 'Press W.H., Flannery B.P., Teukolsky S.A., Vetterling W.T., \"Numerical Recipes\", Cambridge U.', 'Press, 1986.', 'Takeda M., Goodman J.W., Applied Optics, v25.', 'n18, p3033-3046, 1986.', 'Tank D.W., Hopfield I.J., \"Neural computation by concentrating infornation in time\", preprint, 1987.', '\\nPROGRAMMABLE SYNAPTIC CHIP FOR\\nELECTRONIC NEURAL NETWORKS\\nA. Moopenn, H. Langenbacher, A.P.', 'Thakoor, and S.K.', 'Khanna\\nJet Propulsion Laboratory\\nCalifornia Institute of Technology\\nPasadena, CA 91009\\nABSTRACT\\nA binary synaptic matrix chip has been developed for electronic\\nneural networks.', 'The matrix chip contains a programmable 32X32\\narray of \"long channel\" NMOSFET binary connection elements implemented in a 3-um bulk CMOS process.', 'Since the neurons are kept offchip, the synaptic chip serves as a \"cascadable\" building block for\\na multi-chip synaptic network as large as 512X512 in size.', 'As an\\nalternative to the programmable NMOSFET (long channel) connection\\nelements, tailored thin film resistors are deposited, in series with\\nFET switches, on some CMOS test chips, to obtain the weak synaptic\\nconnections.', 'Although deposition and patterning of the resistors\\nrequire additional\\nprocessing steps, they promise substantial\\nsavings in silcon area.', 'The performance of a synaptic chip in a 32neuron breadboard system in an associative memory test application\\nis discussed.', 'INTRODUCTION\\nThe highly parallel and distributive architecture of neural\\nnetworks offers potential advantages in fault-tolerant and high\\nspeed associative information processing.', 'For the past few years,\\nthere has been a growing interest in developing electronic hardware\\nto investigate the computational\\ncapabilities and application\\npotential of\\nneural networks as well as their dynamics and\\ncollective propertiesl - 5 ?', 'In an electronic hardware implementation\\nof neural networks6 ?', '7 r the neurons (analog processing units) are\\nrepresented by threshold amplifiers and the synapses linking the\\nneurons by a resistive connection network.', 'The synaptic strengths\\nbetween neurons (the electrical resistance of the connections)\\nrepresent the stored information or the computing function of the\\nneural network.', 'Because of the massive interconectivity of the neurons and the\\nlarge number of the interconnects required with the increasing\\nnumber of neurons, implementation of a synaptic network using\\ncurrent LSI/VLSI technology can become very difficult.', 'A synaptic\\nnetwork based on a multi-chip architecture would lessen this\\ndifficulty.', 'He have designed, fabricated, and successfully tested\\nCMOS-based programmable synaptic chips which could serve as basic\\n\" cascadabl e\" building blocks for a multi-chip electronic neural\\nnetwork.', 'The synaptic chips feature complete programmability of\\n1024, (32X32) binary synapses.', 'Since the neurons are kept offchip, the synaptic chips can be connected in parallel, to obtain\\nmultiple grey levels of the connection strengths, as well as\\n?', 'American Institute of Physics 1988\\n\\n\\x0c565\\n\\n\"cascaded\" to form larger synaptic arrays for an expansion to a 512neuron system in a feedback or feed-forward architecture.', 'As a\\nresearch tool, such a system would offer a significant speed\\nsoftware-based neural network\\nimprovement\\nover\\nconventional\\nsimulations since convergence times for the parallel hardware system\\nwould be significantly smaller.', \"In this paper, we describe the basic design and operation of\\nsynaptic CMOS chips incorporating MOSFET's as binary connection\\nelements.\", 'The design and fabrication of synaptic test chips with\\ntailored thin film resistors as ballast resistors for controlling\\npower dissipation are also described.', 'Finally, we describe a\\nsynaptic chip-based 32-neuron breadboard system in a feedback\\nconfiguration and discuss its performance in an associative memory\\ntest application.', 'BINARY SYNAPTIC CMOS CHIP WITH MOSFET CONNECTION ELEMENTS\\nThere are two important design requirements for a binary\\nconnection element in a high density synaptic chip.', 'The first\\nrequirement is that the connection in the ON state should be \"weak\"\\nto ensure low overall power dissipation.', 'The required degree of\\n\"weakness\" of the ON connection largely depends on the synapse\\ndensity of the chip.', 'If, for example, a synapse density larger than\\n1000 per chip is desired, a dynamic resistance of the ON connection\\nshould be greater than ~100 X-ohms.', 'The second requirement is that\\nto obtain grey scale synapses with up to four bits of precision from\\nbinary connections, the consistency of the ON state connection\\nresistance must be better than +/-5 percent, to ensure proper\\nthreshold operation of the neurons.', 'Both of the requirements are\\ngenerally difficult to satisfy simultaneously in conventional VLSI\\nCMOS technology.', 'For example, doped-polysilicon resistors could be\\nused to provide the weak connections, but they are difficult to\\nfabricate with a resistance uniformity of better than 5 percent.', \"We have used NMOSFET's as connection elements in a multi-chip\\nsynaptic network.\", \"By designing the NMOSFET's with long channel,\\nboth the required high uniformity and high ON state resistance have\\nbeen obtained.\", \"A block diagram of a binary synaptic test chip\\nincorporating NMOSFET's as programmable connection elements is shown\\nin Fig.\", '1.', 'A photomicrograph of the chip is shown in Fig.', '2.', 'The\\nsynaptic chip was fabricated through MOSIS (MOS Implementation\\nbulk CMOS,\\ntwo-level metal, P-well\\nService) in a 3-micron,\\ntechnology.', 'The chip contains 1024 synaptic cells arranged in a\\n32X32 matrix configuration.', 'Each cell consists of a long channel\\nNMOSFET connected in series with another NMOSFET serving as a simple\\nON/OFF switch.', 'The state of the FET switch is controlled by the\\noutput of a latch which can be externally addressed via the ROW/COL\\naddress decoders.', 'The 32 analog input lines (from the neuron\\noutputs) and 32 analog output lines (to the neuron inputs) allow a\\nnumber of such chips to be connected together to form larger\\nconnection matrices with up to 4-bit planes.', 'The long channel NMOSFET can function as either a purely\\nresistive or a constant current source connection element, depending\\n\\n\\x0c566\\n\\nFROM NEURON OUTPUTS\\n\\n1 ???', '32\\n\\nvG ______~~~----~~----_=~\\n~-.--a.', '....\\n\\nUr-C~\\n\\n/I6-A9\\n\\n\\\\\\n\\nAOOR\\nOECOOER\\n\\nSETRST\\n\\n--------I\\n\\n\\\\\\n\\n.', '.', 'ROW~V~\\nC~\\n\\n.\\\\\\n.\\\\\\n.', 'S\\n\\n?', '\\\\ RST\\n\\nR\\n\\n.', '::-Q\\n\\n???????????????', '?', '?', '1 1\\n??', '?', '0\\n\\n.', '32\\n\\nTO\\nNEURON\\nINPUTS\\n\\n?', 'I L -----d I\\n6=.', '.', '.', '.', '.', '=a\\nAO-M\\n\\nFigure 1.', 'Block diagram of a 32X32 binary synnaptic chip with long\\nchannel NMOSFETs as connection elements .', \"...\\n\\n~'\\n\\n.\", ',,)\\n\\n.,.', ',\"\\n\\nFigure 2.', 'Photomicrographs of a 32X32 binary connection CMOS chip.', 'The blowup on the right shows several synaptic cells; the \"S\"-shape\\nstructures are the long-channel NMOSFETs.', 'on whether analog or binary output neurons are used.', 'As a resistive\\nconnection.', \"the NMOSFET's must operate in the linear region of the\\ntransistor's drain I-V characteristics.\", 'In the linear region.', 'the\\nchannel resistance is approximately given byB\\nRo N\\n\\n=\\n\\n(11K)\\n\\n(LIN)\\n\\n(VG - VT H ) -\\n\\n1 ?', '567\\n\\nHere, K is a proportionality constant which depends on process\\nparameters, Land Ware the channel length and width respectively,\\nVG is the gate voltage, and VTH is the threshold voltage.', 'The\\ntransistor acts as a linear resistor provided the voltage across the\\nchannel is much less than the difference of the gate and threshold\\nvoltages, and thus dictates the operating voltage range of the\\nconnection.', \"The NMOSFET's presently used in our synaptic chip\\ndesign have a channel length of 244 microns and width of 12 microns.\", 'At a gate voltage of 5 volts, a channel resistance of about 200 Kohms was obtained over an operating voltage range of 1.5 volts.', 'The\\nconsistency of the transistor I-V characteristics has been verified\\nto be within +/-3 percent in a single chip and +/-5 percent for\\nchips from different fabrication runs.', 'In the latter case, the\\ntransistor characteristics in the linear region can be further\\nmatched to within +/-3% by the fine adjustment of their common gate\\nbias.', 'With two-state neurons, current source connections may be used\\nby operating the transistor in the saturation mode.', 'Provided the\\nvoltage across the channel is greater than (VG - VTH), the\\ntransistor behaves almost as a constant current source with the\\nsaturation current given approximately byB\\nION = K (W/L)\\n\\n(VG - VTH)2 .', 'With the appropriate selection of L, W, and VG, it is possible to\\nobtain ON-state currents which vary by two orders of magnitude in\\nvalues.', 'Figure 3 shows a set of measured I-V curves for a NMOSFET\\nwith the channel dimensions, L= 244 microns and W=12 microns and\\napplied gate voltages from 2 to 4.5 volts.', \"To ensure constant\\ncurrent source operation, the neuron's ON-state output should be\\ngreater than 3.5 volts.\", 'A consistency of the ON-state currents to\\nwithin +/-5 percent has similarly been observed in a set of chip\\nsamples.', 'With current source connections therefore, quantized grey\\nscale synapses with up to 16 grey levels (4 bits) can be realized\\nusing a network of binary weighted current sources.', 'Figure 3.', 'I-V characteristics of\\nan NMOSFET connection element.', 'L=244 urn,\\nChannel\\ndimension:\\nW=12um\\n\\nFor proper operation of the NMOSFET connections, the analog\\noutput lines (to neuron inputs) should always be held close to\\nground potential.', 'Moreover, the voltages at the analog input lines\\nmust be at or above ground potential.', \"Since the current normally\\n\\n\\x0c568\\n\\nflows from the analog input to the output, the NMOSFET's may be used\\nas either all excitatory or inhibitory type connections.\", \"However,\\nthe complementary connection function can be realized using long\\nFor a PMOSFET\\nchannel PMOSFET's in series with PMOSFET switches.\", 'connection, the voltage of an analog input line would be at or below\\nground.', 'Furthermore, due to the difference in the mobilites of\\nelectrons and holes in the channel, a PMOSFET used as a resistive\\nconnection has a channel resistance about twice as large as an\\nNMOSFET with the same channel dimension.', 'This fact results in a\\nsubtantial reduction in the size of PMOSFET needed.', \"THIN FILM RESISTOR CONNECTIONS\\nThe use of MOSFET's as connection elements in a CMOS synaptic\\nmatrix chip has the major advantage that the complete device can be\\nreadily fabricated in a conventional CMOS production run.\", \"However,\\nthe main disadvantages are the large area (required for the long\\nchannel) for the MOSFET's connections and their non-symmetrical\\ninhibitory/excitatory functional characteristics.\", 'The large overall\\ngate area not only substantially limits the number of synapses that\\ncan be fabricated on a single chip, but the transistors are more\\nsusceptible to processing defects which can lead to excessive gate\\nleakage and thus reduce chip yield considerably.', \"An alternate\\napproach is simply to use resistors in place of MOSFET's.\", 'We have\\ninvestigated one such approach where thin film resistors are\\ndeposited on top of the passivation layer of CMOS-processed chips as\\nan additional special processing step to the normal CMOS fabrication\\nrun.', 'With an appropriate choice of resistive materials, a dense\\narray of resistive connections with highly uniform resistance of up\\nto 10 M-ohms appears feasible.', 'Several candidate materials, including a cermet based on\\nplatinum/aluminum oxide, and amorphous semiconductor/metal alloys\\nsuch as a-Ge:Cu and a-Ge:Al, have been examined for their applicability as thin film resistor connections.', \"These materials are of\\nparticular interest since their resistivity can easily be tailored\\nin the desired semiconducting range of 1-10 ohm-cm by controlling\\nthe metal content'.\", 'The a-Ge/metal films are deposited by thermal\\nevaporation of presynthesized alloys of the desired composition in\\nhigh vacuum, whereas platinum/aluminum oxide films are deposited by\\nco-sputtering from platinum and aluminum oxide targets in a high\\npurity argon and oxygen gas mixture.', 'Room temperature resistivities\\nin the 0.1 to 100 ohm-cm range have been obtained by varying the\\nmetal content in these materials.', 'Other factors which would also\\ndetermine their suitability include their device processing and\\nmaterial compatibilities and their stability with time, temperature, and extended application of normal operating electric current.', 'The temperature coefficient of resistance (TCR) of these materials\\nat room temperature has been measured to be in the 2000 to 6000 ppm\\nrange.', \"Because of their relatively high TCR's, the need for weak\\nconnections to reduce the effect of localized heating is especially\\nimportant here.\", 'The a-Ge/metal alloy films are observed to be\\nrelatively stable with exposure to air for temperatures below 130o C.\\n\\n\\x0c569\\n\\nThe platinum/aluminum oxide film stabilize with time after annealing\\nin air for several hours at 130o C.\\nSample test arrays\\nof thin film resistors based on the\\ndescribed materials have been fabricated to test their consistency.', 'The resistors, with a nominal resistance of 1 M-ohm, were deposited\\non a glass substrate in a 40X40 array over a O.4cm by O.4cm area.', 'Variation in the measured resistance in these test arrays has been\\nfound to be from +/- 2-5 percent for all three materials.', 'Smaller\\ntest arrays of a-Ge:Cu thin film resistors on CMOS test chips have\\nalso been fabricated.', 'A photo-micrograph of a CMOS synaptic test\\nchip containing a 4X4 array of a-Ge:Cu thin film resistors is shown\\nin Fig.', '4.', 'Windows in the passivation layer of silicon nitride\\n(SiN) were opened in the final processing step of a normal CMOS\\nfabrication run to provide access to the aluminum metal for\\nelectrical contacts.', 'A layer of resistive material was deposited\\nand patterned by lift-off.', 'A layer of buffer metal of platinum or\\nnickel was then deposited by RF sputtering and also patterned by\\nlift-off.', 'The buffer metal pads serve as a conducting bridges for\\nconnecting the aluminum electrodes to the thin film resistors.', 'In\\naddition to providing a reliable ohmic contact to the aluminum and\\nresistor, it also provides conformal step coverage over the silicon\\nnitride window edge.', 'The resistor elements on the test chip are 100\\nmicron long, 10 micron wide with a thickness of about 1500 angstroms\\nand a nominal resistance of 250 K-ohms.', 'Resistance variations from\\n10-20 percent have been observed in several such test arrays.', 'The\\nunusually large variation is largely due to the surface roughness of\\nthe chip passivation layer.', 'As one possible solution, a thin spin-\\n\\nFigure 4.', 'Photomicrographs of?', 'a CMOS synaptic test chip with a 4X4\\narray of a-Ge:Cu thin film resistors.', 'The nominal resistance was\\n250 K-ohms.', '570\\n\\non coating of an insulating material such as polyimide to smooth out\\nthe surface of the passivation layer prior to depositing the\\nresistors is under investigation.', 'SYNAPTIC CHIP-BASED 32-NEURON BREADBOARD SYSTEM\\nA 32-neuron breadboard system utilizing an array of discrete\\nneuron electronics has been fabricated to evaluate the operation of\\n32X32 binary synaptic CMOS chips with NMOSFET connection elements.', 'Each neuron consists of an operational amplifier configured as a\\ncurrent to voltage converter (with virtual ground input) followed by\\na fixed-gain voltage difference amplifier.', 'The overall time\\nconstant of the neurons is approximately 10 microseconds.', 'The\\nneuron array is interfaced directly to the synaptic chip in a full\\nfeedback configuration.', 'The system also contains prompt electronics\\nconsisting of a programmable array of RC discharging circuits with a\\nrelaxation time of approximately 5 microseconds.', 'The prompt\\nhardware allows the neuron states to be initialized by precharging\\nthe selected capacitors in the RC circuits.', 'A microcomputer\\ninterfaced to the breadboard system is used for programming the\\nsynaptic matrix chip, controlling the prompt electronics, and\\nreading the neuron outputs.', 'The stability of the breadboard system is tested in an\\nassociative mellory feedback configuration,b.', 'A dozen random dilutecoded binary vectors are stored using the following simplified\\nouter-product storage scheme:\\n~\\ns s\\nif L Vi Vj = 0\\n-1\\nTi\\n\\nj\\n\\n=\\n\\nf\\n10\\n\\nS\\n\\notherwise.', 'In this scheme, the feedback matrix consists of only inhibitory (1lor open (0) connections.', 'The neurons are set to be normally ON\\nand are driven OFF when inhibited by another neuron via the feedback\\nmatrix.', 'The system exhibits excellent stability and associative\\nrecall performance.', 'Convergence to a nearest stored memory in\\nHamming distance is always observed for any given input cue.', 'Figure\\n5 shows some typical neuron output traces for a given test prompt\\nand a set of stored memories.', 'The top traces show the response of\\ntwo neurons that are initially set ON; the bottom traces for two\\nother neurons initially set OFF.', 'Convergence times of 10-50\\nmicroseconds have\\nbeen\\nobserved, depending on the prompt\\nconditions, but are primarily governed by the speed of the neurons.', 'CONCLUSIONS\\nSynaptic CMOS\\nchips containing\\n1024 programmable binary\\nsynapses in a 32X32 array have been designed, fabricated, and\\ntested.', 'These synaptic chips are designed to serve as basic\\nbuilding blocks for large multi-chip synaptic networks.', 'The use of\\nlong channel\\nMOSFET\\'s as either resistive or current source\\nconnection elements meets the \"weak\" connection and consistency\\n\\n\\x0c571\\n\\nFigure 5.', 'Typical neuron response curves for\\n(Horiz scale: 10 microseconds per div)\\n\\na test\\n\\nprompt input.', 'requirements.', 'Alternately, CMOS-based synaptic test chips with\\nspecially deposited thin film high-valued resistors, in series with\\nFET switches,\\noffer an\\nattractive approach to high density\\nprogrammable synaptic chips.', 'A 32-neuron breadboard system\\nincorporating a\\n32X32 NMOSFET\\nsynaptic chip and a feedback\\nconfiguration exhibits excellent stability and associative recall\\nperformance as an associative memory.', 'Using discrete neuron array,\\nconvergence times of 10-50 microseconds have been demonstrated.', 'With optimization of the input/output wiring layout and the use of\\nhigh speed neuron electronics, convergence times can certainly be\\nreduced to less than a microsecond.', 'ACKNOWLEDGEMENTS\\nThis work was performed by the Jet Propulsion Laboratory,\\nCalifornia Institute of Technology, and was sponsored by the Joint\\nTactical Fusion Program Office, through an agreement with the\\nNational Aeronautics and Space Administration.', 'The authors would\\nlike to thank John Lambe for his invaluable suggestions, T. Duong\\nfor his assistance in the breadboard hardware development, J. Lamb\\nand S. Thakoor for their help in the thin film resistor deposition,\\nand R. Nixon and S. Chang for their assistance in the chip layout\\ndesign.', 'REFERENCES\\n1.', '2.', '3.', '4.', '5.', 'J. Lambe, A. Moopenn, and A.P.', 'Thakoor, Proc.', 'AIAA/ACM/NASA/IEEE Computers in Aerospace V, 160 (1985)\\nA.P.', 'Thakoor, J.L.', 'Lamb, A. Moopenn, and S.K.', 'Khanna, MRS\\nProc.', '95, 627 (1987)\\nW. Hubbard, D. Schwartz, J. Denker, H.P.', 'Graf, R. Howard, L.\\nJackel, B. Straughn, and D. Tennant, AIP Conf.', 'Proc.', '151, 227\\n(1986)\\nM.A.', 'Sivilotti, M.R.', 'Emerling, and C. Mead, AIP Conf.', 'Proc.', '151, 408 (1986)\\nJ.P. Sage, K. Thompson, and R.S.', 'Withers, AIP Conf.', 'Proc.', '151,\\n\\n\\x0c572\\n\\n6.', '7.', '8,\\n9.', '381\\n3.3.', '3.3.', 'S.M.', 'ley,\\n3.L.', 'Sci.', '(19861\\nHopfield, Proc.', 'Nat.', 'Acad.', 'SCi., 81, 3088 (1984)\\nHopfield, Proc.', 'Nat.', 'Acad.', 'Sci., 79, 2554 (1982)\\nSze, \"Semiconductor Devices-Physics and Technology,\" (WiNew York, 1985) p.205\\nLamb, A.P.', 'Thakoor, A. Moopenn, and S.K.', 'Khanna, 3.', 'Vac.', 'Tech., A 5(4), 1407 (1987)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence segmentation\")\n",
    "sentences = []\n",
    "for f in files:\n",
    "    f = open(f, \"r\", encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    templist = nltk.sent_tokenize(text)\n",
    "    for item in templist:\n",
    "        sentences.append(item)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Correlational Strength and Computational Algebra\n",
      "of Synaptic Connections Between Neurons\n",
      "Eberhard E. Fetz\n",
      "Department of Physiology & Biophysics,\n",
      "University of Washington, Seattle, WA 98195\n",
      "ABSTRACT\n",
      "Intracellular recordings in spinal cord motoneurons and cerebral\n",
      "cortex neurons have provided new evidence on the correlational strength of\n",
      "monosynaptic connections, and the relation between the shapes of\n",
      "postsynaptic potentials and the associated increased firing probability.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A typical EPSP of 100 ~v triggers about .01 firings per\n",
      "EPSP.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'typical',\n",
       " 'EPSP',\n",
       " 'of',\n",
       " '100',\n",
       " '~v',\n",
       " 'triggers',\n",
       " 'about',\n",
       " '.01',\n",
       " 'firings',\n",
       " 'per',\n",
       " 'EPSP',\n",
       " '.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "word_tokenize(sentences[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{66: 5, 24: 96, 30: 59, 13: 104, 14: 126, 19: 112, 16: 120, 18: 109, 41: 19, 35: 40, 21: 121, 29: 70, 27: 77, 9: 94, 58: 5, 2: 486, 12: 112, 34: 41, 15: 121, 8: 116, 57: 8, 23: 98, 52: 9, 22: 127, 36: 34, 5: 152, 99: 1, 17: 103, 26: 81, 45: 19, 47: 14, 54: 12, 28: 65, 40: 28, 20: 124, 48: 13, 32: 50, 31: 61, 6: 104, 60: 6, 11: 108, 43: 22, 25: 81, 4: 173, 37: 37, 7: 110, 3: 143, 39: 24, 42: 17, 53: 15, 44: 21, 76: 1, 92: 2, 51: 8, 120: 1, 88: 4, 61: 2, 49: 13, 68: 2, 10: 106, 33: 55, 70: 1, 100: 1, 82: 3, 71: 3, 46: 13, 38: 28, 67: 3, 73: 3, 72: 1, 78: 1, 1: 111, 81: 1, 63: 6, 59: 5, 65: 4, 50: 6, 56: 10, 129: 1, 79: 5, 74: 2, 80: 3, 62: 4, 69: 5, 64: 2, 86: 2, 91: 1, 55: 3, 95: 3, 97: 2, 96: 1, 75: 1, 77: 2, 85: 3, 93: 2, 89: 2, 107: 1, 104: 2, 83: 2, 110: 1, 128: 1, 147: 1, 172: 1, 90: 1, 113: 1, 127: 2, 101: 1, 105: 1, 171: 1}\n"
     ]
    }
   ],
   "source": [
    "sentence_length_dict = {}   # keys are length of the sentence, values are number of sentence for that particular length\n",
    "for item in sentences:\n",
    "    item = word_tokenize(item)\n",
    "    sentence_length = len(item)\n",
    "    if sentence_length in sentence_length_dict:\n",
    "        sentence_length_dict[sentence_length] = sentence_length_dict[sentence_length] + 1\n",
    "    else:\n",
    "        sentence_length_dict[sentence_length] = 1\n",
    "        \n",
    "print(sentence_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 24, 30, 13, 14, 19, 16, 18, 41, 35, 21, 29, 27, 9, 58, 2, 12, 34, 15, 8, 57, 23, 52, 22, 36, 5, 99, 17, 26, 45, 47, 54, 28, 40, 20, 48, 32, 31, 6, 60, 11, 43, 25, 4, 37, 7, 3, 39, 42, 53, 44, 76, 92, 51, 120, 88, 61, 49, 68, 10, 33, 70, 100, 82, 71, 46, 38, 67, 73, 72, 78, 1, 81, 63, 59, 65, 50, 56, 129, 79, 74, 80, 62, 69, 64, 86, 91, 55, 95, 97, 96, 75, 77, 85, 93, 89, 107, 104, 83, 110, 128, 147, 172, 90, 113, 127, 101, 105, 171]\n",
      "[5, 96, 59, 104, 126, 112, 120, 109, 19, 40, 121, 70, 77, 94, 5, 486, 112, 41, 121, 116, 8, 98, 9, 127, 34, 152, 1, 103, 81, 19, 14, 12, 65, 28, 124, 13, 50, 61, 104, 6, 108, 22, 81, 173, 37, 110, 143, 24, 17, 15, 21, 1, 2, 8, 1, 4, 2, 13, 2, 106, 55, 1, 1, 3, 3, 13, 28, 3, 3, 1, 1, 111, 1, 6, 5, 4, 6, 10, 1, 5, 2, 3, 4, 5, 2, 2, 1, 3, 3, 2, 1, 1, 2, 3, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(sentence_length_dict.keys()))\n",
    "print(list(sentence_length_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The consequences of these data for information processing by\n",
      "polysynaptic connections is discussed.\n",
      "[('The', 'DT'), ('consequences', 'NNS'), ('of', 'IN'), ('these', 'DT'), ('data', 'NNS'), ('for', 'IN'), ('information', 'NN'), ('processing', 'NN'), ('by', 'IN'), ('polysynaptic', 'JJ'), ('connections', 'NNS'), ('is', 'VBZ'), ('discussed', 'VBN'), ('.', '.')]\n",
      "The effects of sequential polysynaptic\n",
      "links can be calculated by convolving the effects of the underlying\n",
      "monosynaptic connections.\n",
      "[('The', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('sequential', 'JJ'), ('polysynaptic', 'JJ'), ('links', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('calculated', 'VBN'), ('by', 'IN'), ('convolving', 'VBG'), ('the', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('underlying', 'JJ'), ('monosynaptic', 'JJ'), ('connections', 'NNS'), ('.', '.')]\n",
      "The net effect of parallel pathways is the sum of\n",
      "the individual contributions.\n",
      "[('The', 'DT'), ('net', 'JJ'), ('effect', 'NN'), ('of', 'IN'), ('parallel', 'JJ'), ('pathways', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('sum', 'NN'), ('of', 'IN'), ('the', 'DT'), ('individual', 'JJ'), ('contributions', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range (4,7):\n",
    "    print(sentences[i])\n",
    "    print(pos_tag(word_tokenize(sentences[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
